import json
import os
import numpy as np
from typing import List, Dict, Any
import faiss
import pickle
from pathlib import Path
import re

# PDFå¤„ç†
import PyPDF2
from io import BytesIO

# æ¨¡å‹ç›¸å…³
from openai import OpenAI
from modelscope import AutoModelForCausalLM, AutoTokenizer
from sentence_transformers import CrossEncoder

# æ–‡æœ¬å¤„ç†
import jieba
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity


class RAGSystem:
    def __init__(self, 
                 chunk_size=500,
                 chunk_overlap=50,
                 top_k=10,
                 rerank_top_k=3):
        
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.top_k = top_k
        self.rerank_top_k = rerank_top_k
        
        # å›ºå®šä½¿ç”¨Qwen3-14Bæ¨¡å‹å’Œç¼“å­˜ç›®å½•
        model_name = "Qwen/Qwen3-14B"
        model_cache_dir = "autodl-tmp/qwen"
        
        # åˆ›å»ºæ¨¡å‹ç¼“å­˜ç›®å½•
        os.makedirs(model_cache_dir, exist_ok=True)
        print(f"æ¨¡å‹ç¼“å­˜ç›®å½•: {model_cache_dir}")
        
        # åˆå§‹åŒ–æœ¬åœ°LLM
        print("æ­£åœ¨åŠ è½½æœ¬åœ°LLMæ¨¡å‹...")
        print(f"ä½¿ç”¨æ¨¡å‹: {model_name}")
        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                cache_dir=model_cache_dir,  # è®¾ç½®ç¼“å­˜ç›®å½•
                local_files_only=False  # å…è®¸ä»ç½‘ç»œä¸‹è½½
            )
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                cache_dir=model_cache_dir,  # è®¾ç½®ç¼“å­˜ç›®å½•
                torch_dtype="auto",
                device_map="auto",
                low_cpu_mem_usage=True,  # æ·»åŠ ä½å†…å­˜ä½¿ç”¨é€‰é¡¹
                local_files_only=False  # å…è®¸ä»ç½‘ç»œä¸‹è½½
            )
            print("æœ¬åœ°LLMæ¨¡å‹åŠ è½½å®Œæˆ")
        except Exception as e:
            print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–æ¨¡å‹åç§°")
            raise e
        
        # åˆå§‹åŒ–embeddingå®¢æˆ·ç«¯
        self.embedding_client = OpenAI(
            api_key=os.getenv("DASHSCOPE_API_KEY"),
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
        )
        self.embedding_model = "text-embedding-v4"
        
        # åˆå§‹åŒ–rerankæ¨¡å‹
        print("æ­£åœ¨åŠ è½½é‡æ’åºæ¨¡å‹...")
        try:
            self.reranker = CrossEncoder("BAAI/bge-reranker-base")
            print("é‡æ’åºæ¨¡å‹åŠ è½½å®Œæˆ")
        except:
            print("é‡æ’åºæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ä½œä¸ºå¤‡é€‰")
            self.reranker = None
        
        # å‘é‡åº“ç›¸å…³
        self.index = None
        self.chunks = []
        self.chunk_embeddings = []
    
    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """ä»PDFæ–‡ä»¶ä¸­æå–æ–‡æœ¬"""
        text = ""
        try:
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                for page in pdf_reader.pages:
                    text += page.extract_text() + "\n"
        except Exception as e:
            print(f"è¯»å–PDFæ–‡ä»¶ {pdf_path} æ—¶å‡ºé”™: {e}")
        return text
    
    def split_text_into_chunks(self, text: str) -> List[str]:
        """å°†æ–‡æœ¬åˆ†å‰²æˆchunks"""
        # ç®€å•çš„æ–‡æœ¬åˆ†å‰²ç­–ç•¥
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]', text)
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
                
            if len(current_chunk) + len(sentence) <= self.chunk_size:
                current_chunk += sentence + "ã€‚"
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + "ã€‚"
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def get_embeddings(self, texts: List[str]) -> np.ndarray:
        """è·å–æ–‡æœ¬çš„embeddings"""
        embeddings = []
        batch_size = 10  # æ‰¹é‡å¤„ç†ä»¥é¿å…APIé™åˆ¶
        
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i+batch_size]
            try:
                response = self.embedding_client.embeddings.create(
                    model=self.embedding_model,
                    input=batch_texts,
                    dimensions=1024,
                    encoding_format="float"
                )
                
                batch_embeddings = [data.embedding for data in response.data]
                embeddings.extend(batch_embeddings)
                print(f"å·²å¤„ç† {min(i+batch_size, len(texts))}/{len(texts)} æ¡æ–‡æœ¬çš„embedding")
                
            except Exception as e:
                print(f"è·å–embeddingæ—¶å‡ºé”™: {e}")
                # å¦‚æœå‡ºé”™ï¼Œæ·»åŠ é›¶å‘é‡ä½œä¸ºå ä½ç¬¦
                for _ in batch_texts:
                    embeddings.append([0.0] * 1024)
        
        return np.array(embeddings)
    
    def build_knowledge_base(self, pdf_folder: str, save_path: str = "knowledge_base.pkl"):
        """æ„å»ºçŸ¥è¯†åº“"""
        print("æ­£åœ¨æ„å»ºçŸ¥è¯†åº“...")
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨çŸ¥è¯†åº“æ–‡ä»¶
        if os.path.exists(save_path):
            print("å‘ç°å·²å­˜åœ¨çš„çŸ¥è¯†åº“æ–‡ä»¶ï¼Œæ­£åœ¨åŠ è½½...")
            with open(save_path, 'rb') as f:
                data = pickle.load(f)
                self.chunks = data['chunks']
                self.chunk_embeddings = data['embeddings']
                self.index = data['index']
            print("çŸ¥è¯†åº“åŠ è½½å®Œæˆ")
            return
        
        # è¯»å–æ‰€æœ‰PDFæ–‡ä»¶
        all_text = ""
        pdf_files = list(Path(pdf_folder).glob("*.pdf"))
        
        for pdf_file in pdf_files:
            print(f"æ­£åœ¨å¤„ç†: {pdf_file}")
            text = self.extract_text_from_pdf(str(pdf_file))
            all_text += f"\n\n=== {pdf_file.name} ===\n\n" + text
        
        # åˆ†å‰²æ–‡æœ¬
        print("æ­£åœ¨åˆ†å‰²æ–‡æœ¬...")
        self.chunks = self.split_text_into_chunks(all_text)
        print(f"å…±ç”Ÿæˆ {len(self.chunks)} ä¸ªæ–‡æœ¬å—")
        
        # ç”Ÿæˆembeddings
        print("æ­£åœ¨ç”Ÿæˆembeddings...")
        embeddings = self.get_embeddings(self.chunks)
        self.chunk_embeddings = embeddings
        
        # æ„å»ºFAISSç´¢å¼•
        print("æ­£åœ¨æ„å»ºFAISSç´¢å¼•...")
        dimension = embeddings.shape[1]
        self.index = faiss.IndexFlatIP(dimension)  # ä½¿ç”¨å†…ç§¯ç›¸ä¼¼åº¦
        
        # å½’ä¸€åŒ–embeddingsä»¥ä½¿ç”¨å†…ç§¯è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        normalized_embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)
        self.index.add(normalized_embeddings.astype(np.float32))
        
        # ä¿å­˜çŸ¥è¯†åº“
        print("æ­£åœ¨ä¿å­˜çŸ¥è¯†åº“...")
        with open(save_path, 'wb') as f:
            pickle.dump({
                'chunks': self.chunks,
                'embeddings': self.chunk_embeddings,
                'index': self.index
            }, f)
        
        print("çŸ¥è¯†åº“æ„å»ºå®Œæˆ")
    
    def retrieve_relevant_docs(self, query: str) -> List[Dict[str, Any]]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        # è·å–æŸ¥è¯¢çš„embedding
        query_embedding = self.get_embeddings([query])[0]
        query_embedding = query_embedding / np.linalg.norm(query_embedding)
        
        # æ£€ç´¢
        scores, indices = self.index.search(
            query_embedding.reshape(1, -1).astype(np.float32), 
            self.top_k
        )
        
        # æ•´ç†æ£€ç´¢ç»“æœ
        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(self.chunks):  # ç¡®ä¿ç´¢å¼•æœ‰æ•ˆ
                results.append({
                    'text': self.chunks[idx],
                    'score': float(score),
                    'index': int(idx)
                })
        
        return results
    
    def rerank_documents(self, query: str, documents: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """é‡æ’åºæ–‡æ¡£"""
        if not documents:
            return documents
        
        if self.reranker is not None:
            try:
                # ä½¿ç”¨CrossEncoderè¿›è¡Œé‡æ’åº
                pairs = [(query, doc['text']) for doc in documents]
                scores = self.reranker.predict(pairs)
                
                # æ›´æ–°åˆ†æ•°å¹¶æ’åº
                for i, score in enumerate(scores):
                    documents[i]['rerank_score'] = float(score)
                
                documents = sorted(documents, key=lambda x: x['rerank_score'], reverse=True)
            except Exception as e:
                print(f"é‡æ’åºå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹åˆ†æ•°: {e}")
        
        # è¿”å›top_kä¸ªç»“æœ
        return documents[:self.rerank_top_k]
    
    def generate_answer(self, query: str, context_docs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”Ÿæˆç­”æ¡ˆ"""
        # æ„å»ºä¸Šä¸‹æ–‡
        context = "\n\n".join([f"å‚è€ƒæ–‡æ¡£{i+1}:\n{doc['text']}" for i, doc in enumerate(context_docs)])
        
        # æ„å»ºprompt
        prompt = f"""åŸºäºä»¥ä¸‹å‚è€ƒæ–‡æ¡£å›ç­”é—®é¢˜ï¼Œå¦‚æœå‚è€ƒæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´æ˜æ— æ³•ä»æä¾›çš„æ–‡æ¡£ä¸­æ‰¾åˆ°ç­”æ¡ˆã€‚

å‚è€ƒæ–‡æ¡£:
{context}

é—®é¢˜: {query}

è¯·æä¾›å‡†ç¡®ã€è¯¦ç»†çš„ç­”æ¡ˆ:"""

        try:
            # å‡†å¤‡æ¨¡å‹è¾“å…¥
            messages = [{"role": "user", "content": prompt}]
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True,
                enable_thinking=True
            )
            model_inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)
            
            # ç”Ÿæˆç­”æ¡ˆ
            generated_ids = self.model.generate(
                **model_inputs,
                max_new_tokens=512,  # å‡å°‘tokenæ•°é‡ï¼Œé€‚åˆå°æ¨¡å‹
                temperature=0.3,     # é™ä½æ¸©åº¦ï¼Œæé«˜ç­”æ¡ˆçš„ç¡®å®šæ€§
                do_sample=True,
                top_p=0.9,          # æ·»åŠ top_pé‡‡æ ·
                pad_token_id=self.tokenizer.eos_token_id
            )
            output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()
            
            # è§£æthinkingå†…å®¹
            try:
                index = len(output_ids) - output_ids[::-1].index(151668)
            except ValueError:
                index = 0
            
            thinking_content = self.tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip("\n")
            content = self.tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip("\n")
            
            return {
                'answer': content,
                'thinking': thinking_content,
                'context_docs': context_docs,
                'success': True
            }
            
        except Exception as e:
            print(f"ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™: {e}")
            return {
                'answer': f"ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºç°é”™è¯¯: {e}",
                'thinking': "",
                'context_docs': context_docs,
                'success': False
            }
    
    def process_single_query(self, query: str) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªæŸ¥è¯¢"""
        print(f"æ­£åœ¨å¤„ç†æŸ¥è¯¢: {query}")
        
        # æ£€ç´¢ç›¸å…³æ–‡æ¡£
        retrieved_docs = self.retrieve_relevant_docs(query)
        
        # é‡æ’åº
        reranked_docs = self.rerank_documents(query, retrieved_docs)
        
        # ç”Ÿæˆç­”æ¡ˆ
        result = self.generate_answer(query, reranked_docs)
        
        return {
            'query': query,
            'retrieved_docs': retrieved_docs,
            'reranked_docs': reranked_docs,
            'answer': result['answer'],
            'thinking': result['thinking'],
            'success': result['success']
        }
    
    def batch_inference(self, input_json_path: str, output_json_path: str):
        """æ‰¹é‡æ¨ç†"""
        print(f"å¼€å§‹æ‰¹é‡æ¨ç†ï¼Œè¾“å…¥æ–‡ä»¶: {input_json_path}")
        
        # è¯»å–è¾“å…¥æ•°æ®
        with open(input_json_path, 'r', encoding='utf-8') as f:
            input_data = json.load(f)
        
        results = []
        
        for i, item in enumerate(input_data):
            query = item.get('input', '')
            expected_output = item.get('output', '')
            
            print(f"\nå¤„ç†ç¬¬ {i+1}/{len(input_data)} ä¸ªæŸ¥è¯¢...")
            
            # å¤„ç†æŸ¥è¯¢
            result = self.process_single_query(query)
            
            # æ·»åŠ åŸå§‹æ•°æ®å’ŒæœŸæœ›è¾“å‡º
            result.update({
                'original_instruction': item.get('instruction', ''),
                'original_input': item.get('input', ''),
                'expected_output': expected_output,
                'index': i
            })
            
            results.append(result)
            
            # å®šæœŸä¿å­˜ä¸­é—´ç»“æœ
            if (i + 1) % 10 == 0:
                temp_output_path = output_json_path.replace('.json', f'_temp_{i+1}.json')
                with open(temp_output_path, 'w', encoding='utf-8') as f:
                    json.dump(results, f, ensure_ascii=False, indent=2)
                print(f"å·²ä¿å­˜ä¸­é—´ç»“æœåˆ°: {temp_output_path}")
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        with open(output_json_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\næ‰¹é‡æ¨ç†å®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ°: {output_json_path}")
        print(f"æ€»å…±å¤„ç†äº† {len(results)} ä¸ªæŸ¥è¯¢")


def main():
    # é…ç½®å‚æ•°
    PDF_FOLDER = "knowledge_base"  # PDFæ–‡ä»¶å¤¹è·¯å¾„
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    if not os.getenv("DASHSCOPE_API_KEY"):
        print("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEY")
        return
    
    print("RAG + LLM ç³»ç»Ÿé…ç½®:")
    print("- ä½¿ç”¨æ¨¡å‹: Qwen3-14B")
    print("- æ¨¡å‹ç¼“å­˜ç›®å½•: autodl-tmp/qwen")
    print("- æ–‡æœ¬å—å¤§å°: 500å­—ç¬¦")
    print("- æ£€ç´¢æ•°é‡: 10ä¸ªæ–‡æ¡£")
    print("- é‡æ’åºåä¿ç•™: 3ä¸ªæ–‡æ¡£")
    print()
    
    # åˆå§‹åŒ–RAGç³»ç»Ÿ
    rag_system = RAGSystem(
        chunk_size=500,
        chunk_overlap=50,
        top_k=10,
        rerank_top_k=3
    )
    
    # æ„å»ºçŸ¥è¯†åº“
    rag_system.build_knowledge_base(PDF_FOLDER)
    
    print("\n" + "="*60)
    print("RAG + LLM äº¤äº’å¼é—®ç­”ç³»ç»Ÿ")
    print("="*60)
    print("ç³»ç»Ÿå·²å‡†å¤‡å°±ç»ªï¼æ‚¨å¯ä»¥å¼€å§‹æé—®äº†ã€‚")
    print("è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡ºç³»ç»Ÿ")
    print("è¾“å…¥ 'help' æŸ¥çœ‹å¸®åŠ©ä¿¡æ¯")
    print("-"*60)
    
    # äº¤äº’å¼é—®ç­”å¾ªç¯
    while True:
        try:
            # è·å–ç”¨æˆ·è¾“å…¥
            query = input("\nè¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ").strip()
            
            # æ£€æŸ¥é€€å‡ºå‘½ä»¤
            if query.lower() in ['quit', 'exit', 'é€€å‡º', 'q']:
                print("æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼")
                break
            
            # æ£€æŸ¥å¸®åŠ©å‘½ä»¤
            if query.lower() in ['help', 'å¸®åŠ©', 'h']:
                print("\nå¸®åŠ©ä¿¡æ¯:")
                print("- ç›´æ¥è¾“å…¥é—®é¢˜å³å¯è·å¾—ç­”æ¡ˆ")
                print("- ç³»ç»Ÿä¼šè‡ªåŠ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£å¹¶ç”Ÿæˆå›ç­”")
                print("- è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡ºç³»ç»Ÿ")
                print("- è¾“å…¥ 'help' æŸ¥çœ‹æ­¤å¸®åŠ©ä¿¡æ¯")
                continue
            
            # æ£€æŸ¥ç©ºè¾“å…¥
            if not query:
                print("è¯·è¾“å…¥æœ‰æ•ˆçš„é—®é¢˜ã€‚")
                continue
            
            print(f"\næ­£åœ¨å¤„ç†æ‚¨çš„é—®é¢˜: {query}")
            print("-"*40)
            
            # å¤„ç†æŸ¥è¯¢
            result = rag_system.process_single_query(query)
            
            # æ˜¾ç¤ºæ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£
            print("\nğŸ“š æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£:")
            print("-"*40)
            if result['reranked_docs']:
                for i, doc in enumerate(result['reranked_docs']):
                    print(f"æ–‡æ¡£ {i+1} (ç›¸ä¼¼åº¦: {doc.get('score', 0):.4f}):")
                    # æˆªå–æ–‡æ¡£å†…å®¹çš„å‰200ä¸ªå­—ç¬¦
                    doc_preview = doc['text'][:200] + "..." if len(doc['text']) > 200 else doc['text']
                    print(f"  {doc_preview}")
                    print()
            else:
                print("æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
            
            # æ˜¾ç¤ºç”Ÿæˆçš„ç­”æ¡ˆ
            print("\nğŸ¤– AI ç”Ÿæˆçš„ç­”æ¡ˆ:")
            print("-"*40)
            if result['success']:
                print(result['answer'])
                
                # å¦‚æœå¯ç”¨äº†thinkingï¼Œæ˜¾ç¤ºæ¨ç†è¿‡ç¨‹
                if result.get('thinking'):
                    print(f"\nğŸ’­ æ¨ç†è¿‡ç¨‹:")
                    print("-"*20)
                    print(result['thinking'])
            else:
                print(f"âŒ ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºç°é”™è¯¯: {result['answer']}")
            
            print("\n" + "="*60)
            
        except KeyboardInterrupt:
            print("\n\næ£€æµ‹åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨é€€å‡º...")
            break
        except Exception as e:
            print(f"\nâŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            print("è¯·é‡è¯•æˆ–è”ç³»æŠ€æœ¯æ”¯æŒ")



if __name__ == "__main__":
    main()
