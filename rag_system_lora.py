import json
import os
import numpy as np
from typing import List, Dict, Any
import faiss
import pickle
from pathlib import Path
import re
import torch

# PDFå¤„ç†
import PyPDF2
from io import BytesIO

# æ¨¡å‹ç›¸å…³
from openai import OpenAI
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from sentence_transformers import CrossEncoder

# æ–‡æœ¬å¤„ç†
import jieba
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity


class LoRARAGSystem:
    def __init__(self, 
                 base_model_path="./Qwen/Qwen3-0.6B",
                 lora_checkpoint_path="checkpoint",
                 chunk_size=500,
                 chunk_overlap=50,
                 top_k=10,
                 rerank_top_k=3,
                 use_fast=False,
                 trust_remote_code=True):
        
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.top_k = top_k
        self.rerank_top_k = rerank_top_k
        self.base_model_path = base_model_path
        self.lora_checkpoint_path = lora_checkpoint_path
        
        # åˆå§‹åŒ–æœ¬åœ°LLM
        print("æ­£åœ¨åŠ è½½LoRAå¾®è°ƒæ¨¡å‹...")
        print(f"åŸºç¡€æ¨¡å‹è·¯å¾„: {base_model_path}")
        print(f"LoRA checkpointè·¯å¾„: {lora_checkpoint_path}")
        
        try:
            # åŠ è½½åŸä¸‹è½½è·¯å¾„çš„tokenizerå’Œmodel
            print("ğŸ“¥ åŠ è½½åŸºç¡€æ¨¡å‹...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                base_model_path, 
                use_fast=use_fast, 
                trust_remote_code=trust_remote_code
            )
            
            self.base_model = AutoModelForCausalLM.from_pretrained(
                base_model_path, 
                device_map="auto", 
                torch_dtype=torch.bfloat16
            )
            print("âœ… åŸºç¡€æ¨¡å‹åŠ è½½å®Œæˆ")
            
            # åŠ è½½LoRAæ¨¡å‹
            print("ğŸ“¥ åŠ è½½LoRAå¾®è°ƒæƒé‡...")
            self.model = PeftModel.from_pretrained(
                self.base_model, 
                model_id=lora_checkpoint_path
            )
            print("âœ… LoRAå¾®è°ƒæ¨¡å‹åŠ è½½å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("è¯·æ£€æŸ¥æ¨¡å‹è·¯å¾„å’Œæ–‡ä»¶å®Œæ•´æ€§")
            raise e
        
        # åˆå§‹åŒ–embeddingå®¢æˆ·ç«¯
        self.embedding_client = OpenAI(
            api_key=os.getenv("DASHSCOPE_API_KEY"),
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
        )
        self.embedding_model = "text-embedding-v4"
        
        # åˆå§‹åŒ–rerankæ¨¡å‹
        print("æ­£åœ¨åŠ è½½é‡æ’åºæ¨¡å‹...")
        try:
            self.reranker = CrossEncoder("BAAI/bge-reranker-base")
            print("âœ… é‡æ’åºæ¨¡å‹åŠ è½½å®Œæˆ")
        except:
            print("âš ï¸ é‡æ’åºæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ä½œä¸ºå¤‡é€‰")
            self.reranker = None
        
        # å‘é‡åº“ç›¸å…³
        self.index = None
        self.chunks = []
        self.chunk_embeddings = []
    
    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """ä»PDFæ–‡ä»¶ä¸­æå–æ–‡æœ¬"""
        text = ""
        try:
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                for page in pdf_reader.pages:
                    text += page.extract_text() + "\n"
        except Exception as e:
            print(f"è¯»å–PDFæ–‡ä»¶ {pdf_path} æ—¶å‡ºé”™: {e}")
        return text
    
    def split_text_into_chunks(self, text: str) -> List[str]:
        """å°†æ–‡æœ¬åˆ†å‰²æˆchunks"""
        # ç®€å•çš„æ–‡æœ¬åˆ†å‰²ç­–ç•¥
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]', text)
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
                
            if len(current_chunk) + len(sentence) <= self.chunk_size:
                current_chunk += sentence + "ã€‚"
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + "ã€‚"
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def get_embeddings(self, texts: List[str]) -> np.ndarray:
        """è·å–æ–‡æœ¬çš„embeddings"""
        embeddings = []
        batch_size = 10  # æ‰¹é‡å¤„ç†ä»¥é¿å…APIé™åˆ¶
        
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i+batch_size]
            try:
                response = self.embedding_client.embeddings.create(
                    model=self.embedding_model,
                    input=batch_texts,
                    dimensions=1024,
                    encoding_format="float"
                )
                
                batch_embeddings = [data.embedding for data in response.data]
                embeddings.extend(batch_embeddings)
                print(f"å·²å¤„ç† {min(i+batch_size, len(texts))}/{len(texts)} æ¡æ–‡æœ¬çš„embedding")
                
            except Exception as e:
                print(f"è·å–embeddingæ—¶å‡ºé”™: {e}")
                # å¦‚æœå‡ºé”™ï¼Œæ·»åŠ é›¶å‘é‡ä½œä¸ºå ä½ç¬¦
                for _ in batch_texts:
                    embeddings.append([0.0] * 1024)
        
        return np.array(embeddings)
    
    def build_knowledge_base(self, pdf_folder: str, save_path: str = "knowledge_base.pkl"):
        """æ„å»ºçŸ¥è¯†åº“"""
        print("æ­£åœ¨æ„å»ºçŸ¥è¯†åº“...")
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨çŸ¥è¯†åº“æ–‡ä»¶
        if os.path.exists(save_path):
            print("å‘ç°å·²å­˜åœ¨çš„çŸ¥è¯†åº“æ–‡ä»¶ï¼Œæ­£åœ¨åŠ è½½...")
            with open(save_path, 'rb') as f:
                data = pickle.load(f)
                self.chunks = data['chunks']
                self.chunk_embeddings = data['embeddings']
                self.index = data['index']
            print("âœ… çŸ¥è¯†åº“åŠ è½½å®Œæˆ")
            return
        
        # è¯»å–æ‰€æœ‰PDFæ–‡ä»¶
        all_text = ""
        pdf_files = list(Path(pdf_folder).glob("*.pdf"))
        
        if not pdf_files:
            print(f"âŒ åœ¨ {pdf_folder} ç›®å½•ä¸­æœªæ‰¾åˆ°PDFæ–‡ä»¶")
            return
        
        for pdf_file in pdf_files:
            print(f"æ­£åœ¨å¤„ç†: {pdf_file}")
            text = self.extract_text_from_pdf(str(pdf_file))
            all_text += f"\n\n=== {pdf_file.name} ===\n\n" + text
        
        # åˆ†å‰²æ–‡æœ¬
        print("æ­£åœ¨åˆ†å‰²æ–‡æœ¬...")
        self.chunks = self.split_text_into_chunks(all_text)
        print(f"å…±ç”Ÿæˆ {len(self.chunks)} ä¸ªæ–‡æœ¬å—")
        
        # ç”Ÿæˆembeddings
        print("æ­£åœ¨ç”Ÿæˆembeddings...")
        embeddings = self.get_embeddings(self.chunks)
        self.chunk_embeddings = embeddings
        
        # æ„å»ºFAISSç´¢å¼•
        print("æ­£åœ¨æ„å»ºFAISSç´¢å¼•...")
        dimension = embeddings.shape[1]
        self.index = faiss.IndexFlatIP(dimension)  # ä½¿ç”¨å†…ç§¯ç›¸ä¼¼åº¦
        
        # å½’ä¸€åŒ–embeddingsä»¥ä½¿ç”¨å†…ç§¯è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        normalized_embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)
        self.index.add(normalized_embeddings.astype(np.float32))
        
        # ä¿å­˜çŸ¥è¯†åº“
        print("æ­£åœ¨ä¿å­˜çŸ¥è¯†åº“...")
        with open(save_path, 'wb') as f:
            pickle.dump({
                'chunks': self.chunks,
                'embeddings': self.chunk_embeddings,
                'index': self.index
            }, f)
        
        print("âœ… çŸ¥è¯†åº“æ„å»ºå®Œæˆ")
    
    def retrieve_relevant_docs(self, query: str) -> List[Dict[str, Any]]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        # è·å–æŸ¥è¯¢çš„embedding
        query_embedding = self.get_embeddings([query])[0]
        query_embedding = query_embedding / np.linalg.norm(query_embedding)
        
        # æ£€ç´¢
        scores, indices = self.index.search(
            query_embedding.reshape(1, -1).astype(np.float32), 
            self.top_k
        )
        
        # æ•´ç†æ£€ç´¢ç»“æœ
        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(self.chunks):  # ç¡®ä¿ç´¢å¼•æœ‰æ•ˆ
                results.append({
                    'text': self.chunks[idx],
                    'score': float(score),
                    'index': int(idx)
                })
        
        return results
    
    def rerank_documents(self, query: str, documents: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """é‡æ’åºæ–‡æ¡£"""
        if not documents:
            return documents
        
        if self.reranker is not None:
            try:
                # ä½¿ç”¨CrossEncoderè¿›è¡Œé‡æ’åº
                pairs = [(query, doc['text']) for doc in documents]
                scores = self.reranker.predict(pairs)
                
                # æ›´æ–°åˆ†æ•°å¹¶æ’åº
                for i, score in enumerate(scores):
                    documents[i]['rerank_score'] = float(score)
                
                documents = sorted(documents, key=lambda x: x['rerank_score'], reverse=True)
            except Exception as e:
                print(f"é‡æ’åºå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹åˆ†æ•°: {e}")
        
        # è¿”å›top_kä¸ªç»“æœ
        return documents[:self.rerank_top_k]
    
    def generate_answer(self, query: str, context_docs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”Ÿæˆç­”æ¡ˆ"""
        # æ„å»ºä¸Šä¸‹æ–‡
        context = "\n\n".join([f"å‚è€ƒæ–‡æ¡£{i+1}:\n{doc['text']}" for i, doc in enumerate(context_docs)])
        
        # æ„å»ºprompt
        prompt = f"""åŸºäºä»¥ä¸‹å‚è€ƒæ–‡æ¡£å›ç­”é—®é¢˜ï¼Œå¦‚æœå‚è€ƒæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´æ˜æ— æ³•ä»æä¾›çš„æ–‡æ¡£ä¸­æ‰¾åˆ°ç­”æ¡ˆã€‚

å‚è€ƒæ–‡æ¡£:
{context}

é—®é¢˜: {query}

è¯·æä¾›å‡†ç¡®ã€è¯¦ç»†çš„ç­”æ¡ˆ:"""

        try:
            # å‡†å¤‡æ¨¡å‹è¾“å…¥
            messages = [{"role": "user", "content": prompt}]
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True,
                enable_thinking=True
            )
            model_inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)
            
            # ç”Ÿæˆç­”æ¡ˆ
            generated_ids = self.model.generate(
                **model_inputs,
                max_new_tokens=1024,  # ä½¿ç”¨æ›´å¤§çš„tokenæ•°é‡ï¼Œé€‚åˆ14Bæ¨¡å‹
                temperature=0.7,      # é€‚ä¸­çš„æ¸©åº¦
                do_sample=True,
                top_p=0.9,
                pad_token_id=self.tokenizer.eos_token_id
            )
            output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()
            
            # è§£æthinkingå†…å®¹
            try:
                index = len(output_ids) - output_ids[::-1].index(151668)
            except ValueError:
                index = 0
            
            thinking_content = self.tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip("\n")
            content = self.tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip("\n")
            
            return {
                'answer': content,
                'thinking': thinking_content,
                'context_docs': context_docs,
                'success': True
            }
            
        except Exception as e:
            print(f"ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™: {e}")
            return {
                'answer': f"ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºç°é”™è¯¯: {e}",
                'thinking': "",
                'context_docs': context_docs,
                'success': False
            }
    
    def process_single_query(self, query: str) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªæŸ¥è¯¢"""
        print(f"æ­£åœ¨å¤„ç†æŸ¥è¯¢: {query}")
        
        # æ£€ç´¢ç›¸å…³æ–‡æ¡£
        retrieved_docs = self.retrieve_relevant_docs(query)
        
        # é‡æ’åº
        reranked_docs = self.rerank_documents(query, retrieved_docs)
        
        # ç”Ÿæˆç­”æ¡ˆ
        result = self.generate_answer(query, reranked_docs)
        
        return {
            'query': query,
            'retrieved_docs': retrieved_docs,
            'reranked_docs': reranked_docs,
            'answer': result['answer'],
            'thinking': result['thinking'],
            'success': result['success']
        }
