{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 1000,
  "global_step": 50654,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0003948355509930114,
      "grad_norm": 0.5566816329956055,
      "learning_rate": 9.998223240020533e-05,
      "loss": 1.2587,
      "step": 10
    },
    {
      "epoch": 0.0007896711019860229,
      "grad_norm": 0.7906391024589539,
      "learning_rate": 9.996249062265566e-05,
      "loss": 1.083,
      "step": 20
    },
    {
      "epoch": 0.0011845066529790342,
      "grad_norm": 0.7832397222518921,
      "learning_rate": 9.994274884510601e-05,
      "loss": 0.9949,
      "step": 30
    },
    {
      "epoch": 0.0015793422039720457,
      "grad_norm": 0.8375154733657837,
      "learning_rate": 9.992300706755636e-05,
      "loss": 0.9431,
      "step": 40
    },
    {
      "epoch": 0.001974177754965057,
      "grad_norm": 0.7325529456138611,
      "learning_rate": 9.990326529000671e-05,
      "loss": 0.9263,
      "step": 50
    },
    {
      "epoch": 0.0023690133059580685,
      "grad_norm": 0.5340043902397156,
      "learning_rate": 9.988352351245706e-05,
      "loss": 0.9253,
      "step": 60
    },
    {
      "epoch": 0.0027638488569510797,
      "grad_norm": 1.0056209564208984,
      "learning_rate": 9.986378173490741e-05,
      "loss": 0.9395,
      "step": 70
    },
    {
      "epoch": 0.0031586844079440914,
      "grad_norm": 0.6373637914657593,
      "learning_rate": 9.984403995735776e-05,
      "loss": 0.9348,
      "step": 80
    },
    {
      "epoch": 0.0035535199589371027,
      "grad_norm": 0.5474101305007935,
      "learning_rate": 9.982429817980811e-05,
      "loss": 0.8776,
      "step": 90
    },
    {
      "epoch": 0.003948355509930114,
      "grad_norm": 0.5351986289024353,
      "learning_rate": 9.980455640225846e-05,
      "loss": 0.9212,
      "step": 100
    },
    {
      "epoch": 0.004343191060923126,
      "grad_norm": 0.5603713989257812,
      "learning_rate": 9.978481462470881e-05,
      "loss": 0.9347,
      "step": 110
    },
    {
      "epoch": 0.004738026611916137,
      "grad_norm": 0.7375014424324036,
      "learning_rate": 9.976507284715916e-05,
      "loss": 0.9258,
      "step": 120
    },
    {
      "epoch": 0.005132862162909148,
      "grad_norm": 0.6134978532791138,
      "learning_rate": 9.974533106960951e-05,
      "loss": 0.9309,
      "step": 130
    },
    {
      "epoch": 0.005527697713902159,
      "grad_norm": 0.7173211574554443,
      "learning_rate": 9.972558929205986e-05,
      "loss": 0.8994,
      "step": 140
    },
    {
      "epoch": 0.0059225332648951716,
      "grad_norm": 0.6397460103034973,
      "learning_rate": 9.970584751451021e-05,
      "loss": 0.8656,
      "step": 150
    },
    {
      "epoch": 0.006317368815888183,
      "grad_norm": 0.7462206482887268,
      "learning_rate": 9.968610573696055e-05,
      "loss": 0.8791,
      "step": 160
    },
    {
      "epoch": 0.006712204366881194,
      "grad_norm": 0.9492894411087036,
      "learning_rate": 9.966636395941091e-05,
      "loss": 0.9214,
      "step": 170
    },
    {
      "epoch": 0.007107039917874205,
      "grad_norm": 0.7690032720565796,
      "learning_rate": 9.964662218186126e-05,
      "loss": 0.8565,
      "step": 180
    },
    {
      "epoch": 0.007501875468867217,
      "grad_norm": 0.6421863436698914,
      "learning_rate": 9.962688040431161e-05,
      "loss": 0.9107,
      "step": 190
    },
    {
      "epoch": 0.007896711019860229,
      "grad_norm": 0.6356032490730286,
      "learning_rate": 9.960713862676195e-05,
      "loss": 0.9506,
      "step": 200
    },
    {
      "epoch": 0.00829154657085324,
      "grad_norm": 0.6001677513122559,
      "learning_rate": 9.958739684921231e-05,
      "loss": 0.8497,
      "step": 210
    },
    {
      "epoch": 0.008686382121846251,
      "grad_norm": 0.814132809638977,
      "learning_rate": 9.956765507166266e-05,
      "loss": 0.8527,
      "step": 220
    },
    {
      "epoch": 0.009081217672839263,
      "grad_norm": 0.8767328262329102,
      "learning_rate": 9.9547913294113e-05,
      "loss": 0.8983,
      "step": 230
    },
    {
      "epoch": 0.009476053223832274,
      "grad_norm": 0.6108408570289612,
      "learning_rate": 9.952817151656335e-05,
      "loss": 0.8746,
      "step": 240
    },
    {
      "epoch": 0.009870888774825285,
      "grad_norm": 0.5638062357902527,
      "learning_rate": 9.950842973901371e-05,
      "loss": 0.8832,
      "step": 250
    },
    {
      "epoch": 0.010265724325818296,
      "grad_norm": 0.7270739674568176,
      "learning_rate": 9.948868796146406e-05,
      "loss": 0.8994,
      "step": 260
    },
    {
      "epoch": 0.010660559876811308,
      "grad_norm": 0.847141444683075,
      "learning_rate": 9.94689461839144e-05,
      "loss": 0.8413,
      "step": 270
    },
    {
      "epoch": 0.011055395427804319,
      "grad_norm": 0.5558550953865051,
      "learning_rate": 9.944920440636475e-05,
      "loss": 0.8716,
      "step": 280
    },
    {
      "epoch": 0.01145023097879733,
      "grad_norm": 0.6758067607879639,
      "learning_rate": 9.942946262881511e-05,
      "loss": 0.9173,
      "step": 290
    },
    {
      "epoch": 0.011845066529790343,
      "grad_norm": 0.5938756465911865,
      "learning_rate": 9.940972085126545e-05,
      "loss": 0.9109,
      "step": 300
    },
    {
      "epoch": 0.012239902080783354,
      "grad_norm": 0.6522097587585449,
      "learning_rate": 9.93899790737158e-05,
      "loss": 0.9015,
      "step": 310
    },
    {
      "epoch": 0.012634737631776366,
      "grad_norm": 0.710902214050293,
      "learning_rate": 9.937023729616615e-05,
      "loss": 0.8807,
      "step": 320
    },
    {
      "epoch": 0.013029573182769377,
      "grad_norm": 0.5241168141365051,
      "learning_rate": 9.93504955186165e-05,
      "loss": 0.873,
      "step": 330
    },
    {
      "epoch": 0.013424408733762388,
      "grad_norm": 0.6543934941291809,
      "learning_rate": 9.933075374106685e-05,
      "loss": 0.9208,
      "step": 340
    },
    {
      "epoch": 0.0138192442847554,
      "grad_norm": 0.760662317276001,
      "learning_rate": 9.93110119635172e-05,
      "loss": 0.8866,
      "step": 350
    },
    {
      "epoch": 0.01421407983574841,
      "grad_norm": 0.6868493556976318,
      "learning_rate": 9.929127018596755e-05,
      "loss": 0.8491,
      "step": 360
    },
    {
      "epoch": 0.014608915386741422,
      "grad_norm": 0.6783254742622375,
      "learning_rate": 9.92715284084179e-05,
      "loss": 0.8619,
      "step": 370
    },
    {
      "epoch": 0.015003750937734433,
      "grad_norm": 0.6745755076408386,
      "learning_rate": 9.925178663086825e-05,
      "loss": 0.8731,
      "step": 380
    },
    {
      "epoch": 0.015398586488727444,
      "grad_norm": 0.632879912853241,
      "learning_rate": 9.92320448533186e-05,
      "loss": 0.916,
      "step": 390
    },
    {
      "epoch": 0.015793422039720457,
      "grad_norm": 0.8932765126228333,
      "learning_rate": 9.921230307576895e-05,
      "loss": 0.9086,
      "step": 400
    },
    {
      "epoch": 0.01618825759071347,
      "grad_norm": 0.756763219833374,
      "learning_rate": 9.91925612982193e-05,
      "loss": 0.8926,
      "step": 410
    },
    {
      "epoch": 0.01658309314170648,
      "grad_norm": 0.8449229598045349,
      "learning_rate": 9.917281952066965e-05,
      "loss": 0.9461,
      "step": 420
    },
    {
      "epoch": 0.01697792869269949,
      "grad_norm": 0.7328140139579773,
      "learning_rate": 9.915307774312e-05,
      "loss": 0.8518,
      "step": 430
    },
    {
      "epoch": 0.017372764243692503,
      "grad_norm": 0.8752796053886414,
      "learning_rate": 9.913333596557033e-05,
      "loss": 0.8742,
      "step": 440
    },
    {
      "epoch": 0.017767599794685514,
      "grad_norm": 0.6155121326446533,
      "learning_rate": 9.91135941880207e-05,
      "loss": 0.8729,
      "step": 450
    },
    {
      "epoch": 0.018162435345678525,
      "grad_norm": 0.686145544052124,
      "learning_rate": 9.909385241047105e-05,
      "loss": 0.8779,
      "step": 460
    },
    {
      "epoch": 0.018557270896671536,
      "grad_norm": 0.5390613079071045,
      "learning_rate": 9.90741106329214e-05,
      "loss": 0.883,
      "step": 470
    },
    {
      "epoch": 0.018952106447664548,
      "grad_norm": 0.5690732598304749,
      "learning_rate": 9.905436885537173e-05,
      "loss": 0.8742,
      "step": 480
    },
    {
      "epoch": 0.01934694199865756,
      "grad_norm": 0.6047543287277222,
      "learning_rate": 9.90346270778221e-05,
      "loss": 0.8996,
      "step": 490
    },
    {
      "epoch": 0.01974177754965057,
      "grad_norm": 0.7472992539405823,
      "learning_rate": 9.901488530027245e-05,
      "loss": 0.9035,
      "step": 500
    },
    {
      "epoch": 0.02013661310064358,
      "grad_norm": 0.6879084706306458,
      "learning_rate": 9.899514352272278e-05,
      "loss": 0.8732,
      "step": 510
    },
    {
      "epoch": 0.020531448651636593,
      "grad_norm": 0.6190508604049683,
      "learning_rate": 9.897540174517313e-05,
      "loss": 0.896,
      "step": 520
    },
    {
      "epoch": 0.020926284202629604,
      "grad_norm": 0.6462380886077881,
      "learning_rate": 9.89556599676235e-05,
      "loss": 0.8371,
      "step": 530
    },
    {
      "epoch": 0.021321119753622615,
      "grad_norm": 0.7310876250267029,
      "learning_rate": 9.893591819007385e-05,
      "loss": 0.8649,
      "step": 540
    },
    {
      "epoch": 0.021715955304615626,
      "grad_norm": 0.6907662749290466,
      "learning_rate": 9.891617641252418e-05,
      "loss": 0.8484,
      "step": 550
    },
    {
      "epoch": 0.022110790855608638,
      "grad_norm": 0.6441212296485901,
      "learning_rate": 9.889643463497453e-05,
      "loss": 0.8794,
      "step": 560
    },
    {
      "epoch": 0.02250562640660165,
      "grad_norm": 0.6634873747825623,
      "learning_rate": 9.88766928574249e-05,
      "loss": 0.848,
      "step": 570
    },
    {
      "epoch": 0.02290046195759466,
      "grad_norm": 0.5352246165275574,
      "learning_rate": 9.885695107987523e-05,
      "loss": 0.8675,
      "step": 580
    },
    {
      "epoch": 0.02329529750858767,
      "grad_norm": 0.6362677812576294,
      "learning_rate": 9.883720930232558e-05,
      "loss": 0.8483,
      "step": 590
    },
    {
      "epoch": 0.023690133059580686,
      "grad_norm": 0.5668668150901794,
      "learning_rate": 9.881746752477593e-05,
      "loss": 0.8462,
      "step": 600
    },
    {
      "epoch": 0.024084968610573698,
      "grad_norm": 0.6031479239463806,
      "learning_rate": 9.879772574722628e-05,
      "loss": 0.8716,
      "step": 610
    },
    {
      "epoch": 0.02447980416156671,
      "grad_norm": 0.7123826146125793,
      "learning_rate": 9.877798396967663e-05,
      "loss": 0.8728,
      "step": 620
    },
    {
      "epoch": 0.02487463971255972,
      "grad_norm": 0.635829746723175,
      "learning_rate": 9.875824219212698e-05,
      "loss": 0.9025,
      "step": 630
    },
    {
      "epoch": 0.02526947526355273,
      "grad_norm": 0.5916152000427246,
      "learning_rate": 9.873850041457733e-05,
      "loss": 0.7916,
      "step": 640
    },
    {
      "epoch": 0.025664310814545743,
      "grad_norm": 0.573729395866394,
      "learning_rate": 9.871875863702768e-05,
      "loss": 0.8209,
      "step": 650
    },
    {
      "epoch": 0.026059146365538754,
      "grad_norm": 0.7321687936782837,
      "learning_rate": 9.869901685947803e-05,
      "loss": 0.9011,
      "step": 660
    },
    {
      "epoch": 0.026453981916531765,
      "grad_norm": 0.5767333507537842,
      "learning_rate": 9.867927508192838e-05,
      "loss": 0.824,
      "step": 670
    },
    {
      "epoch": 0.026848817467524776,
      "grad_norm": 0.5836392045021057,
      "learning_rate": 9.865953330437873e-05,
      "loss": 0.8801,
      "step": 680
    },
    {
      "epoch": 0.027243653018517788,
      "grad_norm": 0.5016250014305115,
      "learning_rate": 9.863979152682908e-05,
      "loss": 0.8097,
      "step": 690
    },
    {
      "epoch": 0.0276384885695108,
      "grad_norm": 0.6059651970863342,
      "learning_rate": 9.862004974927943e-05,
      "loss": 0.8663,
      "step": 700
    },
    {
      "epoch": 0.02803332412050381,
      "grad_norm": 0.7871519923210144,
      "learning_rate": 9.860030797172978e-05,
      "loss": 0.8543,
      "step": 710
    },
    {
      "epoch": 0.02842815967149682,
      "grad_norm": 0.6582422852516174,
      "learning_rate": 9.858056619418012e-05,
      "loss": 0.8392,
      "step": 720
    },
    {
      "epoch": 0.028822995222489833,
      "grad_norm": 0.73605877161026,
      "learning_rate": 9.856082441663048e-05,
      "loss": 0.8457,
      "step": 730
    },
    {
      "epoch": 0.029217830773482844,
      "grad_norm": 0.6576009392738342,
      "learning_rate": 9.854108263908083e-05,
      "loss": 0.8946,
      "step": 740
    },
    {
      "epoch": 0.029612666324475855,
      "grad_norm": 0.689483106136322,
      "learning_rate": 9.852134086153118e-05,
      "loss": 0.8391,
      "step": 750
    },
    {
      "epoch": 0.030007501875468866,
      "grad_norm": 0.6576967835426331,
      "learning_rate": 9.850159908398152e-05,
      "loss": 0.8403,
      "step": 760
    },
    {
      "epoch": 0.030402337426461878,
      "grad_norm": 0.6839632391929626,
      "learning_rate": 9.848185730643188e-05,
      "loss": 0.8376,
      "step": 770
    },
    {
      "epoch": 0.03079717297745489,
      "grad_norm": 0.66254061460495,
      "learning_rate": 9.846211552888223e-05,
      "loss": 0.8455,
      "step": 780
    },
    {
      "epoch": 0.0311920085284479,
      "grad_norm": 0.540894091129303,
      "learning_rate": 9.844237375133257e-05,
      "loss": 0.8851,
      "step": 790
    },
    {
      "epoch": 0.031586844079440915,
      "grad_norm": 0.694587767124176,
      "learning_rate": 9.842263197378292e-05,
      "loss": 0.824,
      "step": 800
    },
    {
      "epoch": 0.031981679630433926,
      "grad_norm": 0.662779688835144,
      "learning_rate": 9.840289019623328e-05,
      "loss": 0.8717,
      "step": 810
    },
    {
      "epoch": 0.03237651518142694,
      "grad_norm": 0.7712631821632385,
      "learning_rate": 9.838314841868363e-05,
      "loss": 0.8147,
      "step": 820
    },
    {
      "epoch": 0.03277135073241995,
      "grad_norm": 0.5544354915618896,
      "learning_rate": 9.836340664113397e-05,
      "loss": 0.8321,
      "step": 830
    },
    {
      "epoch": 0.03316618628341296,
      "grad_norm": 0.7930859923362732,
      "learning_rate": 9.834366486358432e-05,
      "loss": 0.8458,
      "step": 840
    },
    {
      "epoch": 0.03356102183440597,
      "grad_norm": 0.6090320944786072,
      "learning_rate": 9.832392308603468e-05,
      "loss": 0.8217,
      "step": 850
    },
    {
      "epoch": 0.03395585738539898,
      "grad_norm": 0.6090010404586792,
      "learning_rate": 9.830418130848502e-05,
      "loss": 0.8466,
      "step": 860
    },
    {
      "epoch": 0.034350692936391994,
      "grad_norm": 0.7559329867362976,
      "learning_rate": 9.828443953093537e-05,
      "loss": 0.8378,
      "step": 870
    },
    {
      "epoch": 0.034745528487385005,
      "grad_norm": 0.6812012195587158,
      "learning_rate": 9.826469775338572e-05,
      "loss": 0.8741,
      "step": 880
    },
    {
      "epoch": 0.035140364038378016,
      "grad_norm": 0.688863217830658,
      "learning_rate": 9.824495597583607e-05,
      "loss": 0.8458,
      "step": 890
    },
    {
      "epoch": 0.03553519958937103,
      "grad_norm": 0.6118738651275635,
      "learning_rate": 9.822521419828642e-05,
      "loss": 0.8521,
      "step": 900
    },
    {
      "epoch": 0.03593003514036404,
      "grad_norm": 0.7275187373161316,
      "learning_rate": 9.820547242073677e-05,
      "loss": 0.8004,
      "step": 910
    },
    {
      "epoch": 0.03632487069135705,
      "grad_norm": 0.7126655578613281,
      "learning_rate": 9.818573064318712e-05,
      "loss": 0.8595,
      "step": 920
    },
    {
      "epoch": 0.03671970624235006,
      "grad_norm": 0.5439218282699585,
      "learning_rate": 9.816598886563747e-05,
      "loss": 0.8846,
      "step": 930
    },
    {
      "epoch": 0.03711454179334307,
      "grad_norm": 0.7043747305870056,
      "learning_rate": 9.814624708808782e-05,
      "loss": 0.8849,
      "step": 940
    },
    {
      "epoch": 0.037509377344336084,
      "grad_norm": 0.660592257976532,
      "learning_rate": 9.812650531053817e-05,
      "loss": 0.8973,
      "step": 950
    },
    {
      "epoch": 0.037904212895329095,
      "grad_norm": 0.7400151491165161,
      "learning_rate": 9.810676353298852e-05,
      "loss": 0.8652,
      "step": 960
    },
    {
      "epoch": 0.038299048446322106,
      "grad_norm": 0.6135134696960449,
      "learning_rate": 9.808702175543887e-05,
      "loss": 0.8473,
      "step": 970
    },
    {
      "epoch": 0.03869388399731512,
      "grad_norm": 0.7268866896629333,
      "learning_rate": 9.806727997788922e-05,
      "loss": 0.8583,
      "step": 980
    },
    {
      "epoch": 0.03908871954830813,
      "grad_norm": 0.8893859386444092,
      "learning_rate": 9.804753820033957e-05,
      "loss": 0.8887,
      "step": 990
    },
    {
      "epoch": 0.03948355509930114,
      "grad_norm": 0.7064917087554932,
      "learning_rate": 9.80277964227899e-05,
      "loss": 0.8693,
      "step": 1000
    },
    {
      "epoch": 0.03948355509930114,
      "eval_loss": 0.8704863786697388,
      "eval_runtime": 1217.2797,
      "eval_samples_per_second": 9.248,
      "eval_steps_per_second": 9.248,
      "step": 1000
    },
    {
      "epoch": 0.03987839065029415,
      "grad_norm": 0.6520010828971863,
      "learning_rate": 9.800805464524027e-05,
      "loss": 0.8591,
      "step": 1010
    },
    {
      "epoch": 0.04027322620128716,
      "grad_norm": 0.7566524744033813,
      "learning_rate": 9.798831286769061e-05,
      "loss": 0.8501,
      "step": 1020
    },
    {
      "epoch": 0.040668061752280174,
      "grad_norm": 0.7185056805610657,
      "learning_rate": 9.796857109014096e-05,
      "loss": 0.846,
      "step": 1030
    },
    {
      "epoch": 0.041062897303273185,
      "grad_norm": 0.5200325846672058,
      "learning_rate": 9.79488293125913e-05,
      "loss": 0.8705,
      "step": 1040
    },
    {
      "epoch": 0.0414577328542662,
      "grad_norm": 0.5214594602584839,
      "learning_rate": 9.792908753504166e-05,
      "loss": 0.8759,
      "step": 1050
    },
    {
      "epoch": 0.04185256840525921,
      "grad_norm": 0.6033464670181274,
      "learning_rate": 9.790934575749201e-05,
      "loss": 0.8185,
      "step": 1060
    },
    {
      "epoch": 0.04224740395625222,
      "grad_norm": 0.6505836844444275,
      "learning_rate": 9.788960397994235e-05,
      "loss": 0.8944,
      "step": 1070
    },
    {
      "epoch": 0.04264223950724523,
      "grad_norm": 0.6162398457527161,
      "learning_rate": 9.78698622023927e-05,
      "loss": 0.87,
      "step": 1080
    },
    {
      "epoch": 0.04303707505823824,
      "grad_norm": 0.539269745349884,
      "learning_rate": 9.785012042484306e-05,
      "loss": 0.8398,
      "step": 1090
    },
    {
      "epoch": 0.04343191060923125,
      "grad_norm": 0.7029319405555725,
      "learning_rate": 9.783037864729341e-05,
      "loss": 0.8364,
      "step": 1100
    },
    {
      "epoch": 0.043826746160224264,
      "grad_norm": 0.7377690672874451,
      "learning_rate": 9.781063686974375e-05,
      "loss": 0.8123,
      "step": 1110
    },
    {
      "epoch": 0.044221581711217275,
      "grad_norm": 0.6508048176765442,
      "learning_rate": 9.77908950921941e-05,
      "loss": 0.8918,
      "step": 1120
    },
    {
      "epoch": 0.04461641726221029,
      "grad_norm": 0.509656548500061,
      "learning_rate": 9.777115331464446e-05,
      "loss": 0.8877,
      "step": 1130
    },
    {
      "epoch": 0.0450112528132033,
      "grad_norm": 0.651125431060791,
      "learning_rate": 9.77514115370948e-05,
      "loss": 0.8006,
      "step": 1140
    },
    {
      "epoch": 0.04540608836419631,
      "grad_norm": 0.609565258026123,
      "learning_rate": 9.773166975954515e-05,
      "loss": 0.8953,
      "step": 1150
    },
    {
      "epoch": 0.04580092391518932,
      "grad_norm": 0.6127967834472656,
      "learning_rate": 9.77119279819955e-05,
      "loss": 0.8407,
      "step": 1160
    },
    {
      "epoch": 0.04619575946618233,
      "grad_norm": 0.6013462543487549,
      "learning_rate": 9.769218620444586e-05,
      "loss": 0.8046,
      "step": 1170
    },
    {
      "epoch": 0.04659059501717534,
      "grad_norm": 0.7497217059135437,
      "learning_rate": 9.76724444268962e-05,
      "loss": 0.8129,
      "step": 1180
    },
    {
      "epoch": 0.04698543056816836,
      "grad_norm": 0.6068310737609863,
      "learning_rate": 9.765270264934655e-05,
      "loss": 0.859,
      "step": 1190
    },
    {
      "epoch": 0.04738026611916137,
      "grad_norm": 0.6204031705856323,
      "learning_rate": 9.76329608717969e-05,
      "loss": 0.8239,
      "step": 1200
    },
    {
      "epoch": 0.047775101670154384,
      "grad_norm": 0.5705002546310425,
      "learning_rate": 9.761321909424725e-05,
      "loss": 0.8195,
      "step": 1210
    },
    {
      "epoch": 0.048169937221147395,
      "grad_norm": 0.5486728549003601,
      "learning_rate": 9.75934773166976e-05,
      "loss": 0.8323,
      "step": 1220
    },
    {
      "epoch": 0.048564772772140406,
      "grad_norm": 0.5937260985374451,
      "learning_rate": 9.757373553914795e-05,
      "loss": 0.813,
      "step": 1230
    },
    {
      "epoch": 0.04895960832313342,
      "grad_norm": 0.6382086277008057,
      "learning_rate": 9.75539937615983e-05,
      "loss": 0.7986,
      "step": 1240
    },
    {
      "epoch": 0.04935444387412643,
      "grad_norm": 0.6447972059249878,
      "learning_rate": 9.753425198404865e-05,
      "loss": 0.821,
      "step": 1250
    },
    {
      "epoch": 0.04974927942511944,
      "grad_norm": 0.6123823523521423,
      "learning_rate": 9.7514510206499e-05,
      "loss": 0.8219,
      "step": 1260
    },
    {
      "epoch": 0.05014411497611245,
      "grad_norm": 0.7812431454658508,
      "learning_rate": 9.749476842894935e-05,
      "loss": 0.8235,
      "step": 1270
    },
    {
      "epoch": 0.05053895052710546,
      "grad_norm": 0.7687551379203796,
      "learning_rate": 9.747502665139969e-05,
      "loss": 0.8161,
      "step": 1280
    },
    {
      "epoch": 0.050933786078098474,
      "grad_norm": 0.6917974948883057,
      "learning_rate": 9.745528487385005e-05,
      "loss": 0.8492,
      "step": 1290
    },
    {
      "epoch": 0.051328621629091485,
      "grad_norm": 0.7391928434371948,
      "learning_rate": 9.74355430963004e-05,
      "loss": 0.8231,
      "step": 1300
    },
    {
      "epoch": 0.051723457180084496,
      "grad_norm": 0.6752448081970215,
      "learning_rate": 9.741580131875075e-05,
      "loss": 0.8322,
      "step": 1310
    },
    {
      "epoch": 0.05211829273107751,
      "grad_norm": 0.6129867434501648,
      "learning_rate": 9.739605954120109e-05,
      "loss": 0.8846,
      "step": 1320
    },
    {
      "epoch": 0.05251312828207052,
      "grad_norm": 0.6036114692687988,
      "learning_rate": 9.737631776365145e-05,
      "loss": 0.8454,
      "step": 1330
    },
    {
      "epoch": 0.05290796383306353,
      "grad_norm": 0.7969687581062317,
      "learning_rate": 9.73565759861018e-05,
      "loss": 0.8462,
      "step": 1340
    },
    {
      "epoch": 0.05330279938405654,
      "grad_norm": 0.9954695701599121,
      "learning_rate": 9.733683420855213e-05,
      "loss": 0.8975,
      "step": 1350
    },
    {
      "epoch": 0.05369763493504955,
      "grad_norm": 0.6133082509040833,
      "learning_rate": 9.731709243100248e-05,
      "loss": 0.8213,
      "step": 1360
    },
    {
      "epoch": 0.054092470486042564,
      "grad_norm": 0.7185629606246948,
      "learning_rate": 9.729735065345285e-05,
      "loss": 0.7988,
      "step": 1370
    },
    {
      "epoch": 0.054487306037035575,
      "grad_norm": 0.6024441123008728,
      "learning_rate": 9.72776088759032e-05,
      "loss": 0.7911,
      "step": 1380
    },
    {
      "epoch": 0.054882141588028586,
      "grad_norm": 0.6872857809066772,
      "learning_rate": 9.725786709835353e-05,
      "loss": 0.8938,
      "step": 1390
    },
    {
      "epoch": 0.0552769771390216,
      "grad_norm": 0.6034238934516907,
      "learning_rate": 9.723812532080388e-05,
      "loss": 0.7977,
      "step": 1400
    },
    {
      "epoch": 0.05567181269001461,
      "grad_norm": 0.6228136420249939,
      "learning_rate": 9.721838354325425e-05,
      "loss": 0.7905,
      "step": 1410
    },
    {
      "epoch": 0.05606664824100762,
      "grad_norm": 0.6882700324058533,
      "learning_rate": 9.719864176570458e-05,
      "loss": 0.8407,
      "step": 1420
    },
    {
      "epoch": 0.05646148379200063,
      "grad_norm": 0.7307921051979065,
      "learning_rate": 9.717889998815493e-05,
      "loss": 0.9073,
      "step": 1430
    },
    {
      "epoch": 0.05685631934299364,
      "grad_norm": 0.6158947348594666,
      "learning_rate": 9.715915821060528e-05,
      "loss": 0.8087,
      "step": 1440
    },
    {
      "epoch": 0.057251154893986654,
      "grad_norm": 0.641974687576294,
      "learning_rate": 9.713941643305565e-05,
      "loss": 0.7852,
      "step": 1450
    },
    {
      "epoch": 0.057645990444979665,
      "grad_norm": 0.6349685192108154,
      "learning_rate": 9.711967465550598e-05,
      "loss": 0.8345,
      "step": 1460
    },
    {
      "epoch": 0.05804082599597268,
      "grad_norm": 0.5806257724761963,
      "learning_rate": 9.709993287795633e-05,
      "loss": 0.8329,
      "step": 1470
    },
    {
      "epoch": 0.05843566154696569,
      "grad_norm": 0.6716667413711548,
      "learning_rate": 9.708019110040668e-05,
      "loss": 0.8371,
      "step": 1480
    },
    {
      "epoch": 0.0588304970979587,
      "grad_norm": 0.5664283037185669,
      "learning_rate": 9.706044932285703e-05,
      "loss": 0.8444,
      "step": 1490
    },
    {
      "epoch": 0.05922533264895171,
      "grad_norm": 0.5555499196052551,
      "learning_rate": 9.704070754530738e-05,
      "loss": 0.8869,
      "step": 1500
    },
    {
      "epoch": 0.05962016819994472,
      "grad_norm": 0.5817026495933533,
      "learning_rate": 9.702096576775773e-05,
      "loss": 0.7866,
      "step": 1510
    },
    {
      "epoch": 0.06001500375093773,
      "grad_norm": 0.7645900249481201,
      "learning_rate": 9.700122399020808e-05,
      "loss": 0.8201,
      "step": 1520
    },
    {
      "epoch": 0.060409839301930744,
      "grad_norm": 0.7200111150741577,
      "learning_rate": 9.698148221265843e-05,
      "loss": 0.8477,
      "step": 1530
    },
    {
      "epoch": 0.060804674852923755,
      "grad_norm": 0.6903321743011475,
      "learning_rate": 9.696174043510878e-05,
      "loss": 0.8572,
      "step": 1540
    },
    {
      "epoch": 0.06119951040391677,
      "grad_norm": 0.5333318114280701,
      "learning_rate": 9.694199865755913e-05,
      "loss": 0.8002,
      "step": 1550
    },
    {
      "epoch": 0.06159434595490978,
      "grad_norm": 0.6438504457473755,
      "learning_rate": 9.692225688000948e-05,
      "loss": 0.8269,
      "step": 1560
    },
    {
      "epoch": 0.06198918150590279,
      "grad_norm": 0.6086890697479248,
      "learning_rate": 9.690251510245983e-05,
      "loss": 0.8779,
      "step": 1570
    },
    {
      "epoch": 0.0623840170568958,
      "grad_norm": 0.5909734964370728,
      "learning_rate": 9.688277332491018e-05,
      "loss": 0.886,
      "step": 1580
    },
    {
      "epoch": 0.06277885260788882,
      "grad_norm": 0.7104342579841614,
      "learning_rate": 9.686303154736053e-05,
      "loss": 0.8309,
      "step": 1590
    },
    {
      "epoch": 0.06317368815888183,
      "grad_norm": 0.5664840936660767,
      "learning_rate": 9.684328976981087e-05,
      "loss": 0.7989,
      "step": 1600
    },
    {
      "epoch": 0.06356852370987484,
      "grad_norm": 0.6485583782196045,
      "learning_rate": 9.682354799226123e-05,
      "loss": 0.8413,
      "step": 1610
    },
    {
      "epoch": 0.06396335926086785,
      "grad_norm": 0.7071617841720581,
      "learning_rate": 9.680380621471158e-05,
      "loss": 0.8536,
      "step": 1620
    },
    {
      "epoch": 0.06435819481186086,
      "grad_norm": 0.8348527550697327,
      "learning_rate": 9.678406443716192e-05,
      "loss": 0.8617,
      "step": 1630
    },
    {
      "epoch": 0.06475303036285388,
      "grad_norm": 0.6451224684715271,
      "learning_rate": 9.676432265961227e-05,
      "loss": 0.8497,
      "step": 1640
    },
    {
      "epoch": 0.06514786591384689,
      "grad_norm": 0.5892018675804138,
      "learning_rate": 9.674458088206263e-05,
      "loss": 0.839,
      "step": 1650
    },
    {
      "epoch": 0.0655427014648399,
      "grad_norm": 0.6321547627449036,
      "learning_rate": 9.672483910451298e-05,
      "loss": 0.8529,
      "step": 1660
    },
    {
      "epoch": 0.06593753701583291,
      "grad_norm": 0.8170319199562073,
      "learning_rate": 9.670509732696332e-05,
      "loss": 0.8387,
      "step": 1670
    },
    {
      "epoch": 0.06633237256682592,
      "grad_norm": 0.6632664799690247,
      "learning_rate": 9.668535554941367e-05,
      "loss": 0.8301,
      "step": 1680
    },
    {
      "epoch": 0.06672720811781893,
      "grad_norm": 0.6309064030647278,
      "learning_rate": 9.666561377186403e-05,
      "loss": 0.8625,
      "step": 1690
    },
    {
      "epoch": 0.06712204366881194,
      "grad_norm": 0.861350953578949,
      "learning_rate": 9.664587199431437e-05,
      "loss": 0.83,
      "step": 1700
    },
    {
      "epoch": 0.06751687921980495,
      "grad_norm": 0.6213839650154114,
      "learning_rate": 9.662613021676472e-05,
      "loss": 0.8094,
      "step": 1710
    },
    {
      "epoch": 0.06791171477079797,
      "grad_norm": 0.5547122955322266,
      "learning_rate": 9.660638843921507e-05,
      "loss": 0.8329,
      "step": 1720
    },
    {
      "epoch": 0.06830655032179098,
      "grad_norm": 0.6935812830924988,
      "learning_rate": 9.658664666166543e-05,
      "loss": 0.8435,
      "step": 1730
    },
    {
      "epoch": 0.06870138587278399,
      "grad_norm": 0.712062418460846,
      "learning_rate": 9.656690488411577e-05,
      "loss": 0.8124,
      "step": 1740
    },
    {
      "epoch": 0.069096221423777,
      "grad_norm": 0.653963565826416,
      "learning_rate": 9.654716310656612e-05,
      "loss": 0.8864,
      "step": 1750
    },
    {
      "epoch": 0.06949105697477001,
      "grad_norm": 0.6170716881752014,
      "learning_rate": 9.652742132901647e-05,
      "loss": 0.8243,
      "step": 1760
    },
    {
      "epoch": 0.06988589252576302,
      "grad_norm": 0.6580237150192261,
      "learning_rate": 9.650767955146682e-05,
      "loss": 0.8472,
      "step": 1770
    },
    {
      "epoch": 0.07028072807675603,
      "grad_norm": 0.6669613122940063,
      "learning_rate": 9.648793777391717e-05,
      "loss": 0.8022,
      "step": 1780
    },
    {
      "epoch": 0.07067556362774904,
      "grad_norm": 0.9394167065620422,
      "learning_rate": 9.646819599636752e-05,
      "loss": 0.8045,
      "step": 1790
    },
    {
      "epoch": 0.07107039917874206,
      "grad_norm": 0.7106250524520874,
      "learning_rate": 9.644845421881787e-05,
      "loss": 0.8397,
      "step": 1800
    },
    {
      "epoch": 0.07146523472973507,
      "grad_norm": 0.6142524480819702,
      "learning_rate": 9.642871244126822e-05,
      "loss": 0.8122,
      "step": 1810
    },
    {
      "epoch": 0.07186007028072808,
      "grad_norm": 0.5997885465621948,
      "learning_rate": 9.640897066371857e-05,
      "loss": 0.8332,
      "step": 1820
    },
    {
      "epoch": 0.07225490583172109,
      "grad_norm": 0.5637389421463013,
      "learning_rate": 9.638922888616892e-05,
      "loss": 0.807,
      "step": 1830
    },
    {
      "epoch": 0.0726497413827141,
      "grad_norm": 0.6571172475814819,
      "learning_rate": 9.636948710861927e-05,
      "loss": 0.8198,
      "step": 1840
    },
    {
      "epoch": 0.07304457693370711,
      "grad_norm": 0.6987625360488892,
      "learning_rate": 9.634974533106962e-05,
      "loss": 0.887,
      "step": 1850
    },
    {
      "epoch": 0.07343941248470012,
      "grad_norm": 0.6378837823867798,
      "learning_rate": 9.633000355351997e-05,
      "loss": 0.8308,
      "step": 1860
    },
    {
      "epoch": 0.07383424803569313,
      "grad_norm": 0.6410375237464905,
      "learning_rate": 9.631026177597032e-05,
      "loss": 0.8464,
      "step": 1870
    },
    {
      "epoch": 0.07422908358668615,
      "grad_norm": 0.849456787109375,
      "learning_rate": 9.629051999842065e-05,
      "loss": 0.8306,
      "step": 1880
    },
    {
      "epoch": 0.07462391913767916,
      "grad_norm": 0.5570815205574036,
      "learning_rate": 9.627077822087102e-05,
      "loss": 0.8131,
      "step": 1890
    },
    {
      "epoch": 0.07501875468867217,
      "grad_norm": 0.5491759181022644,
      "learning_rate": 9.625103644332137e-05,
      "loss": 0.8505,
      "step": 1900
    },
    {
      "epoch": 0.07541359023966518,
      "grad_norm": 0.7113925814628601,
      "learning_rate": 9.62312946657717e-05,
      "loss": 0.8626,
      "step": 1910
    },
    {
      "epoch": 0.07580842579065819,
      "grad_norm": 0.6080659627914429,
      "learning_rate": 9.621155288822205e-05,
      "loss": 0.7976,
      "step": 1920
    },
    {
      "epoch": 0.0762032613416512,
      "grad_norm": 0.7992324233055115,
      "learning_rate": 9.619181111067242e-05,
      "loss": 0.8232,
      "step": 1930
    },
    {
      "epoch": 0.07659809689264421,
      "grad_norm": 0.6558125615119934,
      "learning_rate": 9.617206933312277e-05,
      "loss": 0.831,
      "step": 1940
    },
    {
      "epoch": 0.07699293244363722,
      "grad_norm": 0.6389004588127136,
      "learning_rate": 9.61523275555731e-05,
      "loss": 0.7799,
      "step": 1950
    },
    {
      "epoch": 0.07738776799463024,
      "grad_norm": 0.6738341450691223,
      "learning_rate": 9.613258577802345e-05,
      "loss": 0.8501,
      "step": 1960
    },
    {
      "epoch": 0.07778260354562325,
      "grad_norm": 0.7943444848060608,
      "learning_rate": 9.611284400047382e-05,
      "loss": 0.8803,
      "step": 1970
    },
    {
      "epoch": 0.07817743909661626,
      "grad_norm": 0.6542470455169678,
      "learning_rate": 9.609310222292415e-05,
      "loss": 0.8193,
      "step": 1980
    },
    {
      "epoch": 0.07857227464760927,
      "grad_norm": 0.9208022952079773,
      "learning_rate": 9.60733604453745e-05,
      "loss": 0.8883,
      "step": 1990
    },
    {
      "epoch": 0.07896711019860228,
      "grad_norm": 0.5730398297309875,
      "learning_rate": 9.605361866782485e-05,
      "loss": 0.9046,
      "step": 2000
    },
    {
      "epoch": 0.07896711019860228,
      "eval_loss": 0.8524609208106995,
      "eval_runtime": 1216.7224,
      "eval_samples_per_second": 9.252,
      "eval_steps_per_second": 9.252,
      "step": 2000
    },
    {
      "epoch": 0.07936194574959529,
      "grad_norm": 0.5562544465065002,
      "learning_rate": 9.603387689027522e-05,
      "loss": 0.8008,
      "step": 2010
    },
    {
      "epoch": 0.0797567813005883,
      "grad_norm": 0.5168394446372986,
      "learning_rate": 9.601413511272555e-05,
      "loss": 0.8549,
      "step": 2020
    },
    {
      "epoch": 0.08015161685158131,
      "grad_norm": 0.5821062922477722,
      "learning_rate": 9.59943933351759e-05,
      "loss": 0.8405,
      "step": 2030
    },
    {
      "epoch": 0.08054645240257433,
      "grad_norm": 0.5888854265213013,
      "learning_rate": 9.597465155762625e-05,
      "loss": 0.8347,
      "step": 2040
    },
    {
      "epoch": 0.08094128795356734,
      "grad_norm": 0.5437805652618408,
      "learning_rate": 9.59549097800766e-05,
      "loss": 0.8218,
      "step": 2050
    },
    {
      "epoch": 0.08133612350456035,
      "grad_norm": 0.5737281441688538,
      "learning_rate": 9.593516800252695e-05,
      "loss": 0.8159,
      "step": 2060
    },
    {
      "epoch": 0.08173095905555336,
      "grad_norm": 0.7108930945396423,
      "learning_rate": 9.59154262249773e-05,
      "loss": 0.8533,
      "step": 2070
    },
    {
      "epoch": 0.08212579460654637,
      "grad_norm": 0.49229878187179565,
      "learning_rate": 9.589568444742765e-05,
      "loss": 0.8459,
      "step": 2080
    },
    {
      "epoch": 0.08252063015753938,
      "grad_norm": 0.6190510392189026,
      "learning_rate": 9.5875942669878e-05,
      "loss": 0.8138,
      "step": 2090
    },
    {
      "epoch": 0.0829154657085324,
      "grad_norm": 0.5205151438713074,
      "learning_rate": 9.585620089232835e-05,
      "loss": 0.7924,
      "step": 2100
    },
    {
      "epoch": 0.0833103012595254,
      "grad_norm": 0.5867277383804321,
      "learning_rate": 9.58364591147787e-05,
      "loss": 0.8548,
      "step": 2110
    },
    {
      "epoch": 0.08370513681051842,
      "grad_norm": 0.5508303046226501,
      "learning_rate": 9.581671733722905e-05,
      "loss": 0.8,
      "step": 2120
    },
    {
      "epoch": 0.08409997236151143,
      "grad_norm": 0.9964906573295593,
      "learning_rate": 9.57969755596794e-05,
      "loss": 0.8243,
      "step": 2130
    },
    {
      "epoch": 0.08449480791250444,
      "grad_norm": 0.6101039052009583,
      "learning_rate": 9.577723378212975e-05,
      "loss": 0.8406,
      "step": 2140
    },
    {
      "epoch": 0.08488964346349745,
      "grad_norm": 0.6776099801063538,
      "learning_rate": 9.57574920045801e-05,
      "loss": 0.8389,
      "step": 2150
    },
    {
      "epoch": 0.08528447901449046,
      "grad_norm": 0.563689112663269,
      "learning_rate": 9.573775022703044e-05,
      "loss": 0.8083,
      "step": 2160
    },
    {
      "epoch": 0.08567931456548347,
      "grad_norm": 0.7241619229316711,
      "learning_rate": 9.57180084494808e-05,
      "loss": 0.8454,
      "step": 2170
    },
    {
      "epoch": 0.08607415011647648,
      "grad_norm": 0.563775897026062,
      "learning_rate": 9.569826667193115e-05,
      "loss": 0.8007,
      "step": 2180
    },
    {
      "epoch": 0.0864689856674695,
      "grad_norm": 0.7288410663604736,
      "learning_rate": 9.567852489438149e-05,
      "loss": 0.8666,
      "step": 2190
    },
    {
      "epoch": 0.0868638212184625,
      "grad_norm": 0.7568804025650024,
      "learning_rate": 9.565878311683184e-05,
      "loss": 0.8786,
      "step": 2200
    },
    {
      "epoch": 0.08725865676945552,
      "grad_norm": 0.6247081756591797,
      "learning_rate": 9.56390413392822e-05,
      "loss": 0.8415,
      "step": 2210
    },
    {
      "epoch": 0.08765349232044853,
      "grad_norm": 0.6383975148200989,
      "learning_rate": 9.561929956173255e-05,
      "loss": 0.7875,
      "step": 2220
    },
    {
      "epoch": 0.08804832787144154,
      "grad_norm": 0.6864882707595825,
      "learning_rate": 9.559955778418289e-05,
      "loss": 0.8433,
      "step": 2230
    },
    {
      "epoch": 0.08844316342243455,
      "grad_norm": 0.4537576735019684,
      "learning_rate": 9.557981600663324e-05,
      "loss": 0.8297,
      "step": 2240
    },
    {
      "epoch": 0.08883799897342756,
      "grad_norm": 0.6040791273117065,
      "learning_rate": 9.55600742290836e-05,
      "loss": 0.8472,
      "step": 2250
    },
    {
      "epoch": 0.08923283452442057,
      "grad_norm": 0.5010660290718079,
      "learning_rate": 9.554033245153394e-05,
      "loss": 0.8666,
      "step": 2260
    },
    {
      "epoch": 0.08962767007541358,
      "grad_norm": 0.6703259348869324,
      "learning_rate": 9.552059067398429e-05,
      "loss": 0.7752,
      "step": 2270
    },
    {
      "epoch": 0.0900225056264066,
      "grad_norm": 0.6426747441291809,
      "learning_rate": 9.550084889643464e-05,
      "loss": 0.8224,
      "step": 2280
    },
    {
      "epoch": 0.09041734117739961,
      "grad_norm": 0.8872144222259521,
      "learning_rate": 9.5481107118885e-05,
      "loss": 0.8219,
      "step": 2290
    },
    {
      "epoch": 0.09081217672839262,
      "grad_norm": 0.5363165140151978,
      "learning_rate": 9.546136534133534e-05,
      "loss": 0.8113,
      "step": 2300
    },
    {
      "epoch": 0.09120701227938563,
      "grad_norm": 0.8105789422988892,
      "learning_rate": 9.544162356378569e-05,
      "loss": 0.8076,
      "step": 2310
    },
    {
      "epoch": 0.09160184783037864,
      "grad_norm": 0.6174616813659668,
      "learning_rate": 9.542188178623604e-05,
      "loss": 0.7923,
      "step": 2320
    },
    {
      "epoch": 0.09199668338137165,
      "grad_norm": 0.7852112650871277,
      "learning_rate": 9.540214000868639e-05,
      "loss": 0.9001,
      "step": 2330
    },
    {
      "epoch": 0.09239151893236466,
      "grad_norm": 0.5174301862716675,
      "learning_rate": 9.538239823113674e-05,
      "loss": 0.8667,
      "step": 2340
    },
    {
      "epoch": 0.09278635448335767,
      "grad_norm": 0.6342726349830627,
      "learning_rate": 9.536265645358709e-05,
      "loss": 0.8155,
      "step": 2350
    },
    {
      "epoch": 0.09318119003435069,
      "grad_norm": 0.7105669379234314,
      "learning_rate": 9.534291467603744e-05,
      "loss": 0.8275,
      "step": 2360
    },
    {
      "epoch": 0.09357602558534371,
      "grad_norm": 0.5840947031974792,
      "learning_rate": 9.532317289848779e-05,
      "loss": 0.8063,
      "step": 2370
    },
    {
      "epoch": 0.09397086113633672,
      "grad_norm": 0.7129294872283936,
      "learning_rate": 9.530343112093814e-05,
      "loss": 0.8474,
      "step": 2380
    },
    {
      "epoch": 0.09436569668732973,
      "grad_norm": 0.6137329339981079,
      "learning_rate": 9.528368934338849e-05,
      "loss": 0.8362,
      "step": 2390
    },
    {
      "epoch": 0.09476053223832274,
      "grad_norm": 0.6633503437042236,
      "learning_rate": 9.526394756583884e-05,
      "loss": 0.7563,
      "step": 2400
    },
    {
      "epoch": 0.09515536778931576,
      "grad_norm": 0.5214669108390808,
      "learning_rate": 9.524420578828919e-05,
      "loss": 0.8695,
      "step": 2410
    },
    {
      "epoch": 0.09555020334030877,
      "grad_norm": 0.6858692765235901,
      "learning_rate": 9.522446401073954e-05,
      "loss": 0.8109,
      "step": 2420
    },
    {
      "epoch": 0.09594503889130178,
      "grad_norm": 0.631562352180481,
      "learning_rate": 9.520472223318989e-05,
      "loss": 0.7817,
      "step": 2430
    },
    {
      "epoch": 0.09633987444229479,
      "grad_norm": 0.5534237623214722,
      "learning_rate": 9.518498045564022e-05,
      "loss": 0.8209,
      "step": 2440
    },
    {
      "epoch": 0.0967347099932878,
      "grad_norm": 0.8353220820426941,
      "learning_rate": 9.516523867809058e-05,
      "loss": 0.8333,
      "step": 2450
    },
    {
      "epoch": 0.09712954554428081,
      "grad_norm": 0.674754798412323,
      "learning_rate": 9.514549690054093e-05,
      "loss": 0.809,
      "step": 2460
    },
    {
      "epoch": 0.09752438109527382,
      "grad_norm": 0.5658343434333801,
      "learning_rate": 9.512575512299128e-05,
      "loss": 0.794,
      "step": 2470
    },
    {
      "epoch": 0.09791921664626684,
      "grad_norm": 0.5511138439178467,
      "learning_rate": 9.510601334544162e-05,
      "loss": 0.8101,
      "step": 2480
    },
    {
      "epoch": 0.09831405219725985,
      "grad_norm": 0.7298234105110168,
      "learning_rate": 9.508627156789198e-05,
      "loss": 0.8344,
      "step": 2490
    },
    {
      "epoch": 0.09870888774825286,
      "grad_norm": 0.600380539894104,
      "learning_rate": 9.506652979034233e-05,
      "loss": 0.8471,
      "step": 2500
    },
    {
      "epoch": 0.09910372329924587,
      "grad_norm": 0.6153350472450256,
      "learning_rate": 9.504678801279267e-05,
      "loss": 0.7865,
      "step": 2510
    },
    {
      "epoch": 0.09949855885023888,
      "grad_norm": 0.590699315071106,
      "learning_rate": 9.502704623524302e-05,
      "loss": 0.8431,
      "step": 2520
    },
    {
      "epoch": 0.09989339440123189,
      "grad_norm": 0.5919359922409058,
      "learning_rate": 9.500730445769338e-05,
      "loss": 0.8178,
      "step": 2530
    },
    {
      "epoch": 0.1002882299522249,
      "grad_norm": 0.491860032081604,
      "learning_rate": 9.498756268014372e-05,
      "loss": 0.7992,
      "step": 2540
    },
    {
      "epoch": 0.10068306550321791,
      "grad_norm": 0.6876122951507568,
      "learning_rate": 9.496782090259407e-05,
      "loss": 0.7691,
      "step": 2550
    },
    {
      "epoch": 0.10107790105421093,
      "grad_norm": 0.5275731682777405,
      "learning_rate": 9.494807912504442e-05,
      "loss": 0.8085,
      "step": 2560
    },
    {
      "epoch": 0.10147273660520394,
      "grad_norm": 0.5139155983924866,
      "learning_rate": 9.492833734749478e-05,
      "loss": 0.7725,
      "step": 2570
    },
    {
      "epoch": 0.10186757215619695,
      "grad_norm": 0.5986916422843933,
      "learning_rate": 9.490859556994512e-05,
      "loss": 0.8289,
      "step": 2580
    },
    {
      "epoch": 0.10226240770718996,
      "grad_norm": 0.5788695216178894,
      "learning_rate": 9.488885379239547e-05,
      "loss": 0.8441,
      "step": 2590
    },
    {
      "epoch": 0.10265724325818297,
      "grad_norm": 0.8206242322921753,
      "learning_rate": 9.486911201484582e-05,
      "loss": 0.8265,
      "step": 2600
    },
    {
      "epoch": 0.10305207880917598,
      "grad_norm": 0.600432813167572,
      "learning_rate": 9.484937023729617e-05,
      "loss": 0.8572,
      "step": 2610
    },
    {
      "epoch": 0.10344691436016899,
      "grad_norm": 0.7046660780906677,
      "learning_rate": 9.482962845974652e-05,
      "loss": 0.8302,
      "step": 2620
    },
    {
      "epoch": 0.103841749911162,
      "grad_norm": 0.7729104161262512,
      "learning_rate": 9.480988668219687e-05,
      "loss": 0.8404,
      "step": 2630
    },
    {
      "epoch": 0.10423658546215502,
      "grad_norm": 0.5813356637954712,
      "learning_rate": 9.479014490464722e-05,
      "loss": 0.7886,
      "step": 2640
    },
    {
      "epoch": 0.10463142101314803,
      "grad_norm": 0.5990552306175232,
      "learning_rate": 9.477040312709757e-05,
      "loss": 0.8278,
      "step": 2650
    },
    {
      "epoch": 0.10502625656414104,
      "grad_norm": 0.6056170463562012,
      "learning_rate": 9.475066134954792e-05,
      "loss": 0.8328,
      "step": 2660
    },
    {
      "epoch": 0.10542109211513405,
      "grad_norm": 0.5240866541862488,
      "learning_rate": 9.473091957199827e-05,
      "loss": 0.8064,
      "step": 2670
    },
    {
      "epoch": 0.10581592766612706,
      "grad_norm": 0.7682331800460815,
      "learning_rate": 9.471117779444862e-05,
      "loss": 0.8031,
      "step": 2680
    },
    {
      "epoch": 0.10621076321712007,
      "grad_norm": 0.719295859336853,
      "learning_rate": 9.469143601689897e-05,
      "loss": 0.849,
      "step": 2690
    },
    {
      "epoch": 0.10660559876811308,
      "grad_norm": 0.5565750002861023,
      "learning_rate": 9.467169423934932e-05,
      "loss": 0.8787,
      "step": 2700
    },
    {
      "epoch": 0.1070004343191061,
      "grad_norm": 0.4982040524482727,
      "learning_rate": 9.465195246179967e-05,
      "loss": 0.7568,
      "step": 2710
    },
    {
      "epoch": 0.1073952698700991,
      "grad_norm": 0.7181571125984192,
      "learning_rate": 9.463221068425e-05,
      "loss": 0.8682,
      "step": 2720
    },
    {
      "epoch": 0.10779010542109212,
      "grad_norm": 0.6095961928367615,
      "learning_rate": 9.461246890670037e-05,
      "loss": 0.8104,
      "step": 2730
    },
    {
      "epoch": 0.10818494097208513,
      "grad_norm": 0.5870094299316406,
      "learning_rate": 9.459272712915072e-05,
      "loss": 0.8163,
      "step": 2740
    },
    {
      "epoch": 0.10857977652307814,
      "grad_norm": 0.5970776677131653,
      "learning_rate": 9.457298535160107e-05,
      "loss": 0.8191,
      "step": 2750
    },
    {
      "epoch": 0.10897461207407115,
      "grad_norm": 0.8432612419128418,
      "learning_rate": 9.45532435740514e-05,
      "loss": 0.8133,
      "step": 2760
    },
    {
      "epoch": 0.10936944762506416,
      "grad_norm": 0.47420066595077515,
      "learning_rate": 9.453350179650177e-05,
      "loss": 0.8403,
      "step": 2770
    },
    {
      "epoch": 0.10976428317605717,
      "grad_norm": 0.725544273853302,
      "learning_rate": 9.451376001895212e-05,
      "loss": 0.8283,
      "step": 2780
    },
    {
      "epoch": 0.11015911872705018,
      "grad_norm": 0.6595667004585266,
      "learning_rate": 9.449401824140245e-05,
      "loss": 0.7876,
      "step": 2790
    },
    {
      "epoch": 0.1105539542780432,
      "grad_norm": 0.637982964515686,
      "learning_rate": 9.44742764638528e-05,
      "loss": 0.8172,
      "step": 2800
    },
    {
      "epoch": 0.1109487898290362,
      "grad_norm": 0.6528634428977966,
      "learning_rate": 9.445453468630317e-05,
      "loss": 0.8066,
      "step": 2810
    },
    {
      "epoch": 0.11134362538002922,
      "grad_norm": 0.5897892117500305,
      "learning_rate": 9.44347929087535e-05,
      "loss": 0.7857,
      "step": 2820
    },
    {
      "epoch": 0.11173846093102223,
      "grad_norm": 0.6580719947814941,
      "learning_rate": 9.441505113120385e-05,
      "loss": 0.8508,
      "step": 2830
    },
    {
      "epoch": 0.11213329648201524,
      "grad_norm": 0.6670557856559753,
      "learning_rate": 9.43953093536542e-05,
      "loss": 0.8397,
      "step": 2840
    },
    {
      "epoch": 0.11252813203300825,
      "grad_norm": 0.5372582077980042,
      "learning_rate": 9.437556757610457e-05,
      "loss": 0.788,
      "step": 2850
    },
    {
      "epoch": 0.11292296758400126,
      "grad_norm": 0.6640584468841553,
      "learning_rate": 9.43558257985549e-05,
      "loss": 0.864,
      "step": 2860
    },
    {
      "epoch": 0.11331780313499427,
      "grad_norm": 0.6452336311340332,
      "learning_rate": 9.433608402100525e-05,
      "loss": 0.8678,
      "step": 2870
    },
    {
      "epoch": 0.11371263868598729,
      "grad_norm": 0.6586320996284485,
      "learning_rate": 9.43163422434556e-05,
      "loss": 0.7819,
      "step": 2880
    },
    {
      "epoch": 0.1141074742369803,
      "grad_norm": 0.6166437268257141,
      "learning_rate": 9.429660046590595e-05,
      "loss": 0.8197,
      "step": 2890
    },
    {
      "epoch": 0.11450230978797331,
      "grad_norm": 0.6138162612915039,
      "learning_rate": 9.42768586883563e-05,
      "loss": 0.8009,
      "step": 2900
    },
    {
      "epoch": 0.11489714533896632,
      "grad_norm": 0.5831440091133118,
      "learning_rate": 9.425711691080665e-05,
      "loss": 0.8181,
      "step": 2910
    },
    {
      "epoch": 0.11529198088995933,
      "grad_norm": 0.6404352188110352,
      "learning_rate": 9.4237375133257e-05,
      "loss": 0.8374,
      "step": 2920
    },
    {
      "epoch": 0.11568681644095234,
      "grad_norm": 0.6904244422912598,
      "learning_rate": 9.421763335570735e-05,
      "loss": 0.7889,
      "step": 2930
    },
    {
      "epoch": 0.11608165199194535,
      "grad_norm": 0.5951546430587769,
      "learning_rate": 9.41978915781577e-05,
      "loss": 0.8456,
      "step": 2940
    },
    {
      "epoch": 0.11647648754293836,
      "grad_norm": 0.6114855408668518,
      "learning_rate": 9.417814980060805e-05,
      "loss": 0.8109,
      "step": 2950
    },
    {
      "epoch": 0.11687132309393138,
      "grad_norm": 0.8851062655448914,
      "learning_rate": 9.41584080230584e-05,
      "loss": 0.8572,
      "step": 2960
    },
    {
      "epoch": 0.11726615864492439,
      "grad_norm": 0.5887358784675598,
      "learning_rate": 9.413866624550875e-05,
      "loss": 0.801,
      "step": 2970
    },
    {
      "epoch": 0.1176609941959174,
      "grad_norm": 0.5223507285118103,
      "learning_rate": 9.41189244679591e-05,
      "loss": 0.7882,
      "step": 2980
    },
    {
      "epoch": 0.11805582974691041,
      "grad_norm": 0.5657094717025757,
      "learning_rate": 9.409918269040945e-05,
      "loss": 0.852,
      "step": 2990
    },
    {
      "epoch": 0.11845066529790342,
      "grad_norm": 0.7430176138877869,
      "learning_rate": 9.407944091285979e-05,
      "loss": 0.8367,
      "step": 3000
    },
    {
      "epoch": 0.11845066529790342,
      "eval_loss": 0.8375552296638489,
      "eval_runtime": 1217.4112,
      "eval_samples_per_second": 9.247,
      "eval_steps_per_second": 9.247,
      "step": 3000
    },
    {
      "epoch": 0.11884550084889643,
      "grad_norm": 0.7088295221328735,
      "learning_rate": 9.405969913531015e-05,
      "loss": 0.8197,
      "step": 3010
    },
    {
      "epoch": 0.11924033639988944,
      "grad_norm": 0.5641300082206726,
      "learning_rate": 9.40399573577605e-05,
      "loss": 0.8294,
      "step": 3020
    },
    {
      "epoch": 0.11963517195088245,
      "grad_norm": 0.7852630019187927,
      "learning_rate": 9.402021558021085e-05,
      "loss": 0.8449,
      "step": 3030
    },
    {
      "epoch": 0.12003000750187547,
      "grad_norm": 0.7056637406349182,
      "learning_rate": 9.400047380266119e-05,
      "loss": 0.8096,
      "step": 3040
    },
    {
      "epoch": 0.12042484305286848,
      "grad_norm": 0.5993286371231079,
      "learning_rate": 9.398073202511155e-05,
      "loss": 0.8008,
      "step": 3050
    },
    {
      "epoch": 0.12081967860386149,
      "grad_norm": 0.5402933955192566,
      "learning_rate": 9.39609902475619e-05,
      "loss": 0.7797,
      "step": 3060
    },
    {
      "epoch": 0.1212145141548545,
      "grad_norm": 0.8757955431938171,
      "learning_rate": 9.394124847001224e-05,
      "loss": 0.8107,
      "step": 3070
    },
    {
      "epoch": 0.12160934970584751,
      "grad_norm": 0.6889274716377258,
      "learning_rate": 9.392150669246259e-05,
      "loss": 0.8851,
      "step": 3080
    },
    {
      "epoch": 0.12200418525684052,
      "grad_norm": 0.647128164768219,
      "learning_rate": 9.390176491491295e-05,
      "loss": 0.8112,
      "step": 3090
    },
    {
      "epoch": 0.12239902080783353,
      "grad_norm": 0.6943539977073669,
      "learning_rate": 9.388202313736329e-05,
      "loss": 0.8262,
      "step": 3100
    },
    {
      "epoch": 0.12279385635882654,
      "grad_norm": 0.5891082882881165,
      "learning_rate": 9.386228135981364e-05,
      "loss": 0.8324,
      "step": 3110
    },
    {
      "epoch": 0.12318869190981956,
      "grad_norm": 0.5192351937294006,
      "learning_rate": 9.384253958226399e-05,
      "loss": 0.7986,
      "step": 3120
    },
    {
      "epoch": 0.12358352746081257,
      "grad_norm": 0.555940568447113,
      "learning_rate": 9.382279780471435e-05,
      "loss": 0.7646,
      "step": 3130
    },
    {
      "epoch": 0.12397836301180558,
      "grad_norm": 0.5586077570915222,
      "learning_rate": 9.380305602716469e-05,
      "loss": 0.8364,
      "step": 3140
    },
    {
      "epoch": 0.12437319856279859,
      "grad_norm": 0.675777018070221,
      "learning_rate": 9.378331424961504e-05,
      "loss": 0.8759,
      "step": 3150
    },
    {
      "epoch": 0.1247680341137916,
      "grad_norm": 0.5582929253578186,
      "learning_rate": 9.376357247206539e-05,
      "loss": 0.8177,
      "step": 3160
    },
    {
      "epoch": 0.12516286966478463,
      "grad_norm": 0.723554253578186,
      "learning_rate": 9.374383069451574e-05,
      "loss": 0.7925,
      "step": 3170
    },
    {
      "epoch": 0.12555770521577764,
      "grad_norm": 0.7005665898323059,
      "learning_rate": 9.372408891696609e-05,
      "loss": 0.837,
      "step": 3180
    },
    {
      "epoch": 0.12595254076677065,
      "grad_norm": 0.7906779646873474,
      "learning_rate": 9.370434713941644e-05,
      "loss": 0.842,
      "step": 3190
    },
    {
      "epoch": 0.12634737631776366,
      "grad_norm": 0.6607750654220581,
      "learning_rate": 9.368460536186679e-05,
      "loss": 0.8144,
      "step": 3200
    },
    {
      "epoch": 0.12674221186875667,
      "grad_norm": 0.6122361421585083,
      "learning_rate": 9.366486358431714e-05,
      "loss": 0.8648,
      "step": 3210
    },
    {
      "epoch": 0.12713704741974968,
      "grad_norm": 0.6419767737388611,
      "learning_rate": 9.364512180676749e-05,
      "loss": 0.8192,
      "step": 3220
    },
    {
      "epoch": 0.1275318829707427,
      "grad_norm": 0.5519276857376099,
      "learning_rate": 9.362538002921784e-05,
      "loss": 0.8069,
      "step": 3230
    },
    {
      "epoch": 0.1279267185217357,
      "grad_norm": 0.6943264007568359,
      "learning_rate": 9.360563825166819e-05,
      "loss": 0.7999,
      "step": 3240
    },
    {
      "epoch": 0.12832155407272872,
      "grad_norm": 0.4625246822834015,
      "learning_rate": 9.358589647411854e-05,
      "loss": 0.8,
      "step": 3250
    },
    {
      "epoch": 0.12871638962372173,
      "grad_norm": 0.7751611471176147,
      "learning_rate": 9.356615469656889e-05,
      "loss": 0.8009,
      "step": 3260
    },
    {
      "epoch": 0.12911122517471474,
      "grad_norm": 0.6880801320075989,
      "learning_rate": 9.354641291901924e-05,
      "loss": 0.8353,
      "step": 3270
    },
    {
      "epoch": 0.12950606072570775,
      "grad_norm": 0.4892401397228241,
      "learning_rate": 9.352667114146957e-05,
      "loss": 0.814,
      "step": 3280
    },
    {
      "epoch": 0.12990089627670076,
      "grad_norm": 0.7342851758003235,
      "learning_rate": 9.350692936391994e-05,
      "loss": 0.8279,
      "step": 3290
    },
    {
      "epoch": 0.13029573182769377,
      "grad_norm": 0.6129159927368164,
      "learning_rate": 9.348718758637029e-05,
      "loss": 0.8172,
      "step": 3300
    },
    {
      "epoch": 0.13069056737868678,
      "grad_norm": 0.8375661969184875,
      "learning_rate": 9.346744580882064e-05,
      "loss": 0.813,
      "step": 3310
    },
    {
      "epoch": 0.1310854029296798,
      "grad_norm": 0.5257178544998169,
      "learning_rate": 9.344770403127097e-05,
      "loss": 0.8104,
      "step": 3320
    },
    {
      "epoch": 0.1314802384806728,
      "grad_norm": 0.6600773930549622,
      "learning_rate": 9.342796225372134e-05,
      "loss": 0.7994,
      "step": 3330
    },
    {
      "epoch": 0.13187507403166582,
      "grad_norm": 0.8321677446365356,
      "learning_rate": 9.340822047617169e-05,
      "loss": 0.8149,
      "step": 3340
    },
    {
      "epoch": 0.13226990958265883,
      "grad_norm": 0.6373988389968872,
      "learning_rate": 9.338847869862202e-05,
      "loss": 0.8078,
      "step": 3350
    },
    {
      "epoch": 0.13266474513365184,
      "grad_norm": 0.6218620538711548,
      "learning_rate": 9.336873692107237e-05,
      "loss": 0.8382,
      "step": 3360
    },
    {
      "epoch": 0.13305958068464485,
      "grad_norm": 0.7340582609176636,
      "learning_rate": 9.334899514352274e-05,
      "loss": 0.8015,
      "step": 3370
    },
    {
      "epoch": 0.13345441623563786,
      "grad_norm": 0.5302684903144836,
      "learning_rate": 9.332925336597309e-05,
      "loss": 0.8381,
      "step": 3380
    },
    {
      "epoch": 0.13384925178663087,
      "grad_norm": 0.8384373188018799,
      "learning_rate": 9.330951158842342e-05,
      "loss": 0.858,
      "step": 3390
    },
    {
      "epoch": 0.13424408733762389,
      "grad_norm": 0.6136610507965088,
      "learning_rate": 9.328976981087377e-05,
      "loss": 0.8493,
      "step": 3400
    },
    {
      "epoch": 0.1346389228886169,
      "grad_norm": 0.5931761264801025,
      "learning_rate": 9.327002803332412e-05,
      "loss": 0.8027,
      "step": 3410
    },
    {
      "epoch": 0.1350337584396099,
      "grad_norm": 0.596190333366394,
      "learning_rate": 9.325028625577447e-05,
      "loss": 0.8009,
      "step": 3420
    },
    {
      "epoch": 0.13542859399060292,
      "grad_norm": 0.6750569343566895,
      "learning_rate": 9.323054447822482e-05,
      "loss": 0.8272,
      "step": 3430
    },
    {
      "epoch": 0.13582342954159593,
      "grad_norm": 0.6722732782363892,
      "learning_rate": 9.321080270067517e-05,
      "loss": 0.7118,
      "step": 3440
    },
    {
      "epoch": 0.13621826509258894,
      "grad_norm": 0.6015455722808838,
      "learning_rate": 9.319106092312552e-05,
      "loss": 0.7681,
      "step": 3450
    },
    {
      "epoch": 0.13661310064358195,
      "grad_norm": 0.5155925750732422,
      "learning_rate": 9.317131914557587e-05,
      "loss": 0.793,
      "step": 3460
    },
    {
      "epoch": 0.13700793619457496,
      "grad_norm": 0.625324547290802,
      "learning_rate": 9.315157736802622e-05,
      "loss": 0.8131,
      "step": 3470
    },
    {
      "epoch": 0.13740277174556798,
      "grad_norm": 0.5985895395278931,
      "learning_rate": 9.313183559047657e-05,
      "loss": 0.7819,
      "step": 3480
    },
    {
      "epoch": 0.137797607296561,
      "grad_norm": 0.7447977066040039,
      "learning_rate": 9.311209381292692e-05,
      "loss": 0.7922,
      "step": 3490
    },
    {
      "epoch": 0.138192442847554,
      "grad_norm": 0.6917034387588501,
      "learning_rate": 9.309235203537727e-05,
      "loss": 0.8096,
      "step": 3500
    },
    {
      "epoch": 0.138587278398547,
      "grad_norm": 0.5784260034561157,
      "learning_rate": 9.307261025782762e-05,
      "loss": 0.8247,
      "step": 3510
    },
    {
      "epoch": 0.13898211394954002,
      "grad_norm": 0.6404523253440857,
      "learning_rate": 9.305286848027797e-05,
      "loss": 0.8111,
      "step": 3520
    },
    {
      "epoch": 0.13937694950053303,
      "grad_norm": 0.5931258201599121,
      "learning_rate": 9.303312670272831e-05,
      "loss": 0.8148,
      "step": 3530
    },
    {
      "epoch": 0.13977178505152604,
      "grad_norm": 0.6274220943450928,
      "learning_rate": 9.301338492517867e-05,
      "loss": 0.8513,
      "step": 3540
    },
    {
      "epoch": 0.14016662060251905,
      "grad_norm": 0.6242789626121521,
      "learning_rate": 9.299364314762902e-05,
      "loss": 0.8052,
      "step": 3550
    },
    {
      "epoch": 0.14056145615351207,
      "grad_norm": 0.5343210697174072,
      "learning_rate": 9.297390137007936e-05,
      "loss": 0.7705,
      "step": 3560
    },
    {
      "epoch": 0.14095629170450508,
      "grad_norm": 0.658015251159668,
      "learning_rate": 9.295415959252971e-05,
      "loss": 0.7984,
      "step": 3570
    },
    {
      "epoch": 0.1413511272554981,
      "grad_norm": 0.5647412538528442,
      "learning_rate": 9.293441781498007e-05,
      "loss": 0.7974,
      "step": 3580
    },
    {
      "epoch": 0.1417459628064911,
      "grad_norm": 0.6459203958511353,
      "learning_rate": 9.291467603743042e-05,
      "loss": 0.8417,
      "step": 3590
    },
    {
      "epoch": 0.1421407983574841,
      "grad_norm": 0.6590398550033569,
      "learning_rate": 9.289493425988076e-05,
      "loss": 0.7784,
      "step": 3600
    },
    {
      "epoch": 0.14253563390847712,
      "grad_norm": 0.6602696180343628,
      "learning_rate": 9.287519248233111e-05,
      "loss": 0.8006,
      "step": 3610
    },
    {
      "epoch": 0.14293046945947013,
      "grad_norm": 0.7626996636390686,
      "learning_rate": 9.285545070478147e-05,
      "loss": 0.7854,
      "step": 3620
    },
    {
      "epoch": 0.14332530501046314,
      "grad_norm": 0.6530598402023315,
      "learning_rate": 9.283570892723181e-05,
      "loss": 0.7801,
      "step": 3630
    },
    {
      "epoch": 0.14372014056145616,
      "grad_norm": 0.7916349768638611,
      "learning_rate": 9.281596714968216e-05,
      "loss": 0.8398,
      "step": 3640
    },
    {
      "epoch": 0.14411497611244917,
      "grad_norm": 0.6133912801742554,
      "learning_rate": 9.27962253721325e-05,
      "loss": 0.7908,
      "step": 3650
    },
    {
      "epoch": 0.14450981166344218,
      "grad_norm": 0.693202793598175,
      "learning_rate": 9.277648359458287e-05,
      "loss": 0.8425,
      "step": 3660
    },
    {
      "epoch": 0.1449046472144352,
      "grad_norm": 0.5731971859931946,
      "learning_rate": 9.27567418170332e-05,
      "loss": 0.781,
      "step": 3670
    },
    {
      "epoch": 0.1452994827654282,
      "grad_norm": 0.6278063654899597,
      "learning_rate": 9.273700003948356e-05,
      "loss": 0.7964,
      "step": 3680
    },
    {
      "epoch": 0.1456943183164212,
      "grad_norm": 0.6524587273597717,
      "learning_rate": 9.27172582619339e-05,
      "loss": 0.8323,
      "step": 3690
    },
    {
      "epoch": 0.14608915386741422,
      "grad_norm": 0.9963688254356384,
      "learning_rate": 9.269751648438426e-05,
      "loss": 0.8211,
      "step": 3700
    },
    {
      "epoch": 0.14648398941840723,
      "grad_norm": 0.5749876499176025,
      "learning_rate": 9.26777747068346e-05,
      "loss": 0.8,
      "step": 3710
    },
    {
      "epoch": 0.14687882496940025,
      "grad_norm": 0.5796235203742981,
      "learning_rate": 9.265803292928496e-05,
      "loss": 0.8307,
      "step": 3720
    },
    {
      "epoch": 0.14727366052039326,
      "grad_norm": 0.653876543045044,
      "learning_rate": 9.26382911517353e-05,
      "loss": 0.7902,
      "step": 3730
    },
    {
      "epoch": 0.14766849607138627,
      "grad_norm": 0.8122597932815552,
      "learning_rate": 9.261854937418566e-05,
      "loss": 0.7707,
      "step": 3740
    },
    {
      "epoch": 0.14806333162237928,
      "grad_norm": 0.593470573425293,
      "learning_rate": 9.2598807596636e-05,
      "loss": 0.8138,
      "step": 3750
    },
    {
      "epoch": 0.1484581671733723,
      "grad_norm": 0.5390767455101013,
      "learning_rate": 9.257906581908636e-05,
      "loss": 0.7612,
      "step": 3760
    },
    {
      "epoch": 0.1488530027243653,
      "grad_norm": 0.646565854549408,
      "learning_rate": 9.25593240415367e-05,
      "loss": 0.7682,
      "step": 3770
    },
    {
      "epoch": 0.1492478382753583,
      "grad_norm": 0.6343041658401489,
      "learning_rate": 9.253958226398706e-05,
      "loss": 0.7939,
      "step": 3780
    },
    {
      "epoch": 0.14964267382635132,
      "grad_norm": 0.5560788512229919,
      "learning_rate": 9.25198404864374e-05,
      "loss": 0.7978,
      "step": 3790
    },
    {
      "epoch": 0.15003750937734434,
      "grad_norm": 0.5671623349189758,
      "learning_rate": 9.250009870888776e-05,
      "loss": 0.8046,
      "step": 3800
    },
    {
      "epoch": 0.15043234492833735,
      "grad_norm": 0.6954753398895264,
      "learning_rate": 9.248035693133809e-05,
      "loss": 0.8023,
      "step": 3810
    },
    {
      "epoch": 0.15082718047933036,
      "grad_norm": 0.5188674926757812,
      "learning_rate": 9.246061515378846e-05,
      "loss": 0.818,
      "step": 3820
    },
    {
      "epoch": 0.15122201603032337,
      "grad_norm": 0.7276550531387329,
      "learning_rate": 9.24408733762388e-05,
      "loss": 0.8439,
      "step": 3830
    },
    {
      "epoch": 0.15161685158131638,
      "grad_norm": 0.5670598745346069,
      "learning_rate": 9.242113159868914e-05,
      "loss": 0.7676,
      "step": 3840
    },
    {
      "epoch": 0.1520116871323094,
      "grad_norm": 0.6703201532363892,
      "learning_rate": 9.240138982113949e-05,
      "loss": 0.8118,
      "step": 3850
    },
    {
      "epoch": 0.1524065226833024,
      "grad_norm": 0.6494250893592834,
      "learning_rate": 9.238164804358985e-05,
      "loss": 0.7748,
      "step": 3860
    },
    {
      "epoch": 0.15280135823429541,
      "grad_norm": 0.6134788990020752,
      "learning_rate": 9.23619062660402e-05,
      "loss": 0.8119,
      "step": 3870
    },
    {
      "epoch": 0.15319619378528843,
      "grad_norm": 0.6716813445091248,
      "learning_rate": 9.234216448849054e-05,
      "loss": 0.8181,
      "step": 3880
    },
    {
      "epoch": 0.15359102933628144,
      "grad_norm": 0.7988901734352112,
      "learning_rate": 9.232242271094089e-05,
      "loss": 0.8039,
      "step": 3890
    },
    {
      "epoch": 0.15398586488727445,
      "grad_norm": 0.565686821937561,
      "learning_rate": 9.230268093339125e-05,
      "loss": 0.8002,
      "step": 3900
    },
    {
      "epoch": 0.15438070043826746,
      "grad_norm": 0.6669639945030212,
      "learning_rate": 9.228293915584159e-05,
      "loss": 0.8137,
      "step": 3910
    },
    {
      "epoch": 0.15477553598926047,
      "grad_norm": 0.5376688241958618,
      "learning_rate": 9.226319737829194e-05,
      "loss": 0.815,
      "step": 3920
    },
    {
      "epoch": 0.15517037154025348,
      "grad_norm": 0.7226375341415405,
      "learning_rate": 9.224345560074229e-05,
      "loss": 0.8182,
      "step": 3930
    },
    {
      "epoch": 0.1555652070912465,
      "grad_norm": 0.613739013671875,
      "learning_rate": 9.222371382319265e-05,
      "loss": 0.8464,
      "step": 3940
    },
    {
      "epoch": 0.1559600426422395,
      "grad_norm": 0.5474435687065125,
      "learning_rate": 9.220397204564299e-05,
      "loss": 0.7869,
      "step": 3950
    },
    {
      "epoch": 0.15635487819323252,
      "grad_norm": 0.5985241532325745,
      "learning_rate": 9.218423026809334e-05,
      "loss": 0.7855,
      "step": 3960
    },
    {
      "epoch": 0.15674971374422553,
      "grad_norm": 0.6549720764160156,
      "learning_rate": 9.216448849054369e-05,
      "loss": 0.8322,
      "step": 3970
    },
    {
      "epoch": 0.15714454929521854,
      "grad_norm": 0.6668853163719177,
      "learning_rate": 9.214474671299404e-05,
      "loss": 0.8236,
      "step": 3980
    },
    {
      "epoch": 0.15753938484621155,
      "grad_norm": 0.6549100875854492,
      "learning_rate": 9.212500493544439e-05,
      "loss": 0.7681,
      "step": 3990
    },
    {
      "epoch": 0.15793422039720456,
      "grad_norm": 0.7122790217399597,
      "learning_rate": 9.210526315789474e-05,
      "loss": 0.7828,
      "step": 4000
    },
    {
      "epoch": 0.15793422039720456,
      "eval_loss": 0.8317030072212219,
      "eval_runtime": 1219.3813,
      "eval_samples_per_second": 9.232,
      "eval_steps_per_second": 9.232,
      "step": 4000
    },
    {
      "epoch": 0.15832905594819757,
      "grad_norm": 0.5141510367393494,
      "learning_rate": 9.208552138034509e-05,
      "loss": 0.8075,
      "step": 4010
    },
    {
      "epoch": 0.15872389149919058,
      "grad_norm": 0.6278862357139587,
      "learning_rate": 9.206577960279544e-05,
      "loss": 0.832,
      "step": 4020
    },
    {
      "epoch": 0.1591187270501836,
      "grad_norm": 0.5693053603172302,
      "learning_rate": 9.204603782524579e-05,
      "loss": 0.8342,
      "step": 4030
    },
    {
      "epoch": 0.1595135626011766,
      "grad_norm": 0.6229149699211121,
      "learning_rate": 9.202629604769614e-05,
      "loss": 0.8205,
      "step": 4040
    },
    {
      "epoch": 0.15990839815216962,
      "grad_norm": 0.6110191345214844,
      "learning_rate": 9.200655427014649e-05,
      "loss": 0.7857,
      "step": 4050
    },
    {
      "epoch": 0.16030323370316263,
      "grad_norm": 0.6195703744888306,
      "learning_rate": 9.198681249259684e-05,
      "loss": 0.8218,
      "step": 4060
    },
    {
      "epoch": 0.16069806925415564,
      "grad_norm": 0.7104290723800659,
      "learning_rate": 9.196707071504719e-05,
      "loss": 0.7876,
      "step": 4070
    },
    {
      "epoch": 0.16109290480514865,
      "grad_norm": 0.6482582092285156,
      "learning_rate": 9.194732893749754e-05,
      "loss": 0.8371,
      "step": 4080
    },
    {
      "epoch": 0.16148774035614166,
      "grad_norm": 0.5153504014015198,
      "learning_rate": 9.192758715994788e-05,
      "loss": 0.8549,
      "step": 4090
    },
    {
      "epoch": 0.16188257590713467,
      "grad_norm": 0.6003082394599915,
      "learning_rate": 9.190784538239824e-05,
      "loss": 0.8442,
      "step": 4100
    },
    {
      "epoch": 0.16227741145812768,
      "grad_norm": 0.5746949911117554,
      "learning_rate": 9.188810360484859e-05,
      "loss": 0.81,
      "step": 4110
    },
    {
      "epoch": 0.1626722470091207,
      "grad_norm": 0.4934377074241638,
      "learning_rate": 9.186836182729893e-05,
      "loss": 0.7885,
      "step": 4120
    },
    {
      "epoch": 0.1630670825601137,
      "grad_norm": 0.6687523126602173,
      "learning_rate": 9.184862004974928e-05,
      "loss": 0.8018,
      "step": 4130
    },
    {
      "epoch": 0.16346191811110672,
      "grad_norm": 0.5582516193389893,
      "learning_rate": 9.182887827219964e-05,
      "loss": 0.8038,
      "step": 4140
    },
    {
      "epoch": 0.16385675366209973,
      "grad_norm": 0.6015886068344116,
      "learning_rate": 9.180913649464999e-05,
      "loss": 0.7694,
      "step": 4150
    },
    {
      "epoch": 0.16425158921309274,
      "grad_norm": 0.6971840262413025,
      "learning_rate": 9.178939471710033e-05,
      "loss": 0.8179,
      "step": 4160
    },
    {
      "epoch": 0.16464642476408575,
      "grad_norm": 0.641715943813324,
      "learning_rate": 9.176965293955068e-05,
      "loss": 0.8147,
      "step": 4170
    },
    {
      "epoch": 0.16504126031507876,
      "grad_norm": 0.6247872710227966,
      "learning_rate": 9.174991116200104e-05,
      "loss": 0.7957,
      "step": 4180
    },
    {
      "epoch": 0.16543609586607178,
      "grad_norm": 0.618302583694458,
      "learning_rate": 9.173016938445137e-05,
      "loss": 0.8118,
      "step": 4190
    },
    {
      "epoch": 0.1658309314170648,
      "grad_norm": 0.6145169734954834,
      "learning_rate": 9.171042760690172e-05,
      "loss": 0.7969,
      "step": 4200
    },
    {
      "epoch": 0.1662257669680578,
      "grad_norm": 0.5590857863426208,
      "learning_rate": 9.169068582935207e-05,
      "loss": 0.7843,
      "step": 4210
    },
    {
      "epoch": 0.1666206025190508,
      "grad_norm": 0.7153270244598389,
      "learning_rate": 9.167094405180244e-05,
      "loss": 0.8217,
      "step": 4220
    },
    {
      "epoch": 0.16701543807004382,
      "grad_norm": 0.6113826632499695,
      "learning_rate": 9.165120227425277e-05,
      "loss": 0.7669,
      "step": 4230
    },
    {
      "epoch": 0.16741027362103683,
      "grad_norm": 0.5103837251663208,
      "learning_rate": 9.163146049670312e-05,
      "loss": 0.7954,
      "step": 4240
    },
    {
      "epoch": 0.16780510917202984,
      "grad_norm": 0.7501357197761536,
      "learning_rate": 9.161171871915347e-05,
      "loss": 0.7946,
      "step": 4250
    },
    {
      "epoch": 0.16819994472302285,
      "grad_norm": 0.5480955243110657,
      "learning_rate": 9.159197694160382e-05,
      "loss": 0.8385,
      "step": 4260
    },
    {
      "epoch": 0.16859478027401587,
      "grad_norm": 0.6208224892616272,
      "learning_rate": 9.157223516405417e-05,
      "loss": 0.825,
      "step": 4270
    },
    {
      "epoch": 0.16898961582500888,
      "grad_norm": 0.6055359840393066,
      "learning_rate": 9.155249338650452e-05,
      "loss": 0.777,
      "step": 4280
    },
    {
      "epoch": 0.1693844513760019,
      "grad_norm": 0.6297844052314758,
      "learning_rate": 9.153275160895487e-05,
      "loss": 0.7975,
      "step": 4290
    },
    {
      "epoch": 0.1697792869269949,
      "grad_norm": 0.5874000787734985,
      "learning_rate": 9.151300983140522e-05,
      "loss": 0.8538,
      "step": 4300
    },
    {
      "epoch": 0.1701741224779879,
      "grad_norm": 0.811862051486969,
      "learning_rate": 9.149326805385557e-05,
      "loss": 0.8002,
      "step": 4310
    },
    {
      "epoch": 0.17056895802898092,
      "grad_norm": 0.553246796131134,
      "learning_rate": 9.147352627630592e-05,
      "loss": 0.8007,
      "step": 4320
    },
    {
      "epoch": 0.17096379357997393,
      "grad_norm": 0.6295461058616638,
      "learning_rate": 9.145378449875627e-05,
      "loss": 0.8166,
      "step": 4330
    },
    {
      "epoch": 0.17135862913096694,
      "grad_norm": 0.655586838722229,
      "learning_rate": 9.143404272120662e-05,
      "loss": 0.791,
      "step": 4340
    },
    {
      "epoch": 0.17175346468195996,
      "grad_norm": 0.6018398404121399,
      "learning_rate": 9.141430094365697e-05,
      "loss": 0.7561,
      "step": 4350
    },
    {
      "epoch": 0.17214830023295297,
      "grad_norm": 0.653777003288269,
      "learning_rate": 9.139455916610732e-05,
      "loss": 0.8224,
      "step": 4360
    },
    {
      "epoch": 0.17254313578394598,
      "grad_norm": 0.6410332322120667,
      "learning_rate": 9.137481738855766e-05,
      "loss": 0.8665,
      "step": 4370
    },
    {
      "epoch": 0.172937971334939,
      "grad_norm": 0.6508930325508118,
      "learning_rate": 9.135507561100802e-05,
      "loss": 0.8505,
      "step": 4380
    },
    {
      "epoch": 0.173332806885932,
      "grad_norm": 0.48395928740501404,
      "learning_rate": 9.133533383345837e-05,
      "loss": 0.8179,
      "step": 4390
    },
    {
      "epoch": 0.173727642436925,
      "grad_norm": 0.716396152973175,
      "learning_rate": 9.131559205590871e-05,
      "loss": 0.8549,
      "step": 4400
    },
    {
      "epoch": 0.17412247798791802,
      "grad_norm": 0.5444737076759338,
      "learning_rate": 9.129585027835906e-05,
      "loss": 0.7855,
      "step": 4410
    },
    {
      "epoch": 0.17451731353891103,
      "grad_norm": 0.617777407169342,
      "learning_rate": 9.127610850080942e-05,
      "loss": 0.8118,
      "step": 4420
    },
    {
      "epoch": 0.17491214908990405,
      "grad_norm": 0.7571383118629456,
      "learning_rate": 9.125636672325977e-05,
      "loss": 0.7823,
      "step": 4430
    },
    {
      "epoch": 0.17530698464089706,
      "grad_norm": 0.5800400972366333,
      "learning_rate": 9.123662494571011e-05,
      "loss": 0.8307,
      "step": 4440
    },
    {
      "epoch": 0.17570182019189007,
      "grad_norm": 0.5923334360122681,
      "learning_rate": 9.121688316816046e-05,
      "loss": 0.8186,
      "step": 4450
    },
    {
      "epoch": 0.17609665574288308,
      "grad_norm": 0.7100816369056702,
      "learning_rate": 9.119714139061082e-05,
      "loss": 0.8314,
      "step": 4460
    },
    {
      "epoch": 0.1764914912938761,
      "grad_norm": 0.6428755521774292,
      "learning_rate": 9.117739961306116e-05,
      "loss": 0.833,
      "step": 4470
    },
    {
      "epoch": 0.1768863268448691,
      "grad_norm": 0.602989912033081,
      "learning_rate": 9.115765783551151e-05,
      "loss": 0.7589,
      "step": 4480
    },
    {
      "epoch": 0.1772811623958621,
      "grad_norm": 0.6354257464408875,
      "learning_rate": 9.113791605796186e-05,
      "loss": 0.8129,
      "step": 4490
    },
    {
      "epoch": 0.17767599794685512,
      "grad_norm": 0.661274790763855,
      "learning_rate": 9.111817428041222e-05,
      "loss": 0.838,
      "step": 4500
    },
    {
      "epoch": 0.17807083349784814,
      "grad_norm": 0.621731162071228,
      "learning_rate": 9.109843250286256e-05,
      "loss": 0.7945,
      "step": 4510
    },
    {
      "epoch": 0.17846566904884115,
      "grad_norm": 0.5921366810798645,
      "learning_rate": 9.107869072531291e-05,
      "loss": 0.7868,
      "step": 4520
    },
    {
      "epoch": 0.17886050459983416,
      "grad_norm": 0.6431331634521484,
      "learning_rate": 9.105894894776326e-05,
      "loss": 0.8554,
      "step": 4530
    },
    {
      "epoch": 0.17925534015082717,
      "grad_norm": 0.6193559169769287,
      "learning_rate": 9.103920717021361e-05,
      "loss": 0.7816,
      "step": 4540
    },
    {
      "epoch": 0.17965017570182018,
      "grad_norm": 0.5882728695869446,
      "learning_rate": 9.101946539266396e-05,
      "loss": 0.8567,
      "step": 4550
    },
    {
      "epoch": 0.1800450112528132,
      "grad_norm": 0.5997166633605957,
      "learning_rate": 9.099972361511431e-05,
      "loss": 0.8008,
      "step": 4560
    },
    {
      "epoch": 0.1804398468038062,
      "grad_norm": 0.5919065475463867,
      "learning_rate": 9.097998183756466e-05,
      "loss": 0.7838,
      "step": 4570
    },
    {
      "epoch": 0.18083468235479921,
      "grad_norm": 0.6122708916664124,
      "learning_rate": 9.096024006001501e-05,
      "loss": 0.8027,
      "step": 4580
    },
    {
      "epoch": 0.18122951790579223,
      "grad_norm": 0.6056862473487854,
      "learning_rate": 9.094049828246536e-05,
      "loss": 0.8206,
      "step": 4590
    },
    {
      "epoch": 0.18162435345678524,
      "grad_norm": 0.5967695116996765,
      "learning_rate": 9.092075650491571e-05,
      "loss": 0.8512,
      "step": 4600
    },
    {
      "epoch": 0.18201918900777825,
      "grad_norm": 0.5482118725776672,
      "learning_rate": 9.090101472736606e-05,
      "loss": 0.7965,
      "step": 4610
    },
    {
      "epoch": 0.18241402455877126,
      "grad_norm": 0.7087428569793701,
      "learning_rate": 9.088127294981641e-05,
      "loss": 0.7667,
      "step": 4620
    },
    {
      "epoch": 0.18280886010976427,
      "grad_norm": 0.5318366885185242,
      "learning_rate": 9.086153117226676e-05,
      "loss": 0.7584,
      "step": 4630
    },
    {
      "epoch": 0.18320369566075728,
      "grad_norm": 0.6598539352416992,
      "learning_rate": 9.084178939471711e-05,
      "loss": 0.8251,
      "step": 4640
    },
    {
      "epoch": 0.1835985312117503,
      "grad_norm": 0.5410402417182922,
      "learning_rate": 9.082204761716744e-05,
      "loss": 0.8067,
      "step": 4650
    },
    {
      "epoch": 0.1839933667627433,
      "grad_norm": 0.6143144369125366,
      "learning_rate": 9.080230583961781e-05,
      "loss": 0.784,
      "step": 4660
    },
    {
      "epoch": 0.18438820231373632,
      "grad_norm": 0.6985350251197815,
      "learning_rate": 9.078256406206816e-05,
      "loss": 0.88,
      "step": 4670
    },
    {
      "epoch": 0.18478303786472933,
      "grad_norm": 0.5756083726882935,
      "learning_rate": 9.076282228451851e-05,
      "loss": 0.7912,
      "step": 4680
    },
    {
      "epoch": 0.18517787341572234,
      "grad_norm": 0.5793498158454895,
      "learning_rate": 9.074308050696884e-05,
      "loss": 0.7826,
      "step": 4690
    },
    {
      "epoch": 0.18557270896671535,
      "grad_norm": 0.6954014897346497,
      "learning_rate": 9.072333872941921e-05,
      "loss": 0.7945,
      "step": 4700
    },
    {
      "epoch": 0.18596754451770836,
      "grad_norm": 0.5242957472801208,
      "learning_rate": 9.070359695186956e-05,
      "loss": 0.7991,
      "step": 4710
    },
    {
      "epoch": 0.18636238006870137,
      "grad_norm": 0.9358483552932739,
      "learning_rate": 9.06838551743199e-05,
      "loss": 0.8381,
      "step": 4720
    },
    {
      "epoch": 0.1867572156196944,
      "grad_norm": 0.5697196125984192,
      "learning_rate": 9.066411339677024e-05,
      "loss": 0.793,
      "step": 4730
    },
    {
      "epoch": 0.18715205117068742,
      "grad_norm": 0.6979020833969116,
      "learning_rate": 9.06443716192206e-05,
      "loss": 0.8605,
      "step": 4740
    },
    {
      "epoch": 0.18754688672168043,
      "grad_norm": 0.7627646327018738,
      "learning_rate": 9.062462984167094e-05,
      "loss": 0.7893,
      "step": 4750
    },
    {
      "epoch": 0.18794172227267344,
      "grad_norm": 0.4731919467449188,
      "learning_rate": 9.060488806412129e-05,
      "loss": 0.7671,
      "step": 4760
    },
    {
      "epoch": 0.18833655782366646,
      "grad_norm": 0.6652513146400452,
      "learning_rate": 9.058514628657164e-05,
      "loss": 0.815,
      "step": 4770
    },
    {
      "epoch": 0.18873139337465947,
      "grad_norm": 0.6921427249908447,
      "learning_rate": 9.0565404509022e-05,
      "loss": 0.806,
      "step": 4780
    },
    {
      "epoch": 0.18912622892565248,
      "grad_norm": 0.6619049310684204,
      "learning_rate": 9.054566273147234e-05,
      "loss": 0.8301,
      "step": 4790
    },
    {
      "epoch": 0.1895210644766455,
      "grad_norm": 0.6010918021202087,
      "learning_rate": 9.052592095392269e-05,
      "loss": 0.813,
      "step": 4800
    },
    {
      "epoch": 0.1899159000276385,
      "grad_norm": 0.5933099389076233,
      "learning_rate": 9.050617917637304e-05,
      "loss": 0.841,
      "step": 4810
    },
    {
      "epoch": 0.1903107355786315,
      "grad_norm": 0.47208887338638306,
      "learning_rate": 9.048643739882339e-05,
      "loss": 0.8077,
      "step": 4820
    },
    {
      "epoch": 0.19070557112962452,
      "grad_norm": 0.6330463886260986,
      "learning_rate": 9.046669562127374e-05,
      "loss": 0.784,
      "step": 4830
    },
    {
      "epoch": 0.19110040668061753,
      "grad_norm": 0.7708705067634583,
      "learning_rate": 9.044695384372409e-05,
      "loss": 0.8305,
      "step": 4840
    },
    {
      "epoch": 0.19149524223161055,
      "grad_norm": 0.753562867641449,
      "learning_rate": 9.042721206617444e-05,
      "loss": 0.8148,
      "step": 4850
    },
    {
      "epoch": 0.19189007778260356,
      "grad_norm": 0.5790135264396667,
      "learning_rate": 9.040747028862479e-05,
      "loss": 0.8551,
      "step": 4860
    },
    {
      "epoch": 0.19228491333359657,
      "grad_norm": 0.566385805606842,
      "learning_rate": 9.038772851107514e-05,
      "loss": 0.7936,
      "step": 4870
    },
    {
      "epoch": 0.19267974888458958,
      "grad_norm": 0.5666289329528809,
      "learning_rate": 9.036798673352549e-05,
      "loss": 0.813,
      "step": 4880
    },
    {
      "epoch": 0.1930745844355826,
      "grad_norm": 0.7613605260848999,
      "learning_rate": 9.034824495597584e-05,
      "loss": 0.8624,
      "step": 4890
    },
    {
      "epoch": 0.1934694199865756,
      "grad_norm": 0.6700316667556763,
      "learning_rate": 9.032850317842619e-05,
      "loss": 0.7815,
      "step": 4900
    },
    {
      "epoch": 0.1938642555375686,
      "grad_norm": 0.8645413517951965,
      "learning_rate": 9.030876140087654e-05,
      "loss": 0.7402,
      "step": 4910
    },
    {
      "epoch": 0.19425909108856163,
      "grad_norm": 0.6291595697402954,
      "learning_rate": 9.028901962332689e-05,
      "loss": 0.7683,
      "step": 4920
    },
    {
      "epoch": 0.19465392663955464,
      "grad_norm": 0.5879074335098267,
      "learning_rate": 9.026927784577723e-05,
      "loss": 0.7919,
      "step": 4930
    },
    {
      "epoch": 0.19504876219054765,
      "grad_norm": 0.7555118799209595,
      "learning_rate": 9.024953606822759e-05,
      "loss": 0.8234,
      "step": 4940
    },
    {
      "epoch": 0.19544359774154066,
      "grad_norm": 0.5915430784225464,
      "learning_rate": 9.022979429067794e-05,
      "loss": 0.7848,
      "step": 4950
    },
    {
      "epoch": 0.19583843329253367,
      "grad_norm": 0.7001379728317261,
      "learning_rate": 9.021005251312829e-05,
      "loss": 0.8124,
      "step": 4960
    },
    {
      "epoch": 0.19623326884352668,
      "grad_norm": 0.7721809148788452,
      "learning_rate": 9.019031073557863e-05,
      "loss": 0.8128,
      "step": 4970
    },
    {
      "epoch": 0.1966281043945197,
      "grad_norm": 0.5190021395683289,
      "learning_rate": 9.017056895802899e-05,
      "loss": 0.821,
      "step": 4980
    },
    {
      "epoch": 0.1970229399455127,
      "grad_norm": 0.6046333909034729,
      "learning_rate": 9.015082718047934e-05,
      "loss": 0.7772,
      "step": 4990
    },
    {
      "epoch": 0.19741777549650572,
      "grad_norm": 0.7871220707893372,
      "learning_rate": 9.013108540292968e-05,
      "loss": 0.8535,
      "step": 5000
    },
    {
      "epoch": 0.19741777549650572,
      "eval_loss": 0.8239011764526367,
      "eval_runtime": 1218.3046,
      "eval_samples_per_second": 9.24,
      "eval_steps_per_second": 9.24,
      "step": 5000
    },
    {
      "epoch": 0.19781261104749873,
      "grad_norm": 0.5575479865074158,
      "learning_rate": 9.011134362538003e-05,
      "loss": 0.8445,
      "step": 5010
    },
    {
      "epoch": 0.19820744659849174,
      "grad_norm": 0.5481799840927124,
      "learning_rate": 9.009160184783039e-05,
      "loss": 0.7875,
      "step": 5020
    },
    {
      "epoch": 0.19860228214948475,
      "grad_norm": 0.6344864964485168,
      "learning_rate": 9.007186007028073e-05,
      "loss": 0.8094,
      "step": 5030
    },
    {
      "epoch": 0.19899711770047776,
      "grad_norm": 0.5801249146461487,
      "learning_rate": 9.005211829273108e-05,
      "loss": 0.8126,
      "step": 5040
    },
    {
      "epoch": 0.19939195325147077,
      "grad_norm": 0.612858235836029,
      "learning_rate": 9.003237651518143e-05,
      "loss": 0.8465,
      "step": 5050
    },
    {
      "epoch": 0.19978678880246378,
      "grad_norm": 0.7658763527870178,
      "learning_rate": 9.001263473763179e-05,
      "loss": 0.7931,
      "step": 5060
    },
    {
      "epoch": 0.2001816243534568,
      "grad_norm": 0.5897344946861267,
      "learning_rate": 8.999289296008213e-05,
      "loss": 0.7967,
      "step": 5070
    },
    {
      "epoch": 0.2005764599044498,
      "grad_norm": 0.6447782516479492,
      "learning_rate": 8.997315118253248e-05,
      "loss": 0.7945,
      "step": 5080
    },
    {
      "epoch": 0.20097129545544282,
      "grad_norm": 0.639528751373291,
      "learning_rate": 8.995340940498283e-05,
      "loss": 0.7869,
      "step": 5090
    },
    {
      "epoch": 0.20136613100643583,
      "grad_norm": 0.6470285058021545,
      "learning_rate": 8.993366762743318e-05,
      "loss": 0.8492,
      "step": 5100
    },
    {
      "epoch": 0.20176096655742884,
      "grad_norm": 0.7168227434158325,
      "learning_rate": 8.991392584988353e-05,
      "loss": 0.7617,
      "step": 5110
    },
    {
      "epoch": 0.20215580210842185,
      "grad_norm": 0.6796477437019348,
      "learning_rate": 8.989418407233388e-05,
      "loss": 0.814,
      "step": 5120
    },
    {
      "epoch": 0.20255063765941486,
      "grad_norm": 0.709999144077301,
      "learning_rate": 8.987444229478423e-05,
      "loss": 0.8124,
      "step": 5130
    },
    {
      "epoch": 0.20294547321040787,
      "grad_norm": 0.549847424030304,
      "learning_rate": 8.985470051723458e-05,
      "loss": 0.8033,
      "step": 5140
    },
    {
      "epoch": 0.20334030876140088,
      "grad_norm": 0.6241921186447144,
      "learning_rate": 8.983495873968493e-05,
      "loss": 0.793,
      "step": 5150
    },
    {
      "epoch": 0.2037351443123939,
      "grad_norm": 0.6629179120063782,
      "learning_rate": 8.981521696213528e-05,
      "loss": 0.7693,
      "step": 5160
    },
    {
      "epoch": 0.2041299798633869,
      "grad_norm": 1.008419156074524,
      "learning_rate": 8.979547518458563e-05,
      "loss": 0.778,
      "step": 5170
    },
    {
      "epoch": 0.20452481541437992,
      "grad_norm": 0.6363459229469299,
      "learning_rate": 8.977573340703598e-05,
      "loss": 0.8143,
      "step": 5180
    },
    {
      "epoch": 0.20491965096537293,
      "grad_norm": 0.6650004386901855,
      "learning_rate": 8.975599162948633e-05,
      "loss": 0.7752,
      "step": 5190
    },
    {
      "epoch": 0.20531448651636594,
      "grad_norm": 0.7153827548027039,
      "learning_rate": 8.973624985193668e-05,
      "loss": 0.8382,
      "step": 5200
    },
    {
      "epoch": 0.20570932206735895,
      "grad_norm": 0.5705618262290955,
      "learning_rate": 8.971650807438701e-05,
      "loss": 0.7869,
      "step": 5210
    },
    {
      "epoch": 0.20610415761835196,
      "grad_norm": 0.6656118631362915,
      "learning_rate": 8.969676629683738e-05,
      "loss": 0.7537,
      "step": 5220
    },
    {
      "epoch": 0.20649899316934497,
      "grad_norm": 0.7091801166534424,
      "learning_rate": 8.967702451928773e-05,
      "loss": 0.8123,
      "step": 5230
    },
    {
      "epoch": 0.20689382872033799,
      "grad_norm": 0.5430737733840942,
      "learning_rate": 8.965728274173808e-05,
      "loss": 0.7985,
      "step": 5240
    },
    {
      "epoch": 0.207288664271331,
      "grad_norm": 0.5531219840049744,
      "learning_rate": 8.963754096418841e-05,
      "loss": 0.7752,
      "step": 5250
    },
    {
      "epoch": 0.207683499822324,
      "grad_norm": 0.5860461592674255,
      "learning_rate": 8.961779918663878e-05,
      "loss": 0.7896,
      "step": 5260
    },
    {
      "epoch": 0.20807833537331702,
      "grad_norm": 0.49516841769218445,
      "learning_rate": 8.959805740908913e-05,
      "loss": 0.7902,
      "step": 5270
    },
    {
      "epoch": 0.20847317092431003,
      "grad_norm": 0.5466946363449097,
      "learning_rate": 8.957831563153946e-05,
      "loss": 0.7906,
      "step": 5280
    },
    {
      "epoch": 0.20886800647530304,
      "grad_norm": 0.5430125594139099,
      "learning_rate": 8.955857385398981e-05,
      "loss": 0.807,
      "step": 5290
    },
    {
      "epoch": 0.20926284202629605,
      "grad_norm": 0.5991318225860596,
      "learning_rate": 8.953883207644017e-05,
      "loss": 0.7915,
      "step": 5300
    },
    {
      "epoch": 0.20965767757728906,
      "grad_norm": 0.6142441034317017,
      "learning_rate": 8.951909029889051e-05,
      "loss": 0.7821,
      "step": 5310
    },
    {
      "epoch": 0.21005251312828208,
      "grad_norm": 0.673515796661377,
      "learning_rate": 8.949934852134086e-05,
      "loss": 0.825,
      "step": 5320
    },
    {
      "epoch": 0.2104473486792751,
      "grad_norm": 0.4733176529407501,
      "learning_rate": 8.947960674379121e-05,
      "loss": 0.8165,
      "step": 5330
    },
    {
      "epoch": 0.2108421842302681,
      "grad_norm": 0.5891547799110413,
      "learning_rate": 8.945986496624157e-05,
      "loss": 0.8306,
      "step": 5340
    },
    {
      "epoch": 0.2112370197812611,
      "grad_norm": 0.5917994976043701,
      "learning_rate": 8.944012318869191e-05,
      "loss": 0.8155,
      "step": 5350
    },
    {
      "epoch": 0.21163185533225412,
      "grad_norm": 0.6611877679824829,
      "learning_rate": 8.942038141114226e-05,
      "loss": 0.8246,
      "step": 5360
    },
    {
      "epoch": 0.21202669088324713,
      "grad_norm": 0.4994035065174103,
      "learning_rate": 8.940063963359261e-05,
      "loss": 0.7958,
      "step": 5370
    },
    {
      "epoch": 0.21242152643424014,
      "grad_norm": 0.8296348452568054,
      "learning_rate": 8.938089785604296e-05,
      "loss": 0.7116,
      "step": 5380
    },
    {
      "epoch": 0.21281636198523315,
      "grad_norm": 0.5967459082603455,
      "learning_rate": 8.936115607849331e-05,
      "loss": 0.797,
      "step": 5390
    },
    {
      "epoch": 0.21321119753622617,
      "grad_norm": 0.5552107691764832,
      "learning_rate": 8.934141430094366e-05,
      "loss": 0.8215,
      "step": 5400
    },
    {
      "epoch": 0.21360603308721918,
      "grad_norm": 0.6727850437164307,
      "learning_rate": 8.932167252339401e-05,
      "loss": 0.8356,
      "step": 5410
    },
    {
      "epoch": 0.2140008686382122,
      "grad_norm": 0.6542864441871643,
      "learning_rate": 8.930193074584436e-05,
      "loss": 0.8316,
      "step": 5420
    },
    {
      "epoch": 0.2143957041892052,
      "grad_norm": 0.6695694923400879,
      "learning_rate": 8.928218896829471e-05,
      "loss": 0.8142,
      "step": 5430
    },
    {
      "epoch": 0.2147905397401982,
      "grad_norm": 0.6729190349578857,
      "learning_rate": 8.926244719074506e-05,
      "loss": 0.8122,
      "step": 5440
    },
    {
      "epoch": 0.21518537529119122,
      "grad_norm": 0.6331930756568909,
      "learning_rate": 8.924270541319541e-05,
      "loss": 0.815,
      "step": 5450
    },
    {
      "epoch": 0.21558021084218423,
      "grad_norm": 0.6058543920516968,
      "learning_rate": 8.922296363564576e-05,
      "loss": 0.8289,
      "step": 5460
    },
    {
      "epoch": 0.21597504639317724,
      "grad_norm": 0.7081983089447021,
      "learning_rate": 8.920322185809611e-05,
      "loss": 0.8021,
      "step": 5470
    },
    {
      "epoch": 0.21636988194417026,
      "grad_norm": 0.7023096680641174,
      "learning_rate": 8.918348008054646e-05,
      "loss": 0.8076,
      "step": 5480
    },
    {
      "epoch": 0.21676471749516327,
      "grad_norm": 0.6362238526344299,
      "learning_rate": 8.91637383029968e-05,
      "loss": 0.7867,
      "step": 5490
    },
    {
      "epoch": 0.21715955304615628,
      "grad_norm": 0.67039954662323,
      "learning_rate": 8.914399652544716e-05,
      "loss": 0.7824,
      "step": 5500
    },
    {
      "epoch": 0.2175543885971493,
      "grad_norm": 0.5917444825172424,
      "learning_rate": 8.912425474789751e-05,
      "loss": 0.8376,
      "step": 5510
    },
    {
      "epoch": 0.2179492241481423,
      "grad_norm": 0.4925347566604614,
      "learning_rate": 8.910451297034786e-05,
      "loss": 0.7727,
      "step": 5520
    },
    {
      "epoch": 0.2183440596991353,
      "grad_norm": 0.6087199449539185,
      "learning_rate": 8.90847711927982e-05,
      "loss": 0.8122,
      "step": 5530
    },
    {
      "epoch": 0.21873889525012832,
      "grad_norm": 0.5293384790420532,
      "learning_rate": 8.906502941524856e-05,
      "loss": 0.8112,
      "step": 5540
    },
    {
      "epoch": 0.21913373080112133,
      "grad_norm": 0.6754276156425476,
      "learning_rate": 8.904528763769891e-05,
      "loss": 0.758,
      "step": 5550
    },
    {
      "epoch": 0.21952856635211435,
      "grad_norm": 0.5065464377403259,
      "learning_rate": 8.902554586014925e-05,
      "loss": 0.7807,
      "step": 5560
    },
    {
      "epoch": 0.21992340190310736,
      "grad_norm": 0.6496596932411194,
      "learning_rate": 8.90058040825996e-05,
      "loss": 0.8011,
      "step": 5570
    },
    {
      "epoch": 0.22031823745410037,
      "grad_norm": 0.8515673875808716,
      "learning_rate": 8.898606230504996e-05,
      "loss": 0.7552,
      "step": 5580
    },
    {
      "epoch": 0.22071307300509338,
      "grad_norm": 0.8546050786972046,
      "learning_rate": 8.896632052750031e-05,
      "loss": 0.7991,
      "step": 5590
    },
    {
      "epoch": 0.2211079085560864,
      "grad_norm": 0.727332592010498,
      "learning_rate": 8.894657874995065e-05,
      "loss": 0.8169,
      "step": 5600
    },
    {
      "epoch": 0.2215027441070794,
      "grad_norm": 0.7256558537483215,
      "learning_rate": 8.8926836972401e-05,
      "loss": 0.7851,
      "step": 5610
    },
    {
      "epoch": 0.2218975796580724,
      "grad_norm": 0.5621145367622375,
      "learning_rate": 8.890709519485136e-05,
      "loss": 0.8013,
      "step": 5620
    },
    {
      "epoch": 0.22229241520906542,
      "grad_norm": 0.5867154002189636,
      "learning_rate": 8.88873534173017e-05,
      "loss": 0.8038,
      "step": 5630
    },
    {
      "epoch": 0.22268725076005844,
      "grad_norm": 0.5431990623474121,
      "learning_rate": 8.886761163975204e-05,
      "loss": 0.7777,
      "step": 5640
    },
    {
      "epoch": 0.22308208631105145,
      "grad_norm": 0.6644237637519836,
      "learning_rate": 8.88478698622024e-05,
      "loss": 0.8199,
      "step": 5650
    },
    {
      "epoch": 0.22347692186204446,
      "grad_norm": 0.43379178643226624,
      "learning_rate": 8.882812808465274e-05,
      "loss": 0.7634,
      "step": 5660
    },
    {
      "epoch": 0.22387175741303747,
      "grad_norm": 0.648430585861206,
      "learning_rate": 8.88083863071031e-05,
      "loss": 0.7939,
      "step": 5670
    },
    {
      "epoch": 0.22426659296403048,
      "grad_norm": 0.5108388662338257,
      "learning_rate": 8.878864452955344e-05,
      "loss": 0.7541,
      "step": 5680
    },
    {
      "epoch": 0.2246614285150235,
      "grad_norm": 0.5569992065429688,
      "learning_rate": 8.87689027520038e-05,
      "loss": 0.7744,
      "step": 5690
    },
    {
      "epoch": 0.2250562640660165,
      "grad_norm": 0.8237578868865967,
      "learning_rate": 8.874916097445414e-05,
      "loss": 0.7659,
      "step": 5700
    },
    {
      "epoch": 0.22545109961700951,
      "grad_norm": 0.6070302724838257,
      "learning_rate": 8.87294191969045e-05,
      "loss": 0.7242,
      "step": 5710
    },
    {
      "epoch": 0.22584593516800253,
      "grad_norm": 0.6378762125968933,
      "learning_rate": 8.870967741935484e-05,
      "loss": 0.7792,
      "step": 5720
    },
    {
      "epoch": 0.22624077071899554,
      "grad_norm": 0.5589701533317566,
      "learning_rate": 8.86899356418052e-05,
      "loss": 0.8214,
      "step": 5730
    },
    {
      "epoch": 0.22663560626998855,
      "grad_norm": 0.5840030312538147,
      "learning_rate": 8.867019386425554e-05,
      "loss": 0.7937,
      "step": 5740
    },
    {
      "epoch": 0.22703044182098156,
      "grad_norm": 0.5258764624595642,
      "learning_rate": 8.86504520867059e-05,
      "loss": 0.7956,
      "step": 5750
    },
    {
      "epoch": 0.22742527737197457,
      "grad_norm": 0.5434988737106323,
      "learning_rate": 8.863071030915624e-05,
      "loss": 0.7883,
      "step": 5760
    },
    {
      "epoch": 0.22782011292296758,
      "grad_norm": 0.5814865827560425,
      "learning_rate": 8.861096853160658e-05,
      "loss": 0.7575,
      "step": 5770
    },
    {
      "epoch": 0.2282149484739606,
      "grad_norm": 0.5900622010231018,
      "learning_rate": 8.859122675405694e-05,
      "loss": 0.803,
      "step": 5780
    },
    {
      "epoch": 0.2286097840249536,
      "grad_norm": 0.6024449467658997,
      "learning_rate": 8.85714849765073e-05,
      "loss": 0.8093,
      "step": 5790
    },
    {
      "epoch": 0.22900461957594662,
      "grad_norm": 0.6252965927124023,
      "learning_rate": 8.855174319895764e-05,
      "loss": 0.7792,
      "step": 5800
    },
    {
      "epoch": 0.22939945512693963,
      "grad_norm": 0.579966127872467,
      "learning_rate": 8.853200142140798e-05,
      "loss": 0.7954,
      "step": 5810
    },
    {
      "epoch": 0.22979429067793264,
      "grad_norm": 0.6881762742996216,
      "learning_rate": 8.851225964385834e-05,
      "loss": 0.7919,
      "step": 5820
    },
    {
      "epoch": 0.23018912622892565,
      "grad_norm": 0.7193084955215454,
      "learning_rate": 8.849251786630869e-05,
      "loss": 0.7933,
      "step": 5830
    },
    {
      "epoch": 0.23058396177991866,
      "grad_norm": 0.629119336605072,
      "learning_rate": 8.847277608875903e-05,
      "loss": 0.7599,
      "step": 5840
    },
    {
      "epoch": 0.23097879733091167,
      "grad_norm": 0.5545300841331482,
      "learning_rate": 8.845303431120938e-05,
      "loss": 0.7937,
      "step": 5850
    },
    {
      "epoch": 0.23137363288190468,
      "grad_norm": 0.6092456579208374,
      "learning_rate": 8.843329253365974e-05,
      "loss": 0.8195,
      "step": 5860
    },
    {
      "epoch": 0.2317684684328977,
      "grad_norm": 0.862747848033905,
      "learning_rate": 8.841355075611009e-05,
      "loss": 0.8441,
      "step": 5870
    },
    {
      "epoch": 0.2321633039838907,
      "grad_norm": 0.5351832509040833,
      "learning_rate": 8.839380897856043e-05,
      "loss": 0.7865,
      "step": 5880
    },
    {
      "epoch": 0.23255813953488372,
      "grad_norm": 0.6821218729019165,
      "learning_rate": 8.837406720101078e-05,
      "loss": 0.8185,
      "step": 5890
    },
    {
      "epoch": 0.23295297508587673,
      "grad_norm": 0.6749995350837708,
      "learning_rate": 8.835432542346114e-05,
      "loss": 0.8238,
      "step": 5900
    },
    {
      "epoch": 0.23334781063686974,
      "grad_norm": 0.6510782241821289,
      "learning_rate": 8.833458364591148e-05,
      "loss": 0.8141,
      "step": 5910
    },
    {
      "epoch": 0.23374264618786275,
      "grad_norm": 0.6068665981292725,
      "learning_rate": 8.831484186836183e-05,
      "loss": 0.7842,
      "step": 5920
    },
    {
      "epoch": 0.23413748173885576,
      "grad_norm": 0.5273008942604065,
      "learning_rate": 8.829510009081218e-05,
      "loss": 0.7889,
      "step": 5930
    },
    {
      "epoch": 0.23453231728984877,
      "grad_norm": 0.6071767210960388,
      "learning_rate": 8.827535831326253e-05,
      "loss": 0.8151,
      "step": 5940
    },
    {
      "epoch": 0.23492715284084179,
      "grad_norm": 0.7342981100082397,
      "learning_rate": 8.825561653571288e-05,
      "loss": 0.8312,
      "step": 5950
    },
    {
      "epoch": 0.2353219883918348,
      "grad_norm": 0.7072910070419312,
      "learning_rate": 8.823587475816323e-05,
      "loss": 0.8029,
      "step": 5960
    },
    {
      "epoch": 0.2357168239428278,
      "grad_norm": 0.6383873224258423,
      "learning_rate": 8.821613298061358e-05,
      "loss": 0.806,
      "step": 5970
    },
    {
      "epoch": 0.23611165949382082,
      "grad_norm": 0.6611731052398682,
      "learning_rate": 8.819639120306393e-05,
      "loss": 0.7893,
      "step": 5980
    },
    {
      "epoch": 0.23650649504481383,
      "grad_norm": 0.654696524143219,
      "learning_rate": 8.817664942551428e-05,
      "loss": 0.7874,
      "step": 5990
    },
    {
      "epoch": 0.23690133059580684,
      "grad_norm": 0.8103907704353333,
      "learning_rate": 8.815690764796463e-05,
      "loss": 0.7881,
      "step": 6000
    },
    {
      "epoch": 0.23690133059580684,
      "eval_loss": 0.8194077014923096,
      "eval_runtime": 1219.7417,
      "eval_samples_per_second": 9.229,
      "eval_steps_per_second": 9.229,
      "step": 6000
    },
    {
      "epoch": 0.23729616614679985,
      "grad_norm": 0.6348736882209778,
      "learning_rate": 8.813716587041498e-05,
      "loss": 0.7959,
      "step": 6010
    },
    {
      "epoch": 0.23769100169779286,
      "grad_norm": 0.7516477108001709,
      "learning_rate": 8.811742409286533e-05,
      "loss": 0.8233,
      "step": 6020
    },
    {
      "epoch": 0.23808583724878588,
      "grad_norm": 0.7545644044876099,
      "learning_rate": 8.809768231531568e-05,
      "loss": 0.8326,
      "step": 6030
    },
    {
      "epoch": 0.2384806727997789,
      "grad_norm": 0.6232872605323792,
      "learning_rate": 8.807794053776603e-05,
      "loss": 0.8129,
      "step": 6040
    },
    {
      "epoch": 0.2388755083507719,
      "grad_norm": 0.8146457672119141,
      "learning_rate": 8.805819876021636e-05,
      "loss": 0.7952,
      "step": 6050
    },
    {
      "epoch": 0.2392703439017649,
      "grad_norm": 0.6821877956390381,
      "learning_rate": 8.803845698266673e-05,
      "loss": 0.7764,
      "step": 6060
    },
    {
      "epoch": 0.23966517945275792,
      "grad_norm": 0.6610566973686218,
      "learning_rate": 8.801871520511708e-05,
      "loss": 0.7859,
      "step": 6070
    },
    {
      "epoch": 0.24006001500375093,
      "grad_norm": 0.8070412278175354,
      "learning_rate": 8.799897342756743e-05,
      "loss": 0.7722,
      "step": 6080
    },
    {
      "epoch": 0.24045485055474394,
      "grad_norm": 0.7285369634628296,
      "learning_rate": 8.797923165001776e-05,
      "loss": 0.8237,
      "step": 6090
    },
    {
      "epoch": 0.24084968610573695,
      "grad_norm": 0.8261421322822571,
      "learning_rate": 8.795948987246813e-05,
      "loss": 0.7856,
      "step": 6100
    },
    {
      "epoch": 0.24124452165672997,
      "grad_norm": 0.3826836049556732,
      "learning_rate": 8.793974809491848e-05,
      "loss": 0.7556,
      "step": 6110
    },
    {
      "epoch": 0.24163935720772298,
      "grad_norm": 0.6376782059669495,
      "learning_rate": 8.792000631736881e-05,
      "loss": 0.8078,
      "step": 6120
    },
    {
      "epoch": 0.242034192758716,
      "grad_norm": 0.5229352116584778,
      "learning_rate": 8.790026453981916e-05,
      "loss": 0.7971,
      "step": 6130
    },
    {
      "epoch": 0.242429028309709,
      "grad_norm": 0.6309124231338501,
      "learning_rate": 8.788052276226953e-05,
      "loss": 0.7504,
      "step": 6140
    },
    {
      "epoch": 0.242823863860702,
      "grad_norm": 0.6249065399169922,
      "learning_rate": 8.786078098471988e-05,
      "loss": 0.8587,
      "step": 6150
    },
    {
      "epoch": 0.24321869941169502,
      "grad_norm": 0.746279239654541,
      "learning_rate": 8.784103920717021e-05,
      "loss": 0.8258,
      "step": 6160
    },
    {
      "epoch": 0.24361353496268803,
      "grad_norm": 0.6881533861160278,
      "learning_rate": 8.782129742962056e-05,
      "loss": 0.7969,
      "step": 6170
    },
    {
      "epoch": 0.24400837051368104,
      "grad_norm": 0.7120710015296936,
      "learning_rate": 8.780155565207093e-05,
      "loss": 0.7816,
      "step": 6180
    },
    {
      "epoch": 0.24440320606467406,
      "grad_norm": 0.5612241625785828,
      "learning_rate": 8.778181387452126e-05,
      "loss": 0.8171,
      "step": 6190
    },
    {
      "epoch": 0.24479804161566707,
      "grad_norm": 0.6515507698059082,
      "learning_rate": 8.776207209697161e-05,
      "loss": 0.7988,
      "step": 6200
    },
    {
      "epoch": 0.24519287716666008,
      "grad_norm": 0.5157844424247742,
      "learning_rate": 8.774233031942196e-05,
      "loss": 0.7553,
      "step": 6210
    },
    {
      "epoch": 0.2455877127176531,
      "grad_norm": 0.6330558657646179,
      "learning_rate": 8.772258854187231e-05,
      "loss": 0.7877,
      "step": 6220
    },
    {
      "epoch": 0.2459825482686461,
      "grad_norm": 0.9114686846733093,
      "learning_rate": 8.770284676432266e-05,
      "loss": 0.8031,
      "step": 6230
    },
    {
      "epoch": 0.2463773838196391,
      "grad_norm": 0.7683405876159668,
      "learning_rate": 8.768310498677301e-05,
      "loss": 0.8344,
      "step": 6240
    },
    {
      "epoch": 0.24677221937063212,
      "grad_norm": 0.6459365487098694,
      "learning_rate": 8.766336320922336e-05,
      "loss": 0.8891,
      "step": 6250
    },
    {
      "epoch": 0.24716705492162513,
      "grad_norm": 0.5576902627944946,
      "learning_rate": 8.764362143167371e-05,
      "loss": 0.7942,
      "step": 6260
    },
    {
      "epoch": 0.24756189047261815,
      "grad_norm": 0.6279245615005493,
      "learning_rate": 8.762387965412406e-05,
      "loss": 0.7817,
      "step": 6270
    },
    {
      "epoch": 0.24795672602361116,
      "grad_norm": 0.6092052459716797,
      "learning_rate": 8.760413787657441e-05,
      "loss": 0.7634,
      "step": 6280
    },
    {
      "epoch": 0.24835156157460417,
      "grad_norm": 0.7063891887664795,
      "learning_rate": 8.758439609902476e-05,
      "loss": 0.8144,
      "step": 6290
    },
    {
      "epoch": 0.24874639712559718,
      "grad_norm": 0.715153694152832,
      "learning_rate": 8.756465432147511e-05,
      "loss": 0.7578,
      "step": 6300
    },
    {
      "epoch": 0.2491412326765902,
      "grad_norm": 0.5818631052970886,
      "learning_rate": 8.754491254392546e-05,
      "loss": 0.8098,
      "step": 6310
    },
    {
      "epoch": 0.2495360682275832,
      "grad_norm": 0.929878830909729,
      "learning_rate": 8.752517076637581e-05,
      "loss": 0.8103,
      "step": 6320
    },
    {
      "epoch": 0.2499309037785762,
      "grad_norm": 0.5327803492546082,
      "learning_rate": 8.750542898882615e-05,
      "loss": 0.8052,
      "step": 6330
    },
    {
      "epoch": 0.25032573932956925,
      "grad_norm": 0.659476101398468,
      "learning_rate": 8.748568721127651e-05,
      "loss": 0.8013,
      "step": 6340
    },
    {
      "epoch": 0.25072057488056226,
      "grad_norm": 0.753939688205719,
      "learning_rate": 8.746594543372686e-05,
      "loss": 0.8605,
      "step": 6350
    },
    {
      "epoch": 0.2511154104315553,
      "grad_norm": 0.6291862726211548,
      "learning_rate": 8.744620365617721e-05,
      "loss": 0.774,
      "step": 6360
    },
    {
      "epoch": 0.2515102459825483,
      "grad_norm": 0.8154044151306152,
      "learning_rate": 8.742646187862755e-05,
      "loss": 0.7962,
      "step": 6370
    },
    {
      "epoch": 0.2519050815335413,
      "grad_norm": 0.6511362791061401,
      "learning_rate": 8.740672010107791e-05,
      "loss": 0.7801,
      "step": 6380
    },
    {
      "epoch": 0.2522999170845343,
      "grad_norm": 0.5606852769851685,
      "learning_rate": 8.738697832352826e-05,
      "loss": 0.825,
      "step": 6390
    },
    {
      "epoch": 0.2526947526355273,
      "grad_norm": 0.5567122101783752,
      "learning_rate": 8.73672365459786e-05,
      "loss": 0.7946,
      "step": 6400
    },
    {
      "epoch": 0.25308958818652033,
      "grad_norm": 0.6536020636558533,
      "learning_rate": 8.734749476842895e-05,
      "loss": 0.796,
      "step": 6410
    },
    {
      "epoch": 0.25348442373751334,
      "grad_norm": 0.6554883718490601,
      "learning_rate": 8.732775299087931e-05,
      "loss": 0.7657,
      "step": 6420
    },
    {
      "epoch": 0.25387925928850635,
      "grad_norm": 0.6002197861671448,
      "learning_rate": 8.730801121332966e-05,
      "loss": 0.7179,
      "step": 6430
    },
    {
      "epoch": 0.25427409483949936,
      "grad_norm": 0.7788870334625244,
      "learning_rate": 8.728826943578e-05,
      "loss": 0.7943,
      "step": 6440
    },
    {
      "epoch": 0.2546689303904924,
      "grad_norm": 0.5303260087966919,
      "learning_rate": 8.726852765823035e-05,
      "loss": 0.8135,
      "step": 6450
    },
    {
      "epoch": 0.2550637659414854,
      "grad_norm": 0.5857027769088745,
      "learning_rate": 8.724878588068071e-05,
      "loss": 0.7723,
      "step": 6460
    },
    {
      "epoch": 0.2554586014924784,
      "grad_norm": 0.5805368423461914,
      "learning_rate": 8.722904410313105e-05,
      "loss": 0.838,
      "step": 6470
    },
    {
      "epoch": 0.2558534370434714,
      "grad_norm": 0.6853505373001099,
      "learning_rate": 8.72093023255814e-05,
      "loss": 0.7985,
      "step": 6480
    },
    {
      "epoch": 0.2562482725944644,
      "grad_norm": 0.7594323754310608,
      "learning_rate": 8.718956054803175e-05,
      "loss": 0.7653,
      "step": 6490
    },
    {
      "epoch": 0.25664310814545743,
      "grad_norm": 0.6457082033157349,
      "learning_rate": 8.71698187704821e-05,
      "loss": 0.7742,
      "step": 6500
    },
    {
      "epoch": 0.25703794369645044,
      "grad_norm": 0.7824196815490723,
      "learning_rate": 8.715007699293245e-05,
      "loss": 0.8621,
      "step": 6510
    },
    {
      "epoch": 0.25743277924744346,
      "grad_norm": 0.6663336753845215,
      "learning_rate": 8.71303352153828e-05,
      "loss": 0.7931,
      "step": 6520
    },
    {
      "epoch": 0.25782761479843647,
      "grad_norm": 0.5547598600387573,
      "learning_rate": 8.711059343783315e-05,
      "loss": 0.7538,
      "step": 6530
    },
    {
      "epoch": 0.2582224503494295,
      "grad_norm": 0.49221140146255493,
      "learning_rate": 8.70908516602835e-05,
      "loss": 0.8076,
      "step": 6540
    },
    {
      "epoch": 0.2586172859004225,
      "grad_norm": 0.5861372947692871,
      "learning_rate": 8.707110988273385e-05,
      "loss": 0.8067,
      "step": 6550
    },
    {
      "epoch": 0.2590121214514155,
      "grad_norm": 0.5462744832038879,
      "learning_rate": 8.70513681051842e-05,
      "loss": 0.8294,
      "step": 6560
    },
    {
      "epoch": 0.2594069570024085,
      "grad_norm": 0.5581977367401123,
      "learning_rate": 8.703162632763455e-05,
      "loss": 0.7735,
      "step": 6570
    },
    {
      "epoch": 0.2598017925534015,
      "grad_norm": 0.6058467626571655,
      "learning_rate": 8.70118845500849e-05,
      "loss": 0.7929,
      "step": 6580
    },
    {
      "epoch": 0.26019662810439453,
      "grad_norm": 0.6107333302497864,
      "learning_rate": 8.699214277253525e-05,
      "loss": 0.8065,
      "step": 6590
    },
    {
      "epoch": 0.26059146365538755,
      "grad_norm": 0.5683443546295166,
      "learning_rate": 8.69724009949856e-05,
      "loss": 0.8361,
      "step": 6600
    },
    {
      "epoch": 0.26098629920638056,
      "grad_norm": 0.6443132162094116,
      "learning_rate": 8.695265921743593e-05,
      "loss": 0.7521,
      "step": 6610
    },
    {
      "epoch": 0.26138113475737357,
      "grad_norm": 0.5914354920387268,
      "learning_rate": 8.69329174398863e-05,
      "loss": 0.8263,
      "step": 6620
    },
    {
      "epoch": 0.2617759703083666,
      "grad_norm": 0.6580994129180908,
      "learning_rate": 8.691317566233665e-05,
      "loss": 0.8344,
      "step": 6630
    },
    {
      "epoch": 0.2621708058593596,
      "grad_norm": 0.677246630191803,
      "learning_rate": 8.6893433884787e-05,
      "loss": 0.8593,
      "step": 6640
    },
    {
      "epoch": 0.2625656414103526,
      "grad_norm": 0.8826324939727783,
      "learning_rate": 8.687369210723733e-05,
      "loss": 0.8472,
      "step": 6650
    },
    {
      "epoch": 0.2629604769613456,
      "grad_norm": 0.7279840111732483,
      "learning_rate": 8.68539503296877e-05,
      "loss": 0.8077,
      "step": 6660
    },
    {
      "epoch": 0.2633553125123386,
      "grad_norm": 0.7537049055099487,
      "learning_rate": 8.683420855213805e-05,
      "loss": 0.7908,
      "step": 6670
    },
    {
      "epoch": 0.26375014806333164,
      "grad_norm": 0.625728189945221,
      "learning_rate": 8.681446677458838e-05,
      "loss": 0.7559,
      "step": 6680
    },
    {
      "epoch": 0.26414498361432465,
      "grad_norm": 0.7412082552909851,
      "learning_rate": 8.679472499703873e-05,
      "loss": 0.8685,
      "step": 6690
    },
    {
      "epoch": 0.26453981916531766,
      "grad_norm": 0.5586220622062683,
      "learning_rate": 8.67749832194891e-05,
      "loss": 0.7506,
      "step": 6700
    },
    {
      "epoch": 0.26493465471631067,
      "grad_norm": 0.8174424767494202,
      "learning_rate": 8.675524144193944e-05,
      "loss": 0.7616,
      "step": 6710
    },
    {
      "epoch": 0.2653294902673037,
      "grad_norm": 0.5603856444358826,
      "learning_rate": 8.673549966438978e-05,
      "loss": 0.8094,
      "step": 6720
    },
    {
      "epoch": 0.2657243258182967,
      "grad_norm": 0.5198377966880798,
      "learning_rate": 8.671575788684013e-05,
      "loss": 0.7566,
      "step": 6730
    },
    {
      "epoch": 0.2661191613692897,
      "grad_norm": 0.5876734256744385,
      "learning_rate": 8.66960161092905e-05,
      "loss": 0.8077,
      "step": 6740
    },
    {
      "epoch": 0.2665139969202827,
      "grad_norm": 0.6167751550674438,
      "learning_rate": 8.667627433174083e-05,
      "loss": 0.7503,
      "step": 6750
    },
    {
      "epoch": 0.2669088324712757,
      "grad_norm": 0.4956125020980835,
      "learning_rate": 8.665653255419118e-05,
      "loss": 0.7751,
      "step": 6760
    },
    {
      "epoch": 0.26730366802226874,
      "grad_norm": 0.6498464941978455,
      "learning_rate": 8.663679077664153e-05,
      "loss": 0.7818,
      "step": 6770
    },
    {
      "epoch": 0.26769850357326175,
      "grad_norm": 0.5790407061576843,
      "learning_rate": 8.66170489990919e-05,
      "loss": 0.8061,
      "step": 6780
    },
    {
      "epoch": 0.26809333912425476,
      "grad_norm": 0.5765261650085449,
      "learning_rate": 8.659730722154223e-05,
      "loss": 0.7734,
      "step": 6790
    },
    {
      "epoch": 0.26848817467524777,
      "grad_norm": 0.6537556648254395,
      "learning_rate": 8.657756544399258e-05,
      "loss": 0.798,
      "step": 6800
    },
    {
      "epoch": 0.2688830102262408,
      "grad_norm": 0.6290229558944702,
      "learning_rate": 8.655782366644293e-05,
      "loss": 0.7817,
      "step": 6810
    },
    {
      "epoch": 0.2692778457772338,
      "grad_norm": 0.7682828903198242,
      "learning_rate": 8.653808188889328e-05,
      "loss": 0.7469,
      "step": 6820
    },
    {
      "epoch": 0.2696726813282268,
      "grad_norm": 0.5551294684410095,
      "learning_rate": 8.651834011134363e-05,
      "loss": 0.8362,
      "step": 6830
    },
    {
      "epoch": 0.2700675168792198,
      "grad_norm": 0.682253360748291,
      "learning_rate": 8.649859833379398e-05,
      "loss": 0.8073,
      "step": 6840
    },
    {
      "epoch": 0.2704623524302128,
      "grad_norm": 0.6768898367881775,
      "learning_rate": 8.647885655624433e-05,
      "loss": 0.7864,
      "step": 6850
    },
    {
      "epoch": 0.27085718798120584,
      "grad_norm": 0.6993918418884277,
      "learning_rate": 8.645911477869468e-05,
      "loss": 0.7838,
      "step": 6860
    },
    {
      "epoch": 0.27125202353219885,
      "grad_norm": 0.4853118062019348,
      "learning_rate": 8.643937300114503e-05,
      "loss": 0.7659,
      "step": 6870
    },
    {
      "epoch": 0.27164685908319186,
      "grad_norm": 0.6808507442474365,
      "learning_rate": 8.641963122359538e-05,
      "loss": 0.8546,
      "step": 6880
    },
    {
      "epoch": 0.27204169463418487,
      "grad_norm": 0.674784779548645,
      "learning_rate": 8.639988944604573e-05,
      "loss": 0.8115,
      "step": 6890
    },
    {
      "epoch": 0.2724365301851779,
      "grad_norm": 0.5799802541732788,
      "learning_rate": 8.638014766849608e-05,
      "loss": 0.7971,
      "step": 6900
    },
    {
      "epoch": 0.2728313657361709,
      "grad_norm": 0.641696572303772,
      "learning_rate": 8.636040589094643e-05,
      "loss": 0.8185,
      "step": 6910
    },
    {
      "epoch": 0.2732262012871639,
      "grad_norm": 0.511088490486145,
      "learning_rate": 8.634066411339678e-05,
      "loss": 0.7636,
      "step": 6920
    },
    {
      "epoch": 0.2736210368381569,
      "grad_norm": 0.7514007687568665,
      "learning_rate": 8.632092233584712e-05,
      "loss": 0.7837,
      "step": 6930
    },
    {
      "epoch": 0.27401587238914993,
      "grad_norm": 0.5553154349327087,
      "learning_rate": 8.630118055829748e-05,
      "loss": 0.7837,
      "step": 6940
    },
    {
      "epoch": 0.27441070794014294,
      "grad_norm": 0.40211591124534607,
      "learning_rate": 8.628143878074783e-05,
      "loss": 0.8206,
      "step": 6950
    },
    {
      "epoch": 0.27480554349113595,
      "grad_norm": 0.5312802195549011,
      "learning_rate": 8.626169700319817e-05,
      "loss": 0.7575,
      "step": 6960
    },
    {
      "epoch": 0.27520037904212896,
      "grad_norm": 0.5717157125473022,
      "learning_rate": 8.624195522564852e-05,
      "loss": 0.7607,
      "step": 6970
    },
    {
      "epoch": 0.275595214593122,
      "grad_norm": 0.5443392992019653,
      "learning_rate": 8.622221344809888e-05,
      "loss": 0.8424,
      "step": 6980
    },
    {
      "epoch": 0.275990050144115,
      "grad_norm": 0.751955509185791,
      "learning_rate": 8.620247167054923e-05,
      "loss": 0.8389,
      "step": 6990
    },
    {
      "epoch": 0.276384885695108,
      "grad_norm": 0.500495195388794,
      "learning_rate": 8.618272989299957e-05,
      "loss": 0.7536,
      "step": 7000
    },
    {
      "epoch": 0.276384885695108,
      "eval_loss": 0.8140842318534851,
      "eval_runtime": 1220.2908,
      "eval_samples_per_second": 9.225,
      "eval_steps_per_second": 9.225,
      "step": 7000
    },
    {
      "epoch": 0.276779721246101,
      "grad_norm": 0.7585112452507019,
      "learning_rate": 8.616298811544992e-05,
      "loss": 0.8309,
      "step": 7010
    },
    {
      "epoch": 0.277174556797094,
      "grad_norm": 0.5243280529975891,
      "learning_rate": 8.614324633790028e-05,
      "loss": 0.7471,
      "step": 7020
    },
    {
      "epoch": 0.27756939234808703,
      "grad_norm": 0.7161181569099426,
      "learning_rate": 8.612350456035062e-05,
      "loss": 0.8106,
      "step": 7030
    },
    {
      "epoch": 0.27796422789908004,
      "grad_norm": 0.510201632976532,
      "learning_rate": 8.610376278280096e-05,
      "loss": 0.8239,
      "step": 7040
    },
    {
      "epoch": 0.27835906345007305,
      "grad_norm": 0.538231611251831,
      "learning_rate": 8.608402100525131e-05,
      "loss": 0.7857,
      "step": 7050
    },
    {
      "epoch": 0.27875389900106606,
      "grad_norm": 0.8185068964958191,
      "learning_rate": 8.606427922770168e-05,
      "loss": 0.8242,
      "step": 7060
    },
    {
      "epoch": 0.2791487345520591,
      "grad_norm": 0.49781548976898193,
      "learning_rate": 8.604453745015201e-05,
      "loss": 0.7704,
      "step": 7070
    },
    {
      "epoch": 0.2795435701030521,
      "grad_norm": 0.655985414981842,
      "learning_rate": 8.602479567260236e-05,
      "loss": 0.7723,
      "step": 7080
    },
    {
      "epoch": 0.2799384056540451,
      "grad_norm": 0.5535001754760742,
      "learning_rate": 8.600505389505271e-05,
      "loss": 0.8049,
      "step": 7090
    },
    {
      "epoch": 0.2803332412050381,
      "grad_norm": 0.7132011651992798,
      "learning_rate": 8.598531211750306e-05,
      "loss": 0.7982,
      "step": 7100
    },
    {
      "epoch": 0.2807280767560311,
      "grad_norm": 0.5592545866966248,
      "learning_rate": 8.596557033995341e-05,
      "loss": 0.7654,
      "step": 7110
    },
    {
      "epoch": 0.28112291230702413,
      "grad_norm": 0.49806109070777893,
      "learning_rate": 8.594582856240376e-05,
      "loss": 0.7713,
      "step": 7120
    },
    {
      "epoch": 0.28151774785801714,
      "grad_norm": 0.6395636796951294,
      "learning_rate": 8.592608678485411e-05,
      "loss": 0.8025,
      "step": 7130
    },
    {
      "epoch": 0.28191258340901015,
      "grad_norm": 0.6453819870948792,
      "learning_rate": 8.590634500730446e-05,
      "loss": 0.7962,
      "step": 7140
    },
    {
      "epoch": 0.28230741896000316,
      "grad_norm": 0.6771965622901917,
      "learning_rate": 8.588660322975481e-05,
      "loss": 0.777,
      "step": 7150
    },
    {
      "epoch": 0.2827022545109962,
      "grad_norm": 0.5669677257537842,
      "learning_rate": 8.586686145220516e-05,
      "loss": 0.8315,
      "step": 7160
    },
    {
      "epoch": 0.2830970900619892,
      "grad_norm": 0.6650518178939819,
      "learning_rate": 8.584711967465551e-05,
      "loss": 0.802,
      "step": 7170
    },
    {
      "epoch": 0.2834919256129822,
      "grad_norm": 0.4986415207386017,
      "learning_rate": 8.582737789710586e-05,
      "loss": 0.7732,
      "step": 7180
    },
    {
      "epoch": 0.2838867611639752,
      "grad_norm": 0.6900693774223328,
      "learning_rate": 8.580763611955621e-05,
      "loss": 0.7695,
      "step": 7190
    },
    {
      "epoch": 0.2842815967149682,
      "grad_norm": 0.6457132697105408,
      "learning_rate": 8.578789434200656e-05,
      "loss": 0.8117,
      "step": 7200
    },
    {
      "epoch": 0.28467643226596123,
      "grad_norm": 0.5705826282501221,
      "learning_rate": 8.57681525644569e-05,
      "loss": 0.8071,
      "step": 7210
    },
    {
      "epoch": 0.28507126781695424,
      "grad_norm": 0.6348828673362732,
      "learning_rate": 8.574841078690725e-05,
      "loss": 0.8117,
      "step": 7220
    },
    {
      "epoch": 0.28546610336794725,
      "grad_norm": 0.6859167218208313,
      "learning_rate": 8.572866900935761e-05,
      "loss": 0.7893,
      "step": 7230
    },
    {
      "epoch": 0.28586093891894027,
      "grad_norm": 0.8916476368904114,
      "learning_rate": 8.570892723180795e-05,
      "loss": 0.8202,
      "step": 7240
    },
    {
      "epoch": 0.2862557744699333,
      "grad_norm": 0.6806737184524536,
      "learning_rate": 8.56891854542583e-05,
      "loss": 0.7599,
      "step": 7250
    },
    {
      "epoch": 0.2866506100209263,
      "grad_norm": 0.6549164056777954,
      "learning_rate": 8.566944367670865e-05,
      "loss": 0.8274,
      "step": 7260
    },
    {
      "epoch": 0.2870454455719193,
      "grad_norm": 0.6948262453079224,
      "learning_rate": 8.564970189915901e-05,
      "loss": 0.8122,
      "step": 7270
    },
    {
      "epoch": 0.2874402811229123,
      "grad_norm": 0.6640480756759644,
      "learning_rate": 8.562996012160935e-05,
      "loss": 0.8012,
      "step": 7280
    },
    {
      "epoch": 0.2878351166739053,
      "grad_norm": 0.6857892870903015,
      "learning_rate": 8.56102183440597e-05,
      "loss": 0.8074,
      "step": 7290
    },
    {
      "epoch": 0.28822995222489833,
      "grad_norm": 0.570790708065033,
      "learning_rate": 8.559047656651005e-05,
      "loss": 0.7996,
      "step": 7300
    },
    {
      "epoch": 0.28862478777589134,
      "grad_norm": 0.60539311170578,
      "learning_rate": 8.55707347889604e-05,
      "loss": 0.7951,
      "step": 7310
    },
    {
      "epoch": 0.28901962332688436,
      "grad_norm": 0.6039170026779175,
      "learning_rate": 8.555099301141075e-05,
      "loss": 0.8084,
      "step": 7320
    },
    {
      "epoch": 0.28941445887787737,
      "grad_norm": 0.5788591504096985,
      "learning_rate": 8.55312512338611e-05,
      "loss": 0.8318,
      "step": 7330
    },
    {
      "epoch": 0.2898092944288704,
      "grad_norm": 0.5859770774841309,
      "learning_rate": 8.551150945631145e-05,
      "loss": 0.821,
      "step": 7340
    },
    {
      "epoch": 0.2902041299798634,
      "grad_norm": 0.5981845855712891,
      "learning_rate": 8.54917676787618e-05,
      "loss": 0.756,
      "step": 7350
    },
    {
      "epoch": 0.2905989655308564,
      "grad_norm": 0.5204379558563232,
      "learning_rate": 8.547202590121215e-05,
      "loss": 0.7706,
      "step": 7360
    },
    {
      "epoch": 0.2909938010818494,
      "grad_norm": 0.5747259855270386,
      "learning_rate": 8.54522841236625e-05,
      "loss": 0.7785,
      "step": 7370
    },
    {
      "epoch": 0.2913886366328424,
      "grad_norm": 0.6105785965919495,
      "learning_rate": 8.543254234611285e-05,
      "loss": 0.7622,
      "step": 7380
    },
    {
      "epoch": 0.29178347218383544,
      "grad_norm": 0.6381561756134033,
      "learning_rate": 8.54128005685632e-05,
      "loss": 0.8121,
      "step": 7390
    },
    {
      "epoch": 0.29217830773482845,
      "grad_norm": 0.6044787168502808,
      "learning_rate": 8.539305879101355e-05,
      "loss": 0.7784,
      "step": 7400
    },
    {
      "epoch": 0.29257314328582146,
      "grad_norm": 0.696319580078125,
      "learning_rate": 8.53733170134639e-05,
      "loss": 0.8343,
      "step": 7410
    },
    {
      "epoch": 0.29296797883681447,
      "grad_norm": 0.5848259925842285,
      "learning_rate": 8.535357523591423e-05,
      "loss": 0.7688,
      "step": 7420
    },
    {
      "epoch": 0.2933628143878075,
      "grad_norm": 0.5940662026405334,
      "learning_rate": 8.53338334583646e-05,
      "loss": 0.7935,
      "step": 7430
    },
    {
      "epoch": 0.2937576499388005,
      "grad_norm": 0.6548161506652832,
      "learning_rate": 8.531409168081495e-05,
      "loss": 0.7445,
      "step": 7440
    },
    {
      "epoch": 0.2941524854897935,
      "grad_norm": 0.4915376603603363,
      "learning_rate": 8.52943499032653e-05,
      "loss": 0.774,
      "step": 7450
    },
    {
      "epoch": 0.2945473210407865,
      "grad_norm": 0.6388524174690247,
      "learning_rate": 8.527460812571563e-05,
      "loss": 0.7712,
      "step": 7460
    },
    {
      "epoch": 0.2949421565917795,
      "grad_norm": 0.7312759757041931,
      "learning_rate": 8.5254866348166e-05,
      "loss": 0.8004,
      "step": 7470
    },
    {
      "epoch": 0.29533699214277254,
      "grad_norm": 0.5506532192230225,
      "learning_rate": 8.523512457061635e-05,
      "loss": 0.7923,
      "step": 7480
    },
    {
      "epoch": 0.29573182769376555,
      "grad_norm": 0.7323958277702332,
      "learning_rate": 8.521538279306668e-05,
      "loss": 0.7644,
      "step": 7490
    },
    {
      "epoch": 0.29612666324475856,
      "grad_norm": 0.9220669865608215,
      "learning_rate": 8.519564101551703e-05,
      "loss": 0.8314,
      "step": 7500
    },
    {
      "epoch": 0.29652149879575157,
      "grad_norm": 0.5804228186607361,
      "learning_rate": 8.51758992379674e-05,
      "loss": 0.7799,
      "step": 7510
    },
    {
      "epoch": 0.2969163343467446,
      "grad_norm": 0.5597864985466003,
      "learning_rate": 8.515615746041773e-05,
      "loss": 0.8121,
      "step": 7520
    },
    {
      "epoch": 0.2973111698977376,
      "grad_norm": 0.8432160019874573,
      "learning_rate": 8.513641568286808e-05,
      "loss": 0.8135,
      "step": 7530
    },
    {
      "epoch": 0.2977060054487306,
      "grad_norm": 0.6143337488174438,
      "learning_rate": 8.511667390531843e-05,
      "loss": 0.7811,
      "step": 7540
    },
    {
      "epoch": 0.2981008409997236,
      "grad_norm": 0.684735894203186,
      "learning_rate": 8.50969321277688e-05,
      "loss": 0.8162,
      "step": 7550
    },
    {
      "epoch": 0.2984956765507166,
      "grad_norm": 0.6549375057220459,
      "learning_rate": 8.507719035021913e-05,
      "loss": 0.7846,
      "step": 7560
    },
    {
      "epoch": 0.29889051210170964,
      "grad_norm": 0.6241315007209778,
      "learning_rate": 8.505744857266948e-05,
      "loss": 0.8156,
      "step": 7570
    },
    {
      "epoch": 0.29928534765270265,
      "grad_norm": 0.715438723564148,
      "learning_rate": 8.503770679511983e-05,
      "loss": 0.7936,
      "step": 7580
    },
    {
      "epoch": 0.29968018320369566,
      "grad_norm": 0.6526721715927124,
      "learning_rate": 8.501796501757018e-05,
      "loss": 0.7703,
      "step": 7590
    },
    {
      "epoch": 0.30007501875468867,
      "grad_norm": 0.7581892609596252,
      "learning_rate": 8.499822324002053e-05,
      "loss": 0.8101,
      "step": 7600
    },
    {
      "epoch": 0.3004698543056817,
      "grad_norm": 0.7295491099357605,
      "learning_rate": 8.497848146247088e-05,
      "loss": 0.7895,
      "step": 7610
    },
    {
      "epoch": 0.3008646898566747,
      "grad_norm": 0.48445937037467957,
      "learning_rate": 8.495873968492123e-05,
      "loss": 0.8297,
      "step": 7620
    },
    {
      "epoch": 0.3012595254076677,
      "grad_norm": 0.5622922778129578,
      "learning_rate": 8.493899790737158e-05,
      "loss": 0.8055,
      "step": 7630
    },
    {
      "epoch": 0.3016543609586607,
      "grad_norm": 0.6396353244781494,
      "learning_rate": 8.491925612982193e-05,
      "loss": 0.7859,
      "step": 7640
    },
    {
      "epoch": 0.30204919650965373,
      "grad_norm": 0.5385891795158386,
      "learning_rate": 8.489951435227228e-05,
      "loss": 0.7413,
      "step": 7650
    },
    {
      "epoch": 0.30244403206064674,
      "grad_norm": 0.688390851020813,
      "learning_rate": 8.487977257472263e-05,
      "loss": 0.8052,
      "step": 7660
    },
    {
      "epoch": 0.30283886761163975,
      "grad_norm": 0.6420870423316956,
      "learning_rate": 8.486003079717298e-05,
      "loss": 0.7993,
      "step": 7670
    },
    {
      "epoch": 0.30323370316263276,
      "grad_norm": 0.5265534520149231,
      "learning_rate": 8.484028901962333e-05,
      "loss": 0.7813,
      "step": 7680
    },
    {
      "epoch": 0.3036285387136258,
      "grad_norm": 0.6112266182899475,
      "learning_rate": 8.482054724207368e-05,
      "loss": 0.7725,
      "step": 7690
    },
    {
      "epoch": 0.3040233742646188,
      "grad_norm": 0.6183408498764038,
      "learning_rate": 8.480080546452402e-05,
      "loss": 0.7735,
      "step": 7700
    },
    {
      "epoch": 0.3044182098156118,
      "grad_norm": 0.6904022097587585,
      "learning_rate": 8.478106368697438e-05,
      "loss": 0.8287,
      "step": 7710
    },
    {
      "epoch": 0.3048130453666048,
      "grad_norm": 0.5536437034606934,
      "learning_rate": 8.476132190942473e-05,
      "loss": 0.8126,
      "step": 7720
    },
    {
      "epoch": 0.3052078809175978,
      "grad_norm": 0.46365195512771606,
      "learning_rate": 8.474158013187508e-05,
      "loss": 0.7714,
      "step": 7730
    },
    {
      "epoch": 0.30560271646859083,
      "grad_norm": 0.6214125156402588,
      "learning_rate": 8.472183835432542e-05,
      "loss": 0.7569,
      "step": 7740
    },
    {
      "epoch": 0.30599755201958384,
      "grad_norm": 0.4873901605606079,
      "learning_rate": 8.470209657677578e-05,
      "loss": 0.8065,
      "step": 7750
    },
    {
      "epoch": 0.30639238757057685,
      "grad_norm": 0.5584111213684082,
      "learning_rate": 8.468235479922613e-05,
      "loss": 0.839,
      "step": 7760
    },
    {
      "epoch": 0.30678722312156986,
      "grad_norm": 0.5786778330802917,
      "learning_rate": 8.466261302167647e-05,
      "loss": 0.7898,
      "step": 7770
    },
    {
      "epoch": 0.3071820586725629,
      "grad_norm": 0.6040831804275513,
      "learning_rate": 8.464287124412682e-05,
      "loss": 0.8046,
      "step": 7780
    },
    {
      "epoch": 0.3075768942235559,
      "grad_norm": 0.617611289024353,
      "learning_rate": 8.462312946657718e-05,
      "loss": 0.7853,
      "step": 7790
    },
    {
      "epoch": 0.3079717297745489,
      "grad_norm": 0.7661927938461304,
      "learning_rate": 8.460338768902753e-05,
      "loss": 0.8034,
      "step": 7800
    },
    {
      "epoch": 0.3083665653255419,
      "grad_norm": 0.6008449792861938,
      "learning_rate": 8.458364591147787e-05,
      "loss": 0.7907,
      "step": 7810
    },
    {
      "epoch": 0.3087614008765349,
      "grad_norm": 0.5351125597953796,
      "learning_rate": 8.456390413392822e-05,
      "loss": 0.7282,
      "step": 7820
    },
    {
      "epoch": 0.30915623642752793,
      "grad_norm": 0.5399134159088135,
      "learning_rate": 8.454416235637858e-05,
      "loss": 0.7953,
      "step": 7830
    },
    {
      "epoch": 0.30955107197852094,
      "grad_norm": 0.6129035353660583,
      "learning_rate": 8.452442057882892e-05,
      "loss": 0.7683,
      "step": 7840
    },
    {
      "epoch": 0.30994590752951395,
      "grad_norm": 0.556159257888794,
      "learning_rate": 8.450467880127927e-05,
      "loss": 0.7682,
      "step": 7850
    },
    {
      "epoch": 0.31034074308050696,
      "grad_norm": 0.5963506698608398,
      "learning_rate": 8.448493702372962e-05,
      "loss": 0.7375,
      "step": 7860
    },
    {
      "epoch": 0.3107355786315,
      "grad_norm": 0.538813591003418,
      "learning_rate": 8.446519524617997e-05,
      "loss": 0.7712,
      "step": 7870
    },
    {
      "epoch": 0.311130414182493,
      "grad_norm": 0.6443883180618286,
      "learning_rate": 8.444545346863032e-05,
      "loss": 0.8111,
      "step": 7880
    },
    {
      "epoch": 0.311525249733486,
      "grad_norm": 0.5069534778594971,
      "learning_rate": 8.442571169108067e-05,
      "loss": 0.7894,
      "step": 7890
    },
    {
      "epoch": 0.311920085284479,
      "grad_norm": 0.6687589883804321,
      "learning_rate": 8.440596991353102e-05,
      "loss": 0.7818,
      "step": 7900
    },
    {
      "epoch": 0.312314920835472,
      "grad_norm": 0.5389170050621033,
      "learning_rate": 8.438622813598137e-05,
      "loss": 0.8189,
      "step": 7910
    },
    {
      "epoch": 0.31270975638646503,
      "grad_norm": 0.9913470149040222,
      "learning_rate": 8.436648635843172e-05,
      "loss": 0.8207,
      "step": 7920
    },
    {
      "epoch": 0.31310459193745804,
      "grad_norm": 0.6352630257606506,
      "learning_rate": 8.434674458088207e-05,
      "loss": 0.8027,
      "step": 7930
    },
    {
      "epoch": 0.31349942748845105,
      "grad_norm": 0.6836123466491699,
      "learning_rate": 8.432700280333242e-05,
      "loss": 0.7571,
      "step": 7940
    },
    {
      "epoch": 0.31389426303944407,
      "grad_norm": 0.7812294960021973,
      "learning_rate": 8.430726102578277e-05,
      "loss": 0.7561,
      "step": 7950
    },
    {
      "epoch": 0.3142890985904371,
      "grad_norm": 0.635003387928009,
      "learning_rate": 8.428751924823312e-05,
      "loss": 0.7805,
      "step": 7960
    },
    {
      "epoch": 0.3146839341414301,
      "grad_norm": 0.5362065434455872,
      "learning_rate": 8.426777747068347e-05,
      "loss": 0.8156,
      "step": 7970
    },
    {
      "epoch": 0.3150787696924231,
      "grad_norm": 0.6207674741744995,
      "learning_rate": 8.42480356931338e-05,
      "loss": 0.8076,
      "step": 7980
    },
    {
      "epoch": 0.3154736052434161,
      "grad_norm": 0.5568488836288452,
      "learning_rate": 8.422829391558417e-05,
      "loss": 0.8091,
      "step": 7990
    },
    {
      "epoch": 0.3158684407944091,
      "grad_norm": 0.5376144051551819,
      "learning_rate": 8.420855213803452e-05,
      "loss": 0.7844,
      "step": 8000
    },
    {
      "epoch": 0.3158684407944091,
      "eval_loss": 0.8106549382209778,
      "eval_runtime": 1218.5005,
      "eval_samples_per_second": 9.238,
      "eval_steps_per_second": 9.238,
      "step": 8000
    },
    {
      "epoch": 0.31626327634540213,
      "grad_norm": 0.6271286010742188,
      "learning_rate": 8.418881036048487e-05,
      "loss": 0.7797,
      "step": 8010
    },
    {
      "epoch": 0.31665811189639514,
      "grad_norm": 0.6309226751327515,
      "learning_rate": 8.41690685829352e-05,
      "loss": 0.7595,
      "step": 8020
    },
    {
      "epoch": 0.31705294744738816,
      "grad_norm": 0.7518512606620789,
      "learning_rate": 8.414932680538557e-05,
      "loss": 0.8055,
      "step": 8030
    },
    {
      "epoch": 0.31744778299838117,
      "grad_norm": 0.6333184242248535,
      "learning_rate": 8.412958502783592e-05,
      "loss": 0.8166,
      "step": 8040
    },
    {
      "epoch": 0.3178426185493742,
      "grad_norm": 0.6699193120002747,
      "learning_rate": 8.410984325028625e-05,
      "loss": 0.8237,
      "step": 8050
    },
    {
      "epoch": 0.3182374541003672,
      "grad_norm": 0.7194118499755859,
      "learning_rate": 8.40901014727366e-05,
      "loss": 0.7822,
      "step": 8060
    },
    {
      "epoch": 0.3186322896513602,
      "grad_norm": 0.4471079111099243,
      "learning_rate": 8.407035969518697e-05,
      "loss": 0.7066,
      "step": 8070
    },
    {
      "epoch": 0.3190271252023532,
      "grad_norm": 0.5442764163017273,
      "learning_rate": 8.405061791763732e-05,
      "loss": 0.7966,
      "step": 8080
    },
    {
      "epoch": 0.3194219607533462,
      "grad_norm": 0.5182787775993347,
      "learning_rate": 8.403087614008765e-05,
      "loss": 0.7478,
      "step": 8090
    },
    {
      "epoch": 0.31981679630433923,
      "grad_norm": 0.5347098708152771,
      "learning_rate": 8.4011134362538e-05,
      "loss": 0.7672,
      "step": 8100
    },
    {
      "epoch": 0.32021163185533225,
      "grad_norm": 0.6945356130599976,
      "learning_rate": 8.399139258498837e-05,
      "loss": 0.7919,
      "step": 8110
    },
    {
      "epoch": 0.32060646740632526,
      "grad_norm": 0.5491412281990051,
      "learning_rate": 8.39716508074387e-05,
      "loss": 0.7921,
      "step": 8120
    },
    {
      "epoch": 0.32100130295731827,
      "grad_norm": 0.5187836289405823,
      "learning_rate": 8.395190902988905e-05,
      "loss": 0.7674,
      "step": 8130
    },
    {
      "epoch": 0.3213961385083113,
      "grad_norm": 0.7260753512382507,
      "learning_rate": 8.39321672523394e-05,
      "loss": 0.8243,
      "step": 8140
    },
    {
      "epoch": 0.3217909740593043,
      "grad_norm": 0.5792280435562134,
      "learning_rate": 8.391242547478975e-05,
      "loss": 0.769,
      "step": 8150
    },
    {
      "epoch": 0.3221858096102973,
      "grad_norm": 0.49720028042793274,
      "learning_rate": 8.38926836972401e-05,
      "loss": 0.7503,
      "step": 8160
    },
    {
      "epoch": 0.3225806451612903,
      "grad_norm": 0.7943176031112671,
      "learning_rate": 8.387294191969045e-05,
      "loss": 0.7778,
      "step": 8170
    },
    {
      "epoch": 0.3229754807122833,
      "grad_norm": 0.7713205218315125,
      "learning_rate": 8.38532001421408e-05,
      "loss": 0.8105,
      "step": 8180
    },
    {
      "epoch": 0.32337031626327634,
      "grad_norm": 0.5268628597259521,
      "learning_rate": 8.383345836459115e-05,
      "loss": 0.7887,
      "step": 8190
    },
    {
      "epoch": 0.32376515181426935,
      "grad_norm": 0.8050351738929749,
      "learning_rate": 8.38137165870415e-05,
      "loss": 0.8001,
      "step": 8200
    },
    {
      "epoch": 0.32415998736526236,
      "grad_norm": 0.6465829014778137,
      "learning_rate": 8.379397480949185e-05,
      "loss": 0.813,
      "step": 8210
    },
    {
      "epoch": 0.32455482291625537,
      "grad_norm": 0.5902618765830994,
      "learning_rate": 8.37742330319422e-05,
      "loss": 0.8123,
      "step": 8220
    },
    {
      "epoch": 0.3249496584672484,
      "grad_norm": 0.8481038808822632,
      "learning_rate": 8.375449125439255e-05,
      "loss": 0.7833,
      "step": 8230
    },
    {
      "epoch": 0.3253444940182414,
      "grad_norm": 0.6061544418334961,
      "learning_rate": 8.37347494768429e-05,
      "loss": 0.7542,
      "step": 8240
    },
    {
      "epoch": 0.3257393295692344,
      "grad_norm": 0.7112284302711487,
      "learning_rate": 8.371500769929325e-05,
      "loss": 0.811,
      "step": 8250
    },
    {
      "epoch": 0.3261341651202274,
      "grad_norm": 0.6053608655929565,
      "learning_rate": 8.369526592174359e-05,
      "loss": 0.7612,
      "step": 8260
    },
    {
      "epoch": 0.3265290006712204,
      "grad_norm": 0.5480394959449768,
      "learning_rate": 8.367552414419395e-05,
      "loss": 0.7948,
      "step": 8270
    },
    {
      "epoch": 0.32692383622221344,
      "grad_norm": 0.6595221757888794,
      "learning_rate": 8.36557823666443e-05,
      "loss": 0.7873,
      "step": 8280
    },
    {
      "epoch": 0.32731867177320645,
      "grad_norm": 0.5919685363769531,
      "learning_rate": 8.363604058909465e-05,
      "loss": 0.7719,
      "step": 8290
    },
    {
      "epoch": 0.32771350732419946,
      "grad_norm": 0.662302553653717,
      "learning_rate": 8.361629881154499e-05,
      "loss": 0.8118,
      "step": 8300
    },
    {
      "epoch": 0.32810834287519247,
      "grad_norm": 0.5576257109642029,
      "learning_rate": 8.359655703399535e-05,
      "loss": 0.8037,
      "step": 8310
    },
    {
      "epoch": 0.3285031784261855,
      "grad_norm": 0.672760009765625,
      "learning_rate": 8.35768152564457e-05,
      "loss": 0.7841,
      "step": 8320
    },
    {
      "epoch": 0.3288980139771785,
      "grad_norm": 0.530522882938385,
      "learning_rate": 8.355707347889604e-05,
      "loss": 0.7998,
      "step": 8330
    },
    {
      "epoch": 0.3292928495281715,
      "grad_norm": 0.6970371603965759,
      "learning_rate": 8.353733170134639e-05,
      "loss": 0.7917,
      "step": 8340
    },
    {
      "epoch": 0.3296876850791645,
      "grad_norm": 0.5592435598373413,
      "learning_rate": 8.351758992379675e-05,
      "loss": 0.7978,
      "step": 8350
    },
    {
      "epoch": 0.3300825206301575,
      "grad_norm": 0.847171425819397,
      "learning_rate": 8.34978481462471e-05,
      "loss": 0.8139,
      "step": 8360
    },
    {
      "epoch": 0.33047735618115054,
      "grad_norm": 0.6638447642326355,
      "learning_rate": 8.347810636869744e-05,
      "loss": 0.7862,
      "step": 8370
    },
    {
      "epoch": 0.33087219173214355,
      "grad_norm": 0.6974473595619202,
      "learning_rate": 8.345836459114779e-05,
      "loss": 0.7823,
      "step": 8380
    },
    {
      "epoch": 0.33126702728313656,
      "grad_norm": 0.7837671637535095,
      "learning_rate": 8.343862281359815e-05,
      "loss": 0.7941,
      "step": 8390
    },
    {
      "epoch": 0.3316618628341296,
      "grad_norm": 0.7209347486495972,
      "learning_rate": 8.341888103604849e-05,
      "loss": 0.8358,
      "step": 8400
    },
    {
      "epoch": 0.3320566983851226,
      "grad_norm": 0.6533358693122864,
      "learning_rate": 8.339913925849884e-05,
      "loss": 0.7996,
      "step": 8410
    },
    {
      "epoch": 0.3324515339361156,
      "grad_norm": 0.5278224945068359,
      "learning_rate": 8.337939748094919e-05,
      "loss": 0.7576,
      "step": 8420
    },
    {
      "epoch": 0.3328463694871086,
      "grad_norm": 0.738548219203949,
      "learning_rate": 8.335965570339954e-05,
      "loss": 0.8361,
      "step": 8430
    },
    {
      "epoch": 0.3332412050381016,
      "grad_norm": 0.5469256043434143,
      "learning_rate": 8.333991392584989e-05,
      "loss": 0.7639,
      "step": 8440
    },
    {
      "epoch": 0.33363604058909463,
      "grad_norm": 0.7799206972122192,
      "learning_rate": 8.332017214830024e-05,
      "loss": 0.8214,
      "step": 8450
    },
    {
      "epoch": 0.33403087614008764,
      "grad_norm": 0.6018218994140625,
      "learning_rate": 8.330043037075058e-05,
      "loss": 0.7685,
      "step": 8460
    },
    {
      "epoch": 0.33442571169108065,
      "grad_norm": 0.6989965438842773,
      "learning_rate": 8.328068859320093e-05,
      "loss": 0.7943,
      "step": 8470
    },
    {
      "epoch": 0.33482054724207366,
      "grad_norm": 0.6528940200805664,
      "learning_rate": 8.326094681565128e-05,
      "loss": 0.824,
      "step": 8480
    },
    {
      "epoch": 0.3352153827930667,
      "grad_norm": 0.6071321964263916,
      "learning_rate": 8.324120503810163e-05,
      "loss": 0.7777,
      "step": 8490
    },
    {
      "epoch": 0.3356102183440597,
      "grad_norm": 0.618806779384613,
      "learning_rate": 8.322146326055198e-05,
      "loss": 0.8033,
      "step": 8500
    },
    {
      "epoch": 0.3360050538950527,
      "grad_norm": 0.5719307661056519,
      "learning_rate": 8.320172148300233e-05,
      "loss": 0.8068,
      "step": 8510
    },
    {
      "epoch": 0.3363998894460457,
      "grad_norm": 0.7311314940452576,
      "learning_rate": 8.318197970545268e-05,
      "loss": 0.7566,
      "step": 8520
    },
    {
      "epoch": 0.3367947249970387,
      "grad_norm": 0.6756569147109985,
      "learning_rate": 8.316223792790303e-05,
      "loss": 0.8058,
      "step": 8530
    },
    {
      "epoch": 0.33718956054803173,
      "grad_norm": 0.6597543954849243,
      "learning_rate": 8.314249615035337e-05,
      "loss": 0.7577,
      "step": 8540
    },
    {
      "epoch": 0.33758439609902474,
      "grad_norm": 0.6201891303062439,
      "learning_rate": 8.312275437280373e-05,
      "loss": 0.7679,
      "step": 8550
    },
    {
      "epoch": 0.33797923165001775,
      "grad_norm": 0.5555054545402527,
      "learning_rate": 8.310301259525408e-05,
      "loss": 0.7642,
      "step": 8560
    },
    {
      "epoch": 0.33837406720101076,
      "grad_norm": 0.7592301368713379,
      "learning_rate": 8.308327081770443e-05,
      "loss": 0.775,
      "step": 8570
    },
    {
      "epoch": 0.3387689027520038,
      "grad_norm": 0.575192928314209,
      "learning_rate": 8.306352904015477e-05,
      "loss": 0.7965,
      "step": 8580
    },
    {
      "epoch": 0.3391637383029968,
      "grad_norm": 0.7287957668304443,
      "learning_rate": 8.304378726260513e-05,
      "loss": 0.7623,
      "step": 8590
    },
    {
      "epoch": 0.3395585738539898,
      "grad_norm": 0.6211390495300293,
      "learning_rate": 8.302404548505548e-05,
      "loss": 0.8367,
      "step": 8600
    },
    {
      "epoch": 0.3399534094049828,
      "grad_norm": 0.6381972432136536,
      "learning_rate": 8.300430370750582e-05,
      "loss": 0.7982,
      "step": 8610
    },
    {
      "epoch": 0.3403482449559758,
      "grad_norm": 0.7242133021354675,
      "learning_rate": 8.298456192995617e-05,
      "loss": 0.7729,
      "step": 8620
    },
    {
      "epoch": 0.34074308050696883,
      "grad_norm": 0.5700960159301758,
      "learning_rate": 8.296482015240653e-05,
      "loss": 0.7727,
      "step": 8630
    },
    {
      "epoch": 0.34113791605796184,
      "grad_norm": 0.7566488981246948,
      "learning_rate": 8.294507837485688e-05,
      "loss": 0.8001,
      "step": 8640
    },
    {
      "epoch": 0.34153275160895485,
      "grad_norm": 0.5401756167411804,
      "learning_rate": 8.292533659730722e-05,
      "loss": 0.8177,
      "step": 8650
    },
    {
      "epoch": 0.34192758715994787,
      "grad_norm": 0.737187385559082,
      "learning_rate": 8.290559481975757e-05,
      "loss": 0.8101,
      "step": 8660
    },
    {
      "epoch": 0.3423224227109409,
      "grad_norm": 0.6460576057434082,
      "learning_rate": 8.288585304220793e-05,
      "loss": 0.7704,
      "step": 8670
    },
    {
      "epoch": 0.3427172582619339,
      "grad_norm": 0.6873354911804199,
      "learning_rate": 8.286611126465827e-05,
      "loss": 0.815,
      "step": 8680
    },
    {
      "epoch": 0.3431120938129269,
      "grad_norm": 0.893035888671875,
      "learning_rate": 8.284636948710862e-05,
      "loss": 0.8363,
      "step": 8690
    },
    {
      "epoch": 0.3435069293639199,
      "grad_norm": 0.48306891322135925,
      "learning_rate": 8.282662770955897e-05,
      "loss": 0.7781,
      "step": 8700
    },
    {
      "epoch": 0.3439017649149129,
      "grad_norm": 0.6224010586738586,
      "learning_rate": 8.280688593200932e-05,
      "loss": 0.7877,
      "step": 8710
    },
    {
      "epoch": 0.34429660046590593,
      "grad_norm": 0.6264910697937012,
      "learning_rate": 8.278714415445967e-05,
      "loss": 0.8296,
      "step": 8720
    },
    {
      "epoch": 0.34469143601689894,
      "grad_norm": 0.9055359959602356,
      "learning_rate": 8.276740237691002e-05,
      "loss": 0.8038,
      "step": 8730
    },
    {
      "epoch": 0.34508627156789196,
      "grad_norm": 0.6738746762275696,
      "learning_rate": 8.274766059936037e-05,
      "loss": 0.8007,
      "step": 8740
    },
    {
      "epoch": 0.34548110711888497,
      "grad_norm": 0.6146194338798523,
      "learning_rate": 8.272791882181072e-05,
      "loss": 0.7435,
      "step": 8750
    },
    {
      "epoch": 0.345875942669878,
      "grad_norm": 0.5996308326721191,
      "learning_rate": 8.270817704426107e-05,
      "loss": 0.8144,
      "step": 8760
    },
    {
      "epoch": 0.346270778220871,
      "grad_norm": 0.6924228668212891,
      "learning_rate": 8.268843526671142e-05,
      "loss": 0.777,
      "step": 8770
    },
    {
      "epoch": 0.346665613771864,
      "grad_norm": 0.7790807485580444,
      "learning_rate": 8.266869348916177e-05,
      "loss": 0.7514,
      "step": 8780
    },
    {
      "epoch": 0.347060449322857,
      "grad_norm": 0.6185601353645325,
      "learning_rate": 8.264895171161212e-05,
      "loss": 0.8187,
      "step": 8790
    },
    {
      "epoch": 0.34745528487385,
      "grad_norm": 0.5502512454986572,
      "learning_rate": 8.262920993406247e-05,
      "loss": 0.7509,
      "step": 8800
    },
    {
      "epoch": 0.34785012042484303,
      "grad_norm": 0.638987123966217,
      "learning_rate": 8.260946815651282e-05,
      "loss": 0.8045,
      "step": 8810
    },
    {
      "epoch": 0.34824495597583605,
      "grad_norm": 0.7438062429428101,
      "learning_rate": 8.258972637896315e-05,
      "loss": 0.8014,
      "step": 8820
    },
    {
      "epoch": 0.34863979152682906,
      "grad_norm": 0.6299594640731812,
      "learning_rate": 8.256998460141352e-05,
      "loss": 0.77,
      "step": 8830
    },
    {
      "epoch": 0.34903462707782207,
      "grad_norm": 0.7032952308654785,
      "learning_rate": 8.255024282386387e-05,
      "loss": 0.7632,
      "step": 8840
    },
    {
      "epoch": 0.3494294626288151,
      "grad_norm": 0.7169225215911865,
      "learning_rate": 8.253050104631422e-05,
      "loss": 0.8063,
      "step": 8850
    },
    {
      "epoch": 0.3498242981798081,
      "grad_norm": 0.5985692143440247,
      "learning_rate": 8.251075926876455e-05,
      "loss": 0.7657,
      "step": 8860
    },
    {
      "epoch": 0.3502191337308011,
      "grad_norm": 0.8209171891212463,
      "learning_rate": 8.249101749121492e-05,
      "loss": 0.7634,
      "step": 8870
    },
    {
      "epoch": 0.3506139692817941,
      "grad_norm": 0.5714648365974426,
      "learning_rate": 8.247127571366527e-05,
      "loss": 0.7906,
      "step": 8880
    },
    {
      "epoch": 0.3510088048327871,
      "grad_norm": 0.5892763137817383,
      "learning_rate": 8.24515339361156e-05,
      "loss": 0.7881,
      "step": 8890
    },
    {
      "epoch": 0.35140364038378014,
      "grad_norm": 0.6185580492019653,
      "learning_rate": 8.243179215856595e-05,
      "loss": 0.7827,
      "step": 8900
    },
    {
      "epoch": 0.35179847593477315,
      "grad_norm": 0.7636632323265076,
      "learning_rate": 8.241205038101632e-05,
      "loss": 0.7611,
      "step": 8910
    },
    {
      "epoch": 0.35219331148576616,
      "grad_norm": 0.5706204771995544,
      "learning_rate": 8.239230860346667e-05,
      "loss": 0.7972,
      "step": 8920
    },
    {
      "epoch": 0.35258814703675917,
      "grad_norm": 0.9768518805503845,
      "learning_rate": 8.2372566825917e-05,
      "loss": 0.7588,
      "step": 8930
    },
    {
      "epoch": 0.3529829825877522,
      "grad_norm": 0.6401363611221313,
      "learning_rate": 8.235282504836735e-05,
      "loss": 0.7907,
      "step": 8940
    },
    {
      "epoch": 0.3533778181387452,
      "grad_norm": 0.551588237285614,
      "learning_rate": 8.233308327081772e-05,
      "loss": 0.7454,
      "step": 8950
    },
    {
      "epoch": 0.3537726536897382,
      "grad_norm": 0.6727696657180786,
      "learning_rate": 8.231334149326805e-05,
      "loss": 0.7671,
      "step": 8960
    },
    {
      "epoch": 0.3541674892407312,
      "grad_norm": 0.5713658332824707,
      "learning_rate": 8.22935997157184e-05,
      "loss": 0.7685,
      "step": 8970
    },
    {
      "epoch": 0.3545623247917242,
      "grad_norm": 0.6513729691505432,
      "learning_rate": 8.227385793816875e-05,
      "loss": 0.7818,
      "step": 8980
    },
    {
      "epoch": 0.35495716034271724,
      "grad_norm": 0.611497163772583,
      "learning_rate": 8.225411616061912e-05,
      "loss": 0.7722,
      "step": 8990
    },
    {
      "epoch": 0.35535199589371025,
      "grad_norm": 0.7725832462310791,
      "learning_rate": 8.223437438306945e-05,
      "loss": 0.7612,
      "step": 9000
    },
    {
      "epoch": 0.35535199589371025,
      "eval_loss": 0.8072811961174011,
      "eval_runtime": 1220.018,
      "eval_samples_per_second": 9.227,
      "eval_steps_per_second": 9.227,
      "step": 9000
    },
    {
      "epoch": 0.35574683144470326,
      "grad_norm": 0.5566190481185913,
      "learning_rate": 8.22146326055198e-05,
      "loss": 0.8083,
      "step": 9010
    },
    {
      "epoch": 0.35614166699569627,
      "grad_norm": 0.654630720615387,
      "learning_rate": 8.219489082797015e-05,
      "loss": 0.7593,
      "step": 9020
    },
    {
      "epoch": 0.3565365025466893,
      "grad_norm": 0.655211329460144,
      "learning_rate": 8.21751490504205e-05,
      "loss": 0.7741,
      "step": 9030
    },
    {
      "epoch": 0.3569313380976823,
      "grad_norm": 0.7740876078605652,
      "learning_rate": 8.215540727287085e-05,
      "loss": 0.8217,
      "step": 9040
    },
    {
      "epoch": 0.3573261736486753,
      "grad_norm": 0.7061643600463867,
      "learning_rate": 8.21356654953212e-05,
      "loss": 0.778,
      "step": 9050
    },
    {
      "epoch": 0.3577210091996683,
      "grad_norm": 0.6973987817764282,
      "learning_rate": 8.211592371777155e-05,
      "loss": 0.7839,
      "step": 9060
    },
    {
      "epoch": 0.3581158447506613,
      "grad_norm": 0.585822343826294,
      "learning_rate": 8.20961819402219e-05,
      "loss": 0.7493,
      "step": 9070
    },
    {
      "epoch": 0.35851068030165434,
      "grad_norm": 0.696778416633606,
      "learning_rate": 8.207644016267225e-05,
      "loss": 0.7756,
      "step": 9080
    },
    {
      "epoch": 0.35890551585264735,
      "grad_norm": 0.5599328875541687,
      "learning_rate": 8.20566983851226e-05,
      "loss": 0.803,
      "step": 9090
    },
    {
      "epoch": 0.35930035140364036,
      "grad_norm": 0.7398673295974731,
      "learning_rate": 8.203695660757295e-05,
      "loss": 0.8286,
      "step": 9100
    },
    {
      "epoch": 0.3596951869546334,
      "grad_norm": 0.5547484755516052,
      "learning_rate": 8.20172148300233e-05,
      "loss": 0.7665,
      "step": 9110
    },
    {
      "epoch": 0.3600900225056264,
      "grad_norm": 0.7615499496459961,
      "learning_rate": 8.199747305247365e-05,
      "loss": 0.8014,
      "step": 9120
    },
    {
      "epoch": 0.3604848580566194,
      "grad_norm": 0.6679142713546753,
      "learning_rate": 8.1977731274924e-05,
      "loss": 0.839,
      "step": 9130
    },
    {
      "epoch": 0.3608796936076124,
      "grad_norm": 0.6471472382545471,
      "learning_rate": 8.195798949737434e-05,
      "loss": 0.7856,
      "step": 9140
    },
    {
      "epoch": 0.3612745291586054,
      "grad_norm": 0.5072997808456421,
      "learning_rate": 8.19382477198247e-05,
      "loss": 0.7353,
      "step": 9150
    },
    {
      "epoch": 0.36166936470959843,
      "grad_norm": 0.724445104598999,
      "learning_rate": 8.191850594227505e-05,
      "loss": 0.7842,
      "step": 9160
    },
    {
      "epoch": 0.36206420026059144,
      "grad_norm": 0.6473650932312012,
      "learning_rate": 8.189876416472539e-05,
      "loss": 0.7451,
      "step": 9170
    },
    {
      "epoch": 0.36245903581158445,
      "grad_norm": 0.6420485377311707,
      "learning_rate": 8.187902238717574e-05,
      "loss": 0.8099,
      "step": 9180
    },
    {
      "epoch": 0.36285387136257746,
      "grad_norm": 0.5761402249336243,
      "learning_rate": 8.18592806096261e-05,
      "loss": 0.7989,
      "step": 9190
    },
    {
      "epoch": 0.3632487069135705,
      "grad_norm": 0.5775800347328186,
      "learning_rate": 8.183953883207645e-05,
      "loss": 0.7796,
      "step": 9200
    },
    {
      "epoch": 0.3636435424645635,
      "grad_norm": 0.7572172284126282,
      "learning_rate": 8.181979705452679e-05,
      "loss": 0.7625,
      "step": 9210
    },
    {
      "epoch": 0.3640383780155565,
      "grad_norm": 0.9358327984809875,
      "learning_rate": 8.180005527697714e-05,
      "loss": 0.7842,
      "step": 9220
    },
    {
      "epoch": 0.3644332135665495,
      "grad_norm": 0.6506746411323547,
      "learning_rate": 8.17803134994275e-05,
      "loss": 0.8221,
      "step": 9230
    },
    {
      "epoch": 0.3648280491175425,
      "grad_norm": 0.5918694734573364,
      "learning_rate": 8.176057172187784e-05,
      "loss": 0.7621,
      "step": 9240
    },
    {
      "epoch": 0.36522288466853553,
      "grad_norm": 0.6721385717391968,
      "learning_rate": 8.174082994432819e-05,
      "loss": 0.8058,
      "step": 9250
    },
    {
      "epoch": 0.36561772021952854,
      "grad_norm": 0.5952019095420837,
      "learning_rate": 8.172108816677854e-05,
      "loss": 0.7856,
      "step": 9260
    },
    {
      "epoch": 0.36601255577052155,
      "grad_norm": 0.5074834227561951,
      "learning_rate": 8.17013463892289e-05,
      "loss": 0.7188,
      "step": 9270
    },
    {
      "epoch": 0.36640739132151456,
      "grad_norm": 0.6164242029190063,
      "learning_rate": 8.168160461167924e-05,
      "loss": 0.7878,
      "step": 9280
    },
    {
      "epoch": 0.3668022268725076,
      "grad_norm": 0.5873421430587769,
      "learning_rate": 8.166186283412959e-05,
      "loss": 0.8059,
      "step": 9290
    },
    {
      "epoch": 0.3671970624235006,
      "grad_norm": 0.6830157041549683,
      "learning_rate": 8.164212105657994e-05,
      "loss": 0.8019,
      "step": 9300
    },
    {
      "epoch": 0.3675918979744936,
      "grad_norm": 0.7517912983894348,
      "learning_rate": 8.162237927903029e-05,
      "loss": 0.7872,
      "step": 9310
    },
    {
      "epoch": 0.3679867335254866,
      "grad_norm": 0.5720371007919312,
      "learning_rate": 8.160263750148064e-05,
      "loss": 0.7851,
      "step": 9320
    },
    {
      "epoch": 0.3683815690764796,
      "grad_norm": 0.65264892578125,
      "learning_rate": 8.158289572393099e-05,
      "loss": 0.7903,
      "step": 9330
    },
    {
      "epoch": 0.36877640462747263,
      "grad_norm": 0.6675606369972229,
      "learning_rate": 8.156315394638134e-05,
      "loss": 0.7644,
      "step": 9340
    },
    {
      "epoch": 0.36917124017846564,
      "grad_norm": 0.6529134511947632,
      "learning_rate": 8.154341216883169e-05,
      "loss": 0.7934,
      "step": 9350
    },
    {
      "epoch": 0.36956607572945865,
      "grad_norm": 0.6990614533424377,
      "learning_rate": 8.152367039128204e-05,
      "loss": 0.823,
      "step": 9360
    },
    {
      "epoch": 0.36996091128045167,
      "grad_norm": 0.596495509147644,
      "learning_rate": 8.150392861373239e-05,
      "loss": 0.7637,
      "step": 9370
    },
    {
      "epoch": 0.3703557468314447,
      "grad_norm": 0.6507386565208435,
      "learning_rate": 8.148418683618274e-05,
      "loss": 0.7812,
      "step": 9380
    },
    {
      "epoch": 0.3707505823824377,
      "grad_norm": 0.5504390001296997,
      "learning_rate": 8.146444505863309e-05,
      "loss": 0.7631,
      "step": 9390
    },
    {
      "epoch": 0.3711454179334307,
      "grad_norm": 0.7034315466880798,
      "learning_rate": 8.144470328108344e-05,
      "loss": 0.7985,
      "step": 9400
    },
    {
      "epoch": 0.3715402534844237,
      "grad_norm": 0.7095738053321838,
      "learning_rate": 8.142496150353379e-05,
      "loss": 0.7874,
      "step": 9410
    },
    {
      "epoch": 0.3719350890354167,
      "grad_norm": 0.5686175227165222,
      "learning_rate": 8.140521972598412e-05,
      "loss": 0.7928,
      "step": 9420
    },
    {
      "epoch": 0.37232992458640973,
      "grad_norm": 0.5819500684738159,
      "learning_rate": 8.138547794843449e-05,
      "loss": 0.7368,
      "step": 9430
    },
    {
      "epoch": 0.37272476013740274,
      "grad_norm": 0.604844868183136,
      "learning_rate": 8.136573617088484e-05,
      "loss": 0.7684,
      "step": 9440
    },
    {
      "epoch": 0.37311959568839576,
      "grad_norm": 0.8150049448013306,
      "learning_rate": 8.134599439333517e-05,
      "loss": 0.793,
      "step": 9450
    },
    {
      "epoch": 0.3735144312393888,
      "grad_norm": 0.7549526691436768,
      "learning_rate": 8.132625261578552e-05,
      "loss": 0.7994,
      "step": 9460
    },
    {
      "epoch": 0.37390926679038183,
      "grad_norm": 0.7225307822227478,
      "learning_rate": 8.130651083823589e-05,
      "loss": 0.7854,
      "step": 9470
    },
    {
      "epoch": 0.37430410234137484,
      "grad_norm": 0.578192949295044,
      "learning_rate": 8.128676906068624e-05,
      "loss": 0.8255,
      "step": 9480
    },
    {
      "epoch": 0.37469893789236786,
      "grad_norm": 0.7108449339866638,
      "learning_rate": 8.126702728313657e-05,
      "loss": 0.7606,
      "step": 9490
    },
    {
      "epoch": 0.37509377344336087,
      "grad_norm": 0.7238970994949341,
      "learning_rate": 8.124728550558692e-05,
      "loss": 0.7878,
      "step": 9500
    },
    {
      "epoch": 0.3754886089943539,
      "grad_norm": 0.6484872698783875,
      "learning_rate": 8.122754372803729e-05,
      "loss": 0.7639,
      "step": 9510
    },
    {
      "epoch": 0.3758834445453469,
      "grad_norm": 0.6083911657333374,
      "learning_rate": 8.120780195048762e-05,
      "loss": 0.7989,
      "step": 9520
    },
    {
      "epoch": 0.3762782800963399,
      "grad_norm": 0.6123417615890503,
      "learning_rate": 8.118806017293797e-05,
      "loss": 0.7432,
      "step": 9530
    },
    {
      "epoch": 0.3766731156473329,
      "grad_norm": 0.5927038192749023,
      "learning_rate": 8.116831839538832e-05,
      "loss": 0.7783,
      "step": 9540
    },
    {
      "epoch": 0.3770679511983259,
      "grad_norm": 0.5649508833885193,
      "learning_rate": 8.114857661783868e-05,
      "loss": 0.8016,
      "step": 9550
    },
    {
      "epoch": 0.37746278674931893,
      "grad_norm": 0.7119706869125366,
      "learning_rate": 8.112883484028902e-05,
      "loss": 0.7505,
      "step": 9560
    },
    {
      "epoch": 0.37785762230031195,
      "grad_norm": 0.7564078569412231,
      "learning_rate": 8.110909306273937e-05,
      "loss": 0.7446,
      "step": 9570
    },
    {
      "epoch": 0.37825245785130496,
      "grad_norm": 0.859160840511322,
      "learning_rate": 8.108935128518972e-05,
      "loss": 0.8172,
      "step": 9580
    },
    {
      "epoch": 0.37864729340229797,
      "grad_norm": 0.606708288192749,
      "learning_rate": 8.106960950764007e-05,
      "loss": 0.811,
      "step": 9590
    },
    {
      "epoch": 0.379042128953291,
      "grad_norm": 0.5939193964004517,
      "learning_rate": 8.104986773009042e-05,
      "loss": 0.779,
      "step": 9600
    },
    {
      "epoch": 0.379436964504284,
      "grad_norm": 0.6208918690681458,
      "learning_rate": 8.103012595254077e-05,
      "loss": 0.8013,
      "step": 9610
    },
    {
      "epoch": 0.379831800055277,
      "grad_norm": 0.6205408573150635,
      "learning_rate": 8.101038417499112e-05,
      "loss": 0.7923,
      "step": 9620
    },
    {
      "epoch": 0.38022663560627,
      "grad_norm": 0.6402466893196106,
      "learning_rate": 8.099064239744147e-05,
      "loss": 0.8848,
      "step": 9630
    },
    {
      "epoch": 0.380621471157263,
      "grad_norm": 0.6976323127746582,
      "learning_rate": 8.097090061989182e-05,
      "loss": 0.7895,
      "step": 9640
    },
    {
      "epoch": 0.38101630670825604,
      "grad_norm": 0.6210833787918091,
      "learning_rate": 8.095115884234217e-05,
      "loss": 0.7472,
      "step": 9650
    },
    {
      "epoch": 0.38141114225924905,
      "grad_norm": 0.6442210674285889,
      "learning_rate": 8.093141706479252e-05,
      "loss": 0.8405,
      "step": 9660
    },
    {
      "epoch": 0.38180597781024206,
      "grad_norm": 0.6391270756721497,
      "learning_rate": 8.091167528724287e-05,
      "loss": 0.7801,
      "step": 9670
    },
    {
      "epoch": 0.38220081336123507,
      "grad_norm": 0.6672566533088684,
      "learning_rate": 8.089193350969322e-05,
      "loss": 0.8018,
      "step": 9680
    },
    {
      "epoch": 0.3825956489122281,
      "grad_norm": 0.7266657948493958,
      "learning_rate": 8.087219173214357e-05,
      "loss": 0.7943,
      "step": 9690
    },
    {
      "epoch": 0.3829904844632211,
      "grad_norm": 0.6538970470428467,
      "learning_rate": 8.08524499545939e-05,
      "loss": 0.7925,
      "step": 9700
    },
    {
      "epoch": 0.3833853200142141,
      "grad_norm": 0.6305189728736877,
      "learning_rate": 8.083270817704427e-05,
      "loss": 0.7641,
      "step": 9710
    },
    {
      "epoch": 0.3837801555652071,
      "grad_norm": 0.5722177028656006,
      "learning_rate": 8.081296639949462e-05,
      "loss": 0.7846,
      "step": 9720
    },
    {
      "epoch": 0.3841749911162001,
      "grad_norm": 0.6634821891784668,
      "learning_rate": 8.079322462194496e-05,
      "loss": 0.7859,
      "step": 9730
    },
    {
      "epoch": 0.38456982666719314,
      "grad_norm": 0.6417931318283081,
      "learning_rate": 8.07734828443953e-05,
      "loss": 0.7454,
      "step": 9740
    },
    {
      "epoch": 0.38496466221818615,
      "grad_norm": 0.6106062531471252,
      "learning_rate": 8.075374106684567e-05,
      "loss": 0.7738,
      "step": 9750
    },
    {
      "epoch": 0.38535949776917916,
      "grad_norm": 0.6527099013328552,
      "learning_rate": 8.073399928929602e-05,
      "loss": 0.8039,
      "step": 9760
    },
    {
      "epoch": 0.38575433332017217,
      "grad_norm": 0.63971346616745,
      "learning_rate": 8.071425751174636e-05,
      "loss": 0.7716,
      "step": 9770
    },
    {
      "epoch": 0.3861491688711652,
      "grad_norm": 0.7316755652427673,
      "learning_rate": 8.06945157341967e-05,
      "loss": 0.8074,
      "step": 9780
    },
    {
      "epoch": 0.3865440044221582,
      "grad_norm": 0.5479134917259216,
      "learning_rate": 8.067477395664707e-05,
      "loss": 0.7941,
      "step": 9790
    },
    {
      "epoch": 0.3869388399731512,
      "grad_norm": 0.7438876032829285,
      "learning_rate": 8.06550321790974e-05,
      "loss": 0.8405,
      "step": 9800
    },
    {
      "epoch": 0.3873336755241442,
      "grad_norm": 0.5942518711090088,
      "learning_rate": 8.063529040154776e-05,
      "loss": 0.7253,
      "step": 9810
    },
    {
      "epoch": 0.3877285110751372,
      "grad_norm": 0.6120887398719788,
      "learning_rate": 8.06155486239981e-05,
      "loss": 0.7455,
      "step": 9820
    },
    {
      "epoch": 0.38812334662613024,
      "grad_norm": 0.9588524103164673,
      "learning_rate": 8.059580684644847e-05,
      "loss": 0.7462,
      "step": 9830
    },
    {
      "epoch": 0.38851818217712325,
      "grad_norm": 0.5721936225891113,
      "learning_rate": 8.05760650688988e-05,
      "loss": 0.7709,
      "step": 9840
    },
    {
      "epoch": 0.38891301772811626,
      "grad_norm": 0.6036329865455627,
      "learning_rate": 8.055632329134916e-05,
      "loss": 0.7688,
      "step": 9850
    },
    {
      "epoch": 0.3893078532791093,
      "grad_norm": 0.6220201253890991,
      "learning_rate": 8.05365815137995e-05,
      "loss": 0.7851,
      "step": 9860
    },
    {
      "epoch": 0.3897026888301023,
      "grad_norm": 0.6549647450447083,
      "learning_rate": 8.051683973624986e-05,
      "loss": 0.8157,
      "step": 9870
    },
    {
      "epoch": 0.3900975243810953,
      "grad_norm": 0.6967124938964844,
      "learning_rate": 8.04970979587002e-05,
      "loss": 0.7461,
      "step": 9880
    },
    {
      "epoch": 0.3904923599320883,
      "grad_norm": 0.6073374152183533,
      "learning_rate": 8.047735618115055e-05,
      "loss": 0.7743,
      "step": 9890
    },
    {
      "epoch": 0.3908871954830813,
      "grad_norm": 0.8182199001312256,
      "learning_rate": 8.04576144036009e-05,
      "loss": 0.7335,
      "step": 9900
    },
    {
      "epoch": 0.39128203103407433,
      "grad_norm": 0.8695344924926758,
      "learning_rate": 8.043787262605125e-05,
      "loss": 0.7792,
      "step": 9910
    },
    {
      "epoch": 0.39167686658506734,
      "grad_norm": 0.7869327068328857,
      "learning_rate": 8.04181308485016e-05,
      "loss": 0.7768,
      "step": 9920
    },
    {
      "epoch": 0.39207170213606035,
      "grad_norm": 0.708355724811554,
      "learning_rate": 8.039838907095195e-05,
      "loss": 0.7682,
      "step": 9930
    },
    {
      "epoch": 0.39246653768705336,
      "grad_norm": 0.6383198499679565,
      "learning_rate": 8.03786472934023e-05,
      "loss": 0.7918,
      "step": 9940
    },
    {
      "epoch": 0.3928613732380464,
      "grad_norm": 0.5621379017829895,
      "learning_rate": 8.035890551585265e-05,
      "loss": 0.7271,
      "step": 9950
    },
    {
      "epoch": 0.3932562087890394,
      "grad_norm": 0.6480624675750732,
      "learning_rate": 8.0339163738303e-05,
      "loss": 0.7671,
      "step": 9960
    },
    {
      "epoch": 0.3936510443400324,
      "grad_norm": 0.6408190131187439,
      "learning_rate": 8.031942196075335e-05,
      "loss": 0.8505,
      "step": 9970
    },
    {
      "epoch": 0.3940458798910254,
      "grad_norm": 0.719403862953186,
      "learning_rate": 8.029968018320369e-05,
      "loss": 0.8056,
      "step": 9980
    },
    {
      "epoch": 0.3944407154420184,
      "grad_norm": 0.6156302690505981,
      "learning_rate": 8.027993840565405e-05,
      "loss": 0.752,
      "step": 9990
    },
    {
      "epoch": 0.39483555099301143,
      "grad_norm": 0.5616315007209778,
      "learning_rate": 8.02601966281044e-05,
      "loss": 0.7945,
      "step": 10000
    },
    {
      "epoch": 0.39483555099301143,
      "eval_loss": 0.8069303631782532,
      "eval_runtime": 1219.2253,
      "eval_samples_per_second": 9.233,
      "eval_steps_per_second": 9.233,
      "step": 10000
    },
    {
      "epoch": 0.39523038654400444,
      "grad_norm": 0.6248340010643005,
      "learning_rate": 8.024045485055475e-05,
      "loss": 0.8071,
      "step": 10010
    },
    {
      "epoch": 0.39562522209499745,
      "grad_norm": 0.5832061171531677,
      "learning_rate": 8.022071307300509e-05,
      "loss": 0.7644,
      "step": 10020
    },
    {
      "epoch": 0.39602005764599046,
      "grad_norm": 0.7669797539710999,
      "learning_rate": 8.020097129545545e-05,
      "loss": 0.8289,
      "step": 10030
    },
    {
      "epoch": 0.3964148931969835,
      "grad_norm": 0.6711285710334778,
      "learning_rate": 8.01812295179058e-05,
      "loss": 0.7854,
      "step": 10040
    },
    {
      "epoch": 0.3968097287479765,
      "grad_norm": 0.6303422451019287,
      "learning_rate": 8.016148774035614e-05,
      "loss": 0.746,
      "step": 10050
    },
    {
      "epoch": 0.3972045642989695,
      "grad_norm": 0.6733649373054504,
      "learning_rate": 8.014174596280649e-05,
      "loss": 0.7853,
      "step": 10060
    },
    {
      "epoch": 0.3975993998499625,
      "grad_norm": 0.5917131900787354,
      "learning_rate": 8.012200418525685e-05,
      "loss": 0.7793,
      "step": 10070
    },
    {
      "epoch": 0.3979942354009555,
      "grad_norm": 0.585186779499054,
      "learning_rate": 8.010226240770719e-05,
      "loss": 0.7755,
      "step": 10080
    },
    {
      "epoch": 0.39838907095194853,
      "grad_norm": 0.6194852590560913,
      "learning_rate": 8.008252063015754e-05,
      "loss": 0.7624,
      "step": 10090
    },
    {
      "epoch": 0.39878390650294154,
      "grad_norm": 0.6112386584281921,
      "learning_rate": 8.006277885260789e-05,
      "loss": 0.7636,
      "step": 10100
    },
    {
      "epoch": 0.39917874205393455,
      "grad_norm": 0.5392723679542542,
      "learning_rate": 8.004303707505825e-05,
      "loss": 0.7714,
      "step": 10110
    },
    {
      "epoch": 0.39957357760492757,
      "grad_norm": 0.6972442865371704,
      "learning_rate": 8.002329529750859e-05,
      "loss": 0.7801,
      "step": 10120
    },
    {
      "epoch": 0.3999684131559206,
      "grad_norm": 0.5949366688728333,
      "learning_rate": 8.000355351995894e-05,
      "loss": 0.7358,
      "step": 10130
    },
    {
      "epoch": 0.4003632487069136,
      "grad_norm": 0.6046795845031738,
      "learning_rate": 7.998381174240929e-05,
      "loss": 0.7935,
      "step": 10140
    },
    {
      "epoch": 0.4007580842579066,
      "grad_norm": 0.6068110466003418,
      "learning_rate": 7.996406996485964e-05,
      "loss": 0.7691,
      "step": 10150
    },
    {
      "epoch": 0.4011529198088996,
      "grad_norm": 0.5546009540557861,
      "learning_rate": 7.994432818730999e-05,
      "loss": 0.7595,
      "step": 10160
    },
    {
      "epoch": 0.4015477553598926,
      "grad_norm": 0.5486030578613281,
      "learning_rate": 7.992458640976034e-05,
      "loss": 0.7326,
      "step": 10170
    },
    {
      "epoch": 0.40194259091088563,
      "grad_norm": 0.6093944907188416,
      "learning_rate": 7.990484463221069e-05,
      "loss": 0.7788,
      "step": 10180
    },
    {
      "epoch": 0.40233742646187864,
      "grad_norm": 0.6824545860290527,
      "learning_rate": 7.988510285466104e-05,
      "loss": 0.7931,
      "step": 10190
    },
    {
      "epoch": 0.40273226201287166,
      "grad_norm": 0.6430226564407349,
      "learning_rate": 7.986536107711139e-05,
      "loss": 0.7948,
      "step": 10200
    },
    {
      "epoch": 0.40312709756386467,
      "grad_norm": 0.6296742558479309,
      "learning_rate": 7.984561929956174e-05,
      "loss": 0.7693,
      "step": 10210
    },
    {
      "epoch": 0.4035219331148577,
      "grad_norm": 0.6332743167877197,
      "learning_rate": 7.982587752201209e-05,
      "loss": 0.7803,
      "step": 10220
    },
    {
      "epoch": 0.4039167686658507,
      "grad_norm": 0.6263532042503357,
      "learning_rate": 7.980613574446244e-05,
      "loss": 0.7916,
      "step": 10230
    },
    {
      "epoch": 0.4043116042168437,
      "grad_norm": 0.661749541759491,
      "learning_rate": 7.978639396691279e-05,
      "loss": 0.8257,
      "step": 10240
    },
    {
      "epoch": 0.4047064397678367,
      "grad_norm": 0.5174313187599182,
      "learning_rate": 7.976665218936314e-05,
      "loss": 0.7555,
      "step": 10250
    },
    {
      "epoch": 0.4051012753188297,
      "grad_norm": 0.5707756876945496,
      "learning_rate": 7.974691041181347e-05,
      "loss": 0.7387,
      "step": 10260
    },
    {
      "epoch": 0.40549611086982273,
      "grad_norm": 0.6172621250152588,
      "learning_rate": 7.972716863426384e-05,
      "loss": 0.7363,
      "step": 10270
    },
    {
      "epoch": 0.40589094642081575,
      "grad_norm": 0.5760958194732666,
      "learning_rate": 7.970742685671419e-05,
      "loss": 0.7443,
      "step": 10280
    },
    {
      "epoch": 0.40628578197180876,
      "grad_norm": 0.47997725009918213,
      "learning_rate": 7.968768507916454e-05,
      "loss": 0.8021,
      "step": 10290
    },
    {
      "epoch": 0.40668061752280177,
      "grad_norm": 0.6206231117248535,
      "learning_rate": 7.966794330161487e-05,
      "loss": 0.753,
      "step": 10300
    },
    {
      "epoch": 0.4070754530737948,
      "grad_norm": 0.5833979249000549,
      "learning_rate": 7.964820152406524e-05,
      "loss": 0.8106,
      "step": 10310
    },
    {
      "epoch": 0.4074702886247878,
      "grad_norm": 0.6264281272888184,
      "learning_rate": 7.962845974651559e-05,
      "loss": 0.8017,
      "step": 10320
    },
    {
      "epoch": 0.4078651241757808,
      "grad_norm": 0.4768303334712982,
      "learning_rate": 7.960871796896592e-05,
      "loss": 0.7544,
      "step": 10330
    },
    {
      "epoch": 0.4082599597267738,
      "grad_norm": 0.5885325074195862,
      "learning_rate": 7.958897619141627e-05,
      "loss": 0.7586,
      "step": 10340
    },
    {
      "epoch": 0.4086547952777668,
      "grad_norm": 0.5748338103294373,
      "learning_rate": 7.956923441386664e-05,
      "loss": 0.831,
      "step": 10350
    },
    {
      "epoch": 0.40904963082875984,
      "grad_norm": 0.5912254452705383,
      "learning_rate": 7.954949263631697e-05,
      "loss": 0.7899,
      "step": 10360
    },
    {
      "epoch": 0.40944446637975285,
      "grad_norm": 0.5866057872772217,
      "learning_rate": 7.952975085876732e-05,
      "loss": 0.7806,
      "step": 10370
    },
    {
      "epoch": 0.40983930193074586,
      "grad_norm": 0.5721470713615417,
      "learning_rate": 7.951000908121767e-05,
      "loss": 0.8335,
      "step": 10380
    },
    {
      "epoch": 0.41023413748173887,
      "grad_norm": 0.6130290627479553,
      "learning_rate": 7.949026730366804e-05,
      "loss": 0.7658,
      "step": 10390
    },
    {
      "epoch": 0.4106289730327319,
      "grad_norm": 0.5582466125488281,
      "learning_rate": 7.947052552611837e-05,
      "loss": 0.7153,
      "step": 10400
    },
    {
      "epoch": 0.4110238085837249,
      "grad_norm": 0.5716137886047363,
      "learning_rate": 7.945078374856872e-05,
      "loss": 0.7928,
      "step": 10410
    },
    {
      "epoch": 0.4114186441347179,
      "grad_norm": 0.9347785115242004,
      "learning_rate": 7.943104197101907e-05,
      "loss": 0.8239,
      "step": 10420
    },
    {
      "epoch": 0.4118134796857109,
      "grad_norm": 0.571219265460968,
      "learning_rate": 7.941130019346942e-05,
      "loss": 0.7948,
      "step": 10430
    },
    {
      "epoch": 0.4122083152367039,
      "grad_norm": 0.9266180396080017,
      "learning_rate": 7.939155841591977e-05,
      "loss": 0.7431,
      "step": 10440
    },
    {
      "epoch": 0.41260315078769694,
      "grad_norm": 0.7053554058074951,
      "learning_rate": 7.937181663837012e-05,
      "loss": 0.7661,
      "step": 10450
    },
    {
      "epoch": 0.41299798633868995,
      "grad_norm": 0.6001470685005188,
      "learning_rate": 7.935207486082047e-05,
      "loss": 0.7868,
      "step": 10460
    },
    {
      "epoch": 0.41339282188968296,
      "grad_norm": 0.6880695223808289,
      "learning_rate": 7.933233308327082e-05,
      "loss": 0.7776,
      "step": 10470
    },
    {
      "epoch": 0.41378765744067597,
      "grad_norm": 0.7112447619438171,
      "learning_rate": 7.931259130572117e-05,
      "loss": 0.798,
      "step": 10480
    },
    {
      "epoch": 0.414182492991669,
      "grad_norm": 0.5369895100593567,
      "learning_rate": 7.929284952817152e-05,
      "loss": 0.7602,
      "step": 10490
    },
    {
      "epoch": 0.414577328542662,
      "grad_norm": 0.5969012975692749,
      "learning_rate": 7.927310775062187e-05,
      "loss": 0.7438,
      "step": 10500
    },
    {
      "epoch": 0.414972164093655,
      "grad_norm": 0.5592411756515503,
      "learning_rate": 7.925336597307222e-05,
      "loss": 0.798,
      "step": 10510
    },
    {
      "epoch": 0.415366999644648,
      "grad_norm": 0.7702694535255432,
      "learning_rate": 7.923362419552257e-05,
      "loss": 0.7563,
      "step": 10520
    },
    {
      "epoch": 0.415761835195641,
      "grad_norm": 0.6867560744285583,
      "learning_rate": 7.921388241797292e-05,
      "loss": 0.7758,
      "step": 10530
    },
    {
      "epoch": 0.41615667074663404,
      "grad_norm": 0.6389646530151367,
      "learning_rate": 7.919414064042326e-05,
      "loss": 0.7705,
      "step": 10540
    },
    {
      "epoch": 0.41655150629762705,
      "grad_norm": 0.5514368414878845,
      "learning_rate": 7.917439886287362e-05,
      "loss": 0.754,
      "step": 10550
    },
    {
      "epoch": 0.41694634184862006,
      "grad_norm": 0.7363442182540894,
      "learning_rate": 7.915465708532397e-05,
      "loss": 0.7931,
      "step": 10560
    },
    {
      "epoch": 0.4173411773996131,
      "grad_norm": 0.5571404695510864,
      "learning_rate": 7.913491530777432e-05,
      "loss": 0.7394,
      "step": 10570
    },
    {
      "epoch": 0.4177360129506061,
      "grad_norm": 0.6186783313751221,
      "learning_rate": 7.911517353022466e-05,
      "loss": 0.7742,
      "step": 10580
    },
    {
      "epoch": 0.4181308485015991,
      "grad_norm": 0.6191704273223877,
      "learning_rate": 7.909543175267502e-05,
      "loss": 0.7758,
      "step": 10590
    },
    {
      "epoch": 0.4185256840525921,
      "grad_norm": 0.5606446266174316,
      "learning_rate": 7.907568997512537e-05,
      "loss": 0.7754,
      "step": 10600
    },
    {
      "epoch": 0.4189205196035851,
      "grad_norm": 0.6329748630523682,
      "learning_rate": 7.905594819757571e-05,
      "loss": 0.8179,
      "step": 10610
    },
    {
      "epoch": 0.41931535515457813,
      "grad_norm": 0.6428183913230896,
      "learning_rate": 7.903620642002606e-05,
      "loss": 0.7822,
      "step": 10620
    },
    {
      "epoch": 0.41971019070557114,
      "grad_norm": 0.6314879059791565,
      "learning_rate": 7.901646464247642e-05,
      "loss": 0.778,
      "step": 10630
    },
    {
      "epoch": 0.42010502625656415,
      "grad_norm": 0.5932490825653076,
      "learning_rate": 7.899672286492676e-05,
      "loss": 0.7907,
      "step": 10640
    },
    {
      "epoch": 0.42049986180755716,
      "grad_norm": 0.6160391569137573,
      "learning_rate": 7.897698108737711e-05,
      "loss": 0.7765,
      "step": 10650
    },
    {
      "epoch": 0.4208946973585502,
      "grad_norm": 0.751589834690094,
      "learning_rate": 7.895723930982746e-05,
      "loss": 0.8089,
      "step": 10660
    },
    {
      "epoch": 0.4212895329095432,
      "grad_norm": 0.65382981300354,
      "learning_rate": 7.893749753227782e-05,
      "loss": 0.777,
      "step": 10670
    },
    {
      "epoch": 0.4216843684605362,
      "grad_norm": 0.6959202885627747,
      "learning_rate": 7.891775575472816e-05,
      "loss": 0.7734,
      "step": 10680
    },
    {
      "epoch": 0.4220792040115292,
      "grad_norm": 0.8051504492759705,
      "learning_rate": 7.889801397717851e-05,
      "loss": 0.7706,
      "step": 10690
    },
    {
      "epoch": 0.4224740395625222,
      "grad_norm": 0.6367541551589966,
      "learning_rate": 7.887827219962886e-05,
      "loss": 0.7712,
      "step": 10700
    },
    {
      "epoch": 0.42286887511351523,
      "grad_norm": 0.5509194135665894,
      "learning_rate": 7.885853042207921e-05,
      "loss": 0.7885,
      "step": 10710
    },
    {
      "epoch": 0.42326371066450824,
      "grad_norm": 0.5388572812080383,
      "learning_rate": 7.883878864452956e-05,
      "loss": 0.7521,
      "step": 10720
    },
    {
      "epoch": 0.42365854621550125,
      "grad_norm": 0.5850213766098022,
      "learning_rate": 7.881904686697991e-05,
      "loss": 0.8103,
      "step": 10730
    },
    {
      "epoch": 0.42405338176649426,
      "grad_norm": 0.8835914134979248,
      "learning_rate": 7.879930508943026e-05,
      "loss": 0.7848,
      "step": 10740
    },
    {
      "epoch": 0.4244482173174873,
      "grad_norm": 0.5876768827438354,
      "learning_rate": 7.877956331188061e-05,
      "loss": 0.7525,
      "step": 10750
    },
    {
      "epoch": 0.4248430528684803,
      "grad_norm": 0.7152480483055115,
      "learning_rate": 7.875982153433096e-05,
      "loss": 0.8228,
      "step": 10760
    },
    {
      "epoch": 0.4252378884194733,
      "grad_norm": 0.609553873538971,
      "learning_rate": 7.87400797567813e-05,
      "loss": 0.7713,
      "step": 10770
    },
    {
      "epoch": 0.4256327239704663,
      "grad_norm": 0.6430588960647583,
      "learning_rate": 7.872033797923166e-05,
      "loss": 0.8043,
      "step": 10780
    },
    {
      "epoch": 0.4260275595214593,
      "grad_norm": 0.5735174417495728,
      "learning_rate": 7.8700596201682e-05,
      "loss": 0.7601,
      "step": 10790
    },
    {
      "epoch": 0.42642239507245233,
      "grad_norm": 0.5590189099311829,
      "learning_rate": 7.868085442413236e-05,
      "loss": 0.7755,
      "step": 10800
    },
    {
      "epoch": 0.42681723062344534,
      "grad_norm": 0.6007696390151978,
      "learning_rate": 7.86611126465827e-05,
      "loss": 0.7515,
      "step": 10810
    },
    {
      "epoch": 0.42721206617443835,
      "grad_norm": 1.0671344995498657,
      "learning_rate": 7.864137086903304e-05,
      "loss": 0.7892,
      "step": 10820
    },
    {
      "epoch": 0.42760690172543137,
      "grad_norm": 0.8574020862579346,
      "learning_rate": 7.86216290914834e-05,
      "loss": 0.781,
      "step": 10830
    },
    {
      "epoch": 0.4280017372764244,
      "grad_norm": 0.5459825396537781,
      "learning_rate": 7.860188731393376e-05,
      "loss": 0.8249,
      "step": 10840
    },
    {
      "epoch": 0.4283965728274174,
      "grad_norm": 0.6124423146247864,
      "learning_rate": 7.85821455363841e-05,
      "loss": 0.771,
      "step": 10850
    },
    {
      "epoch": 0.4287914083784104,
      "grad_norm": 0.5678876042366028,
      "learning_rate": 7.856240375883444e-05,
      "loss": 0.7732,
      "step": 10860
    },
    {
      "epoch": 0.4291862439294034,
      "grad_norm": 0.626761257648468,
      "learning_rate": 7.85426619812848e-05,
      "loss": 0.776,
      "step": 10870
    },
    {
      "epoch": 0.4295810794803964,
      "grad_norm": 0.5961143970489502,
      "learning_rate": 7.852292020373516e-05,
      "loss": 0.7503,
      "step": 10880
    },
    {
      "epoch": 0.42997591503138943,
      "grad_norm": 0.7545872330665588,
      "learning_rate": 7.850317842618549e-05,
      "loss": 0.8112,
      "step": 10890
    },
    {
      "epoch": 0.43037075058238244,
      "grad_norm": 0.5637950301170349,
      "learning_rate": 7.848343664863584e-05,
      "loss": 0.7996,
      "step": 10900
    },
    {
      "epoch": 0.43076558613337546,
      "grad_norm": 0.9460324645042419,
      "learning_rate": 7.84636948710862e-05,
      "loss": 0.8025,
      "step": 10910
    },
    {
      "epoch": 0.43116042168436847,
      "grad_norm": 0.8904045224189758,
      "learning_rate": 7.844395309353654e-05,
      "loss": 0.7876,
      "step": 10920
    },
    {
      "epoch": 0.4315552572353615,
      "grad_norm": 0.5654942393302917,
      "learning_rate": 7.842421131598689e-05,
      "loss": 0.8155,
      "step": 10930
    },
    {
      "epoch": 0.4319500927863545,
      "grad_norm": 0.6249062418937683,
      "learning_rate": 7.840446953843724e-05,
      "loss": 0.7615,
      "step": 10940
    },
    {
      "epoch": 0.4323449283373475,
      "grad_norm": 0.707798182964325,
      "learning_rate": 7.83847277608876e-05,
      "loss": 0.7794,
      "step": 10950
    },
    {
      "epoch": 0.4327397638883405,
      "grad_norm": 0.7055657505989075,
      "learning_rate": 7.836498598333794e-05,
      "loss": 0.8003,
      "step": 10960
    },
    {
      "epoch": 0.4331345994393335,
      "grad_norm": 0.737001359462738,
      "learning_rate": 7.834524420578829e-05,
      "loss": 0.8132,
      "step": 10970
    },
    {
      "epoch": 0.43352943499032653,
      "grad_norm": 0.5627896189689636,
      "learning_rate": 7.832550242823864e-05,
      "loss": 0.7723,
      "step": 10980
    },
    {
      "epoch": 0.43392427054131955,
      "grad_norm": 0.6045676469802856,
      "learning_rate": 7.830576065068899e-05,
      "loss": 0.8118,
      "step": 10990
    },
    {
      "epoch": 0.43431910609231256,
      "grad_norm": 0.7014980316162109,
      "learning_rate": 7.828601887313934e-05,
      "loss": 0.7591,
      "step": 11000
    },
    {
      "epoch": 0.43431910609231256,
      "eval_loss": 0.8028453588485718,
      "eval_runtime": 1218.6476,
      "eval_samples_per_second": 9.237,
      "eval_steps_per_second": 9.237,
      "step": 11000
    },
    {
      "epoch": 0.43471394164330557,
      "grad_norm": 0.5235527157783508,
      "learning_rate": 7.826627709558969e-05,
      "loss": 0.7712,
      "step": 11010
    },
    {
      "epoch": 0.4351087771942986,
      "grad_norm": 0.689193069934845,
      "learning_rate": 7.824653531804004e-05,
      "loss": 0.7366,
      "step": 11020
    },
    {
      "epoch": 0.4355036127452916,
      "grad_norm": 0.5520110726356506,
      "learning_rate": 7.822679354049038e-05,
      "loss": 0.7921,
      "step": 11030
    },
    {
      "epoch": 0.4358984482962846,
      "grad_norm": 0.5769424438476562,
      "learning_rate": 7.820705176294074e-05,
      "loss": 0.8033,
      "step": 11040
    },
    {
      "epoch": 0.4362932838472776,
      "grad_norm": 0.8008092641830444,
      "learning_rate": 7.818730998539109e-05,
      "loss": 0.7472,
      "step": 11050
    },
    {
      "epoch": 0.4366881193982706,
      "grad_norm": 0.5891882181167603,
      "learning_rate": 7.816756820784144e-05,
      "loss": 0.7886,
      "step": 11060
    },
    {
      "epoch": 0.43708295494926364,
      "grad_norm": 0.6016013622283936,
      "learning_rate": 7.814782643029178e-05,
      "loss": 0.7486,
      "step": 11070
    },
    {
      "epoch": 0.43747779050025665,
      "grad_norm": 0.6913837194442749,
      "learning_rate": 7.812808465274214e-05,
      "loss": 0.8244,
      "step": 11080
    },
    {
      "epoch": 0.43787262605124966,
      "grad_norm": 0.7185695171356201,
      "learning_rate": 7.810834287519249e-05,
      "loss": 0.7734,
      "step": 11090
    },
    {
      "epoch": 0.43826746160224267,
      "grad_norm": 1.0689364671707153,
      "learning_rate": 7.808860109764283e-05,
      "loss": 0.7986,
      "step": 11100
    },
    {
      "epoch": 0.4386622971532357,
      "grad_norm": 0.6286224722862244,
      "learning_rate": 7.806885932009318e-05,
      "loss": 0.7646,
      "step": 11110
    },
    {
      "epoch": 0.4390571327042287,
      "grad_norm": 0.7025226354598999,
      "learning_rate": 7.804911754254354e-05,
      "loss": 0.8113,
      "step": 11120
    },
    {
      "epoch": 0.4394519682552217,
      "grad_norm": 0.6238678693771362,
      "learning_rate": 7.802937576499389e-05,
      "loss": 0.7753,
      "step": 11130
    },
    {
      "epoch": 0.4398468038062147,
      "grad_norm": 0.6760612726211548,
      "learning_rate": 7.800963398744423e-05,
      "loss": 0.7254,
      "step": 11140
    },
    {
      "epoch": 0.4402416393572077,
      "grad_norm": 0.5640722513198853,
      "learning_rate": 7.798989220989458e-05,
      "loss": 0.8247,
      "step": 11150
    },
    {
      "epoch": 0.44063647490820074,
      "grad_norm": 0.7261711359024048,
      "learning_rate": 7.797015043234494e-05,
      "loss": 0.7545,
      "step": 11160
    },
    {
      "epoch": 0.44103131045919375,
      "grad_norm": 0.5068609118461609,
      "learning_rate": 7.795040865479528e-05,
      "loss": 0.7687,
      "step": 11170
    },
    {
      "epoch": 0.44142614601018676,
      "grad_norm": 0.49453040957450867,
      "learning_rate": 7.793066687724563e-05,
      "loss": 0.7668,
      "step": 11180
    },
    {
      "epoch": 0.44182098156117977,
      "grad_norm": 0.6289105415344238,
      "learning_rate": 7.791092509969598e-05,
      "loss": 0.8102,
      "step": 11190
    },
    {
      "epoch": 0.4422158171121728,
      "grad_norm": 0.6880951523780823,
      "learning_rate": 7.789118332214634e-05,
      "loss": 0.8392,
      "step": 11200
    },
    {
      "epoch": 0.4426106526631658,
      "grad_norm": 0.7563783526420593,
      "learning_rate": 7.787144154459668e-05,
      "loss": 0.8302,
      "step": 11210
    },
    {
      "epoch": 0.4430054882141588,
      "grad_norm": 0.6237449645996094,
      "learning_rate": 7.785169976704703e-05,
      "loss": 0.7881,
      "step": 11220
    },
    {
      "epoch": 0.4434003237651518,
      "grad_norm": 0.8816928267478943,
      "learning_rate": 7.783195798949738e-05,
      "loss": 0.7813,
      "step": 11230
    },
    {
      "epoch": 0.4437951593161448,
      "grad_norm": 0.6561572551727295,
      "learning_rate": 7.781221621194773e-05,
      "loss": 0.743,
      "step": 11240
    },
    {
      "epoch": 0.44418999486713784,
      "grad_norm": 0.7103161811828613,
      "learning_rate": 7.779247443439808e-05,
      "loss": 0.8159,
      "step": 11250
    },
    {
      "epoch": 0.44458483041813085,
      "grad_norm": 0.6636072993278503,
      "learning_rate": 7.777273265684843e-05,
      "loss": 0.793,
      "step": 11260
    },
    {
      "epoch": 0.44497966596912386,
      "grad_norm": 0.7602379322052002,
      "learning_rate": 7.775299087929878e-05,
      "loss": 0.7717,
      "step": 11270
    },
    {
      "epoch": 0.44537450152011687,
      "grad_norm": 0.5484144687652588,
      "learning_rate": 7.773324910174913e-05,
      "loss": 0.775,
      "step": 11280
    },
    {
      "epoch": 0.4457693370711099,
      "grad_norm": 0.477060467004776,
      "learning_rate": 7.771350732419948e-05,
      "loss": 0.7477,
      "step": 11290
    },
    {
      "epoch": 0.4461641726221029,
      "grad_norm": 0.6573362946510315,
      "learning_rate": 7.769376554664982e-05,
      "loss": 0.8272,
      "step": 11300
    },
    {
      "epoch": 0.4465590081730959,
      "grad_norm": 0.6046025156974792,
      "learning_rate": 7.767402376910017e-05,
      "loss": 0.7626,
      "step": 11310
    },
    {
      "epoch": 0.4469538437240889,
      "grad_norm": 0.6280928254127502,
      "learning_rate": 7.765428199155052e-05,
      "loss": 0.8306,
      "step": 11320
    },
    {
      "epoch": 0.44734867927508193,
      "grad_norm": 0.5151898264884949,
      "learning_rate": 7.763454021400087e-05,
      "loss": 0.7749,
      "step": 11330
    },
    {
      "epoch": 0.44774351482607494,
      "grad_norm": 0.604720413684845,
      "learning_rate": 7.761479843645122e-05,
      "loss": 0.8278,
      "step": 11340
    },
    {
      "epoch": 0.44813835037706795,
      "grad_norm": 0.6212661862373352,
      "learning_rate": 7.759505665890156e-05,
      "loss": 0.7589,
      "step": 11350
    },
    {
      "epoch": 0.44853318592806096,
      "grad_norm": 0.6468335390090942,
      "learning_rate": 7.757531488135192e-05,
      "loss": 0.7822,
      "step": 11360
    },
    {
      "epoch": 0.448928021479054,
      "grad_norm": 0.7043411731719971,
      "learning_rate": 7.755557310380227e-05,
      "loss": 0.7543,
      "step": 11370
    },
    {
      "epoch": 0.449322857030047,
      "grad_norm": 0.8183386325836182,
      "learning_rate": 7.753583132625261e-05,
      "loss": 0.7614,
      "step": 11380
    },
    {
      "epoch": 0.44971769258104,
      "grad_norm": 0.6032211184501648,
      "learning_rate": 7.751608954870296e-05,
      "loss": 0.808,
      "step": 11390
    },
    {
      "epoch": 0.450112528132033,
      "grad_norm": 0.6356963515281677,
      "learning_rate": 7.749634777115332e-05,
      "loss": 0.8053,
      "step": 11400
    },
    {
      "epoch": 0.450507363683026,
      "grad_norm": 0.70098477602005,
      "learning_rate": 7.747660599360367e-05,
      "loss": 0.7836,
      "step": 11410
    },
    {
      "epoch": 0.45090219923401903,
      "grad_norm": 0.6196197867393494,
      "learning_rate": 7.745686421605401e-05,
      "loss": 0.7441,
      "step": 11420
    },
    {
      "epoch": 0.45129703478501204,
      "grad_norm": 0.8427067995071411,
      "learning_rate": 7.743712243850436e-05,
      "loss": 0.7699,
      "step": 11430
    },
    {
      "epoch": 0.45169187033600505,
      "grad_norm": 0.5709606409072876,
      "learning_rate": 7.741738066095472e-05,
      "loss": 0.7804,
      "step": 11440
    },
    {
      "epoch": 0.45208670588699806,
      "grad_norm": 0.7092307209968567,
      "learning_rate": 7.739763888340506e-05,
      "loss": 0.7853,
      "step": 11450
    },
    {
      "epoch": 0.4524815414379911,
      "grad_norm": 0.6539269685745239,
      "learning_rate": 7.737789710585541e-05,
      "loss": 0.7754,
      "step": 11460
    },
    {
      "epoch": 0.4528763769889841,
      "grad_norm": 0.6526788473129272,
      "learning_rate": 7.735815532830576e-05,
      "loss": 0.7753,
      "step": 11470
    },
    {
      "epoch": 0.4532712125399771,
      "grad_norm": 0.8863881826400757,
      "learning_rate": 7.733841355075612e-05,
      "loss": 0.7812,
      "step": 11480
    },
    {
      "epoch": 0.4536660480909701,
      "grad_norm": 0.605750322341919,
      "learning_rate": 7.731867177320646e-05,
      "loss": 0.7893,
      "step": 11490
    },
    {
      "epoch": 0.4540608836419631,
      "grad_norm": 0.7848479747772217,
      "learning_rate": 7.729892999565681e-05,
      "loss": 0.7437,
      "step": 11500
    },
    {
      "epoch": 0.45445571919295613,
      "grad_norm": 0.5754777193069458,
      "learning_rate": 7.727918821810716e-05,
      "loss": 0.7972,
      "step": 11510
    },
    {
      "epoch": 0.45485055474394914,
      "grad_norm": 0.5175760388374329,
      "learning_rate": 7.725944644055751e-05,
      "loss": 0.7587,
      "step": 11520
    },
    {
      "epoch": 0.45524539029494215,
      "grad_norm": 0.7111361026763916,
      "learning_rate": 7.723970466300786e-05,
      "loss": 0.7792,
      "step": 11530
    },
    {
      "epoch": 0.45564022584593517,
      "grad_norm": 0.6475380063056946,
      "learning_rate": 7.721996288545821e-05,
      "loss": 0.8311,
      "step": 11540
    },
    {
      "epoch": 0.4560350613969282,
      "grad_norm": 0.5774902105331421,
      "learning_rate": 7.720022110790856e-05,
      "loss": 0.814,
      "step": 11550
    },
    {
      "epoch": 0.4564298969479212,
      "grad_norm": 0.6649884581565857,
      "learning_rate": 7.718047933035891e-05,
      "loss": 0.7315,
      "step": 11560
    },
    {
      "epoch": 0.4568247324989142,
      "grad_norm": 0.79709792137146,
      "learning_rate": 7.716073755280926e-05,
      "loss": 0.784,
      "step": 11570
    },
    {
      "epoch": 0.4572195680499072,
      "grad_norm": 0.5858976244926453,
      "learning_rate": 7.714099577525961e-05,
      "loss": 0.7674,
      "step": 11580
    },
    {
      "epoch": 0.4576144036009002,
      "grad_norm": 0.7127171158790588,
      "learning_rate": 7.712125399770996e-05,
      "loss": 0.761,
      "step": 11590
    },
    {
      "epoch": 0.45800923915189323,
      "grad_norm": 0.6975634694099426,
      "learning_rate": 7.710151222016031e-05,
      "loss": 0.7407,
      "step": 11600
    },
    {
      "epoch": 0.45840407470288624,
      "grad_norm": 0.9099305868148804,
      "learning_rate": 7.708177044261066e-05,
      "loss": 0.8393,
      "step": 11610
    },
    {
      "epoch": 0.45879891025387926,
      "grad_norm": 0.7610312700271606,
      "learning_rate": 7.706202866506101e-05,
      "loss": 0.7412,
      "step": 11620
    },
    {
      "epoch": 0.45919374580487227,
      "grad_norm": 0.49052533507347107,
      "learning_rate": 7.704228688751134e-05,
      "loss": 0.8089,
      "step": 11630
    },
    {
      "epoch": 0.4595885813558653,
      "grad_norm": 0.7969157099723816,
      "learning_rate": 7.702254510996171e-05,
      "loss": 0.7664,
      "step": 11640
    },
    {
      "epoch": 0.4599834169068583,
      "grad_norm": 0.5576817989349365,
      "learning_rate": 7.700280333241206e-05,
      "loss": 0.8039,
      "step": 11650
    },
    {
      "epoch": 0.4603782524578513,
      "grad_norm": 0.9413078427314758,
      "learning_rate": 7.69830615548624e-05,
      "loss": 0.8512,
      "step": 11660
    },
    {
      "epoch": 0.4607730880088443,
      "grad_norm": 0.5337902903556824,
      "learning_rate": 7.696331977731274e-05,
      "loss": 0.7543,
      "step": 11670
    },
    {
      "epoch": 0.4611679235598373,
      "grad_norm": 0.6732361316680908,
      "learning_rate": 7.694357799976311e-05,
      "loss": 0.7732,
      "step": 11680
    },
    {
      "epoch": 0.46156275911083033,
      "grad_norm": 0.5412266850471497,
      "learning_rate": 7.692383622221346e-05,
      "loss": 0.7833,
      "step": 11690
    },
    {
      "epoch": 0.46195759466182335,
      "grad_norm": 0.8496806025505066,
      "learning_rate": 7.69040944446638e-05,
      "loss": 0.7058,
      "step": 11700
    },
    {
      "epoch": 0.46235243021281636,
      "grad_norm": 0.802869975566864,
      "learning_rate": 7.688435266711414e-05,
      "loss": 0.8003,
      "step": 11710
    },
    {
      "epoch": 0.46274726576380937,
      "grad_norm": 0.6764482259750366,
      "learning_rate": 7.686461088956451e-05,
      "loss": 0.7614,
      "step": 11720
    },
    {
      "epoch": 0.4631421013148024,
      "grad_norm": 0.693666934967041,
      "learning_rate": 7.684486911201484e-05,
      "loss": 0.8161,
      "step": 11730
    },
    {
      "epoch": 0.4635369368657954,
      "grad_norm": 0.5092883110046387,
      "learning_rate": 7.68251273344652e-05,
      "loss": 0.8026,
      "step": 11740
    },
    {
      "epoch": 0.4639317724167884,
      "grad_norm": 0.6059123277664185,
      "learning_rate": 7.680538555691554e-05,
      "loss": 0.7823,
      "step": 11750
    },
    {
      "epoch": 0.4643266079677814,
      "grad_norm": 0.5742544531822205,
      "learning_rate": 7.678564377936591e-05,
      "loss": 0.7688,
      "step": 11760
    },
    {
      "epoch": 0.4647214435187744,
      "grad_norm": 0.6084069609642029,
      "learning_rate": 7.676590200181624e-05,
      "loss": 0.7109,
      "step": 11770
    },
    {
      "epoch": 0.46511627906976744,
      "grad_norm": 0.7106086015701294,
      "learning_rate": 7.67461602242666e-05,
      "loss": 0.7471,
      "step": 11780
    },
    {
      "epoch": 0.46551111462076045,
      "grad_norm": 0.6430622339248657,
      "learning_rate": 7.672641844671694e-05,
      "loss": 0.7985,
      "step": 11790
    },
    {
      "epoch": 0.46590595017175346,
      "grad_norm": 0.794526994228363,
      "learning_rate": 7.67066766691673e-05,
      "loss": 0.7687,
      "step": 11800
    },
    {
      "epoch": 0.46630078572274647,
      "grad_norm": 0.7159641981124878,
      "learning_rate": 7.668693489161764e-05,
      "loss": 0.7682,
      "step": 11810
    },
    {
      "epoch": 0.4666956212737395,
      "grad_norm": 0.6485618352890015,
      "learning_rate": 7.6667193114068e-05,
      "loss": 0.7787,
      "step": 11820
    },
    {
      "epoch": 0.4670904568247325,
      "grad_norm": 0.6397709250450134,
      "learning_rate": 7.664745133651834e-05,
      "loss": 0.823,
      "step": 11830
    },
    {
      "epoch": 0.4674852923757255,
      "grad_norm": 0.7383333444595337,
      "learning_rate": 7.662770955896869e-05,
      "loss": 0.7991,
      "step": 11840
    },
    {
      "epoch": 0.4678801279267185,
      "grad_norm": 0.6217195987701416,
      "learning_rate": 7.660796778141904e-05,
      "loss": 0.7518,
      "step": 11850
    },
    {
      "epoch": 0.4682749634777115,
      "grad_norm": 0.5647761225700378,
      "learning_rate": 7.658822600386939e-05,
      "loss": 0.7628,
      "step": 11860
    },
    {
      "epoch": 0.46866979902870454,
      "grad_norm": 0.609319269657135,
      "learning_rate": 7.656848422631974e-05,
      "loss": 0.7604,
      "step": 11870
    },
    {
      "epoch": 0.46906463457969755,
      "grad_norm": 0.8427051901817322,
      "learning_rate": 7.654874244877009e-05,
      "loss": 0.8216,
      "step": 11880
    },
    {
      "epoch": 0.46945947013069056,
      "grad_norm": 0.6751169562339783,
      "learning_rate": 7.652900067122044e-05,
      "loss": 0.756,
      "step": 11890
    },
    {
      "epoch": 0.46985430568168357,
      "grad_norm": 0.6505918502807617,
      "learning_rate": 7.650925889367079e-05,
      "loss": 0.7877,
      "step": 11900
    },
    {
      "epoch": 0.4702491412326766,
      "grad_norm": 0.6652233004570007,
      "learning_rate": 7.648951711612113e-05,
      "loss": 0.7737,
      "step": 11910
    },
    {
      "epoch": 0.4706439767836696,
      "grad_norm": 0.7379721999168396,
      "learning_rate": 7.646977533857149e-05,
      "loss": 0.774,
      "step": 11920
    },
    {
      "epoch": 0.4710388123346626,
      "grad_norm": 0.6765847206115723,
      "learning_rate": 7.645003356102184e-05,
      "loss": 0.8075,
      "step": 11930
    },
    {
      "epoch": 0.4714336478856556,
      "grad_norm": 0.7207562327384949,
      "learning_rate": 7.643029178347218e-05,
      "loss": 0.8017,
      "step": 11940
    },
    {
      "epoch": 0.4718284834366486,
      "grad_norm": 0.6946515440940857,
      "learning_rate": 7.641055000592253e-05,
      "loss": 0.7984,
      "step": 11950
    },
    {
      "epoch": 0.47222331898764164,
      "grad_norm": 0.5900394916534424,
      "learning_rate": 7.639080822837289e-05,
      "loss": 0.8232,
      "step": 11960
    },
    {
      "epoch": 0.47261815453863465,
      "grad_norm": 0.5711060762405396,
      "learning_rate": 7.637106645082324e-05,
      "loss": 0.744,
      "step": 11970
    },
    {
      "epoch": 0.47301299008962766,
      "grad_norm": 0.7314627766609192,
      "learning_rate": 7.635132467327358e-05,
      "loss": 0.7196,
      "step": 11980
    },
    {
      "epoch": 0.47340782564062067,
      "grad_norm": 0.7772358655929565,
      "learning_rate": 7.633158289572393e-05,
      "loss": 0.7652,
      "step": 11990
    },
    {
      "epoch": 0.4738026611916137,
      "grad_norm": 0.6173547506332397,
      "learning_rate": 7.631184111817429e-05,
      "loss": 0.7914,
      "step": 12000
    },
    {
      "epoch": 0.4738026611916137,
      "eval_loss": 0.800610363483429,
      "eval_runtime": 1217.0471,
      "eval_samples_per_second": 9.249,
      "eval_steps_per_second": 9.249,
      "step": 12000
    },
    {
      "epoch": 0.4741974967426067,
      "grad_norm": 0.724773645401001,
      "learning_rate": 7.629209934062463e-05,
      "loss": 0.7599,
      "step": 12010
    },
    {
      "epoch": 0.4745923322935997,
      "grad_norm": 0.7255359888076782,
      "learning_rate": 7.627235756307498e-05,
      "loss": 0.8078,
      "step": 12020
    },
    {
      "epoch": 0.4749871678445927,
      "grad_norm": 0.6368002891540527,
      "learning_rate": 7.625261578552533e-05,
      "loss": 0.7681,
      "step": 12030
    },
    {
      "epoch": 0.47538200339558573,
      "grad_norm": 0.583946943283081,
      "learning_rate": 7.623287400797569e-05,
      "loss": 0.8104,
      "step": 12040
    },
    {
      "epoch": 0.47577683894657874,
      "grad_norm": 0.8104519248008728,
      "learning_rate": 7.621313223042603e-05,
      "loss": 0.7491,
      "step": 12050
    },
    {
      "epoch": 0.47617167449757175,
      "grad_norm": 0.838659942150116,
      "learning_rate": 7.619339045287638e-05,
      "loss": 0.7706,
      "step": 12060
    },
    {
      "epoch": 0.47656651004856476,
      "grad_norm": 0.6716185212135315,
      "learning_rate": 7.617364867532673e-05,
      "loss": 0.8469,
      "step": 12070
    },
    {
      "epoch": 0.4769613455995578,
      "grad_norm": 0.7217696905136108,
      "learning_rate": 7.615390689777708e-05,
      "loss": 0.7324,
      "step": 12080
    },
    {
      "epoch": 0.4773561811505508,
      "grad_norm": 0.6986469626426697,
      "learning_rate": 7.613416512022743e-05,
      "loss": 0.7861,
      "step": 12090
    },
    {
      "epoch": 0.4777510167015438,
      "grad_norm": 0.7877245545387268,
      "learning_rate": 7.611442334267778e-05,
      "loss": 0.7593,
      "step": 12100
    },
    {
      "epoch": 0.4781458522525368,
      "grad_norm": 0.5684877038002014,
      "learning_rate": 7.609468156512813e-05,
      "loss": 0.7327,
      "step": 12110
    },
    {
      "epoch": 0.4785406878035298,
      "grad_norm": 0.7588016390800476,
      "learning_rate": 7.607493978757848e-05,
      "loss": 0.8039,
      "step": 12120
    },
    {
      "epoch": 0.47893552335452283,
      "grad_norm": 0.8011178970336914,
      "learning_rate": 7.605519801002883e-05,
      "loss": 0.7262,
      "step": 12130
    },
    {
      "epoch": 0.47933035890551584,
      "grad_norm": 0.5131425857543945,
      "learning_rate": 7.603545623247918e-05,
      "loss": 0.7972,
      "step": 12140
    },
    {
      "epoch": 0.47972519445650885,
      "grad_norm": 0.6444962024688721,
      "learning_rate": 7.601571445492953e-05,
      "loss": 0.7757,
      "step": 12150
    },
    {
      "epoch": 0.48012003000750186,
      "grad_norm": 0.6317589282989502,
      "learning_rate": 7.599597267737988e-05,
      "loss": 0.7958,
      "step": 12160
    },
    {
      "epoch": 0.4805148655584949,
      "grad_norm": 0.7763316035270691,
      "learning_rate": 7.597623089983023e-05,
      "loss": 0.7637,
      "step": 12170
    },
    {
      "epoch": 0.4809097011094879,
      "grad_norm": 0.5731996297836304,
      "learning_rate": 7.595648912228058e-05,
      "loss": 0.7766,
      "step": 12180
    },
    {
      "epoch": 0.4813045366604809,
      "grad_norm": 0.5893585681915283,
      "learning_rate": 7.593674734473091e-05,
      "loss": 0.8053,
      "step": 12190
    },
    {
      "epoch": 0.4816993722114739,
      "grad_norm": 0.6542052030563354,
      "learning_rate": 7.591700556718128e-05,
      "loss": 0.8174,
      "step": 12200
    },
    {
      "epoch": 0.4820942077624669,
      "grad_norm": 0.7257959842681885,
      "learning_rate": 7.589726378963163e-05,
      "loss": 0.7761,
      "step": 12210
    },
    {
      "epoch": 0.48248904331345993,
      "grad_norm": 0.5800817012786865,
      "learning_rate": 7.587752201208198e-05,
      "loss": 0.7822,
      "step": 12220
    },
    {
      "epoch": 0.48288387886445294,
      "grad_norm": 0.8854871988296509,
      "learning_rate": 7.585778023453231e-05,
      "loss": 0.81,
      "step": 12230
    },
    {
      "epoch": 0.48327871441544595,
      "grad_norm": 0.8287608623504639,
      "learning_rate": 7.583803845698268e-05,
      "loss": 0.7802,
      "step": 12240
    },
    {
      "epoch": 0.48367354996643896,
      "grad_norm": 0.6338925957679749,
      "learning_rate": 7.581829667943303e-05,
      "loss": 0.8126,
      "step": 12250
    },
    {
      "epoch": 0.484068385517432,
      "grad_norm": 0.6285956501960754,
      "learning_rate": 7.579855490188336e-05,
      "loss": 0.7888,
      "step": 12260
    },
    {
      "epoch": 0.484463221068425,
      "grad_norm": 0.8282953500747681,
      "learning_rate": 7.577881312433371e-05,
      "loss": 0.7539,
      "step": 12270
    },
    {
      "epoch": 0.484858056619418,
      "grad_norm": 0.5217421054840088,
      "learning_rate": 7.575907134678408e-05,
      "loss": 0.7743,
      "step": 12280
    },
    {
      "epoch": 0.485252892170411,
      "grad_norm": 0.5855716466903687,
      "learning_rate": 7.573932956923441e-05,
      "loss": 0.7336,
      "step": 12290
    },
    {
      "epoch": 0.485647727721404,
      "grad_norm": 0.615601122379303,
      "learning_rate": 7.571958779168476e-05,
      "loss": 0.8046,
      "step": 12300
    },
    {
      "epoch": 0.48604256327239703,
      "grad_norm": 0.6344769597053528,
      "learning_rate": 7.569984601413511e-05,
      "loss": 0.7668,
      "step": 12310
    },
    {
      "epoch": 0.48643739882339004,
      "grad_norm": 0.5652663707733154,
      "learning_rate": 7.568010423658548e-05,
      "loss": 0.7971,
      "step": 12320
    },
    {
      "epoch": 0.48683223437438305,
      "grad_norm": 0.6381734013557434,
      "learning_rate": 7.566036245903581e-05,
      "loss": 0.7899,
      "step": 12330
    },
    {
      "epoch": 0.48722706992537607,
      "grad_norm": 0.7916887402534485,
      "learning_rate": 7.564062068148616e-05,
      "loss": 0.8185,
      "step": 12340
    },
    {
      "epoch": 0.4876219054763691,
      "grad_norm": 0.6230306029319763,
      "learning_rate": 7.562087890393651e-05,
      "loss": 0.7919,
      "step": 12350
    },
    {
      "epoch": 0.4880167410273621,
      "grad_norm": 0.6173121929168701,
      "learning_rate": 7.560113712638686e-05,
      "loss": 0.7551,
      "step": 12360
    },
    {
      "epoch": 0.4884115765783551,
      "grad_norm": 0.569236695766449,
      "learning_rate": 7.558139534883721e-05,
      "loss": 0.7609,
      "step": 12370
    },
    {
      "epoch": 0.4888064121293481,
      "grad_norm": 0.7227854132652283,
      "learning_rate": 7.556165357128756e-05,
      "loss": 0.7643,
      "step": 12380
    },
    {
      "epoch": 0.4892012476803411,
      "grad_norm": 0.6859042644500732,
      "learning_rate": 7.554191179373791e-05,
      "loss": 0.7805,
      "step": 12390
    },
    {
      "epoch": 0.48959608323133413,
      "grad_norm": 0.7783278822898865,
      "learning_rate": 7.552217001618826e-05,
      "loss": 0.7485,
      "step": 12400
    },
    {
      "epoch": 0.48999091878232715,
      "grad_norm": 0.856353223323822,
      "learning_rate": 7.550242823863861e-05,
      "loss": 0.8183,
      "step": 12410
    },
    {
      "epoch": 0.49038575433332016,
      "grad_norm": 0.5756076574325562,
      "learning_rate": 7.548268646108896e-05,
      "loss": 0.7654,
      "step": 12420
    },
    {
      "epoch": 0.49078058988431317,
      "grad_norm": 0.6476726531982422,
      "learning_rate": 7.546294468353931e-05,
      "loss": 0.7692,
      "step": 12430
    },
    {
      "epoch": 0.4911754254353062,
      "grad_norm": 0.7880082130432129,
      "learning_rate": 7.544320290598966e-05,
      "loss": 0.7796,
      "step": 12440
    },
    {
      "epoch": 0.4915702609862992,
      "grad_norm": 0.6801888942718506,
      "learning_rate": 7.542346112844001e-05,
      "loss": 0.7226,
      "step": 12450
    },
    {
      "epoch": 0.4919650965372922,
      "grad_norm": 0.5137718319892883,
      "learning_rate": 7.540371935089036e-05,
      "loss": 0.7834,
      "step": 12460
    },
    {
      "epoch": 0.4923599320882852,
      "grad_norm": 0.5988634824752808,
      "learning_rate": 7.53839775733407e-05,
      "loss": 0.7915,
      "step": 12470
    },
    {
      "epoch": 0.4927547676392782,
      "grad_norm": 0.8123577833175659,
      "learning_rate": 7.536423579579106e-05,
      "loss": 0.7876,
      "step": 12480
    },
    {
      "epoch": 0.49314960319027124,
      "grad_norm": 0.716190755367279,
      "learning_rate": 7.534449401824141e-05,
      "loss": 0.7678,
      "step": 12490
    },
    {
      "epoch": 0.49354443874126425,
      "grad_norm": 0.9392123818397522,
      "learning_rate": 7.532475224069176e-05,
      "loss": 0.7721,
      "step": 12500
    },
    {
      "epoch": 0.49393927429225726,
      "grad_norm": 0.5584532618522644,
      "learning_rate": 7.53050104631421e-05,
      "loss": 0.7722,
      "step": 12510
    },
    {
      "epoch": 0.49433410984325027,
      "grad_norm": 0.6531572341918945,
      "learning_rate": 7.528526868559246e-05,
      "loss": 0.741,
      "step": 12520
    },
    {
      "epoch": 0.4947289453942433,
      "grad_norm": 0.6921104192733765,
      "learning_rate": 7.526552690804281e-05,
      "loss": 0.7849,
      "step": 12530
    },
    {
      "epoch": 0.4951237809452363,
      "grad_norm": 0.5255240201950073,
      "learning_rate": 7.524578513049315e-05,
      "loss": 0.7751,
      "step": 12540
    },
    {
      "epoch": 0.4955186164962293,
      "grad_norm": 0.5430408716201782,
      "learning_rate": 7.52260433529435e-05,
      "loss": 0.8084,
      "step": 12550
    },
    {
      "epoch": 0.4959134520472223,
      "grad_norm": 0.6497927904129028,
      "learning_rate": 7.520630157539386e-05,
      "loss": 0.8298,
      "step": 12560
    },
    {
      "epoch": 0.4963082875982153,
      "grad_norm": 0.5467302203178406,
      "learning_rate": 7.51865597978442e-05,
      "loss": 0.7584,
      "step": 12570
    },
    {
      "epoch": 0.49670312314920834,
      "grad_norm": 0.7148890495300293,
      "learning_rate": 7.516681802029455e-05,
      "loss": 0.8125,
      "step": 12580
    },
    {
      "epoch": 0.49709795870020135,
      "grad_norm": 0.6957055330276489,
      "learning_rate": 7.51470762427449e-05,
      "loss": 0.7671,
      "step": 12590
    },
    {
      "epoch": 0.49749279425119436,
      "grad_norm": 0.6557899713516235,
      "learning_rate": 7.512733446519526e-05,
      "loss": 0.8297,
      "step": 12600
    },
    {
      "epoch": 0.49788762980218737,
      "grad_norm": 0.6963269114494324,
      "learning_rate": 7.51075926876456e-05,
      "loss": 0.807,
      "step": 12610
    },
    {
      "epoch": 0.4982824653531804,
      "grad_norm": 0.8006145358085632,
      "learning_rate": 7.508785091009595e-05,
      "loss": 0.7782,
      "step": 12620
    },
    {
      "epoch": 0.4986773009041734,
      "grad_norm": 0.6630496382713318,
      "learning_rate": 7.50681091325463e-05,
      "loss": 0.7586,
      "step": 12630
    },
    {
      "epoch": 0.4990721364551664,
      "grad_norm": 0.5858656167984009,
      "learning_rate": 7.504836735499665e-05,
      "loss": 0.7235,
      "step": 12640
    },
    {
      "epoch": 0.4994669720061594,
      "grad_norm": 0.6826276183128357,
      "learning_rate": 7.5028625577447e-05,
      "loss": 0.7926,
      "step": 12650
    },
    {
      "epoch": 0.4998618075571524,
      "grad_norm": 0.7323305010795593,
      "learning_rate": 7.500888379989735e-05,
      "loss": 0.7478,
      "step": 12660
    },
    {
      "epoch": 0.5002566431081454,
      "grad_norm": 0.9626603126525879,
      "learning_rate": 7.49891420223477e-05,
      "loss": 0.7534,
      "step": 12670
    },
    {
      "epoch": 0.5006514786591385,
      "grad_norm": 0.6883087754249573,
      "learning_rate": 7.496940024479805e-05,
      "loss": 0.8224,
      "step": 12680
    },
    {
      "epoch": 0.5010463142101315,
      "grad_norm": 0.6977061033248901,
      "learning_rate": 7.49496584672484e-05,
      "loss": 0.7639,
      "step": 12690
    },
    {
      "epoch": 0.5014411497611245,
      "grad_norm": 0.6579100489616394,
      "learning_rate": 7.492991668969875e-05,
      "loss": 0.799,
      "step": 12700
    },
    {
      "epoch": 0.5018359853121175,
      "grad_norm": 0.577250599861145,
      "learning_rate": 7.49101749121491e-05,
      "loss": 0.7473,
      "step": 12710
    },
    {
      "epoch": 0.5022308208631105,
      "grad_norm": 0.7450656294822693,
      "learning_rate": 7.489043313459944e-05,
      "loss": 0.7636,
      "step": 12720
    },
    {
      "epoch": 0.5026256564141035,
      "grad_norm": 0.754802405834198,
      "learning_rate": 7.48706913570498e-05,
      "loss": 0.8137,
      "step": 12730
    },
    {
      "epoch": 0.5030204919650966,
      "grad_norm": 0.6281603574752808,
      "learning_rate": 7.485094957950014e-05,
      "loss": 0.7572,
      "step": 12740
    },
    {
      "epoch": 0.5034153275160895,
      "grad_norm": 0.71197909116745,
      "learning_rate": 7.483120780195048e-05,
      "loss": 0.8243,
      "step": 12750
    },
    {
      "epoch": 0.5038101630670826,
      "grad_norm": 0.7216219902038574,
      "learning_rate": 7.481146602440084e-05,
      "loss": 0.7955,
      "step": 12760
    },
    {
      "epoch": 0.5042049986180756,
      "grad_norm": 0.7211502194404602,
      "learning_rate": 7.47917242468512e-05,
      "loss": 0.7881,
      "step": 12770
    },
    {
      "epoch": 0.5045998341690686,
      "grad_norm": 0.6265305876731873,
      "learning_rate": 7.477198246930154e-05,
      "loss": 0.7851,
      "step": 12780
    },
    {
      "epoch": 0.5049946697200616,
      "grad_norm": 0.6742202043533325,
      "learning_rate": 7.475224069175188e-05,
      "loss": 0.7536,
      "step": 12790
    },
    {
      "epoch": 0.5053895052710546,
      "grad_norm": 0.8650020360946655,
      "learning_rate": 7.473249891420224e-05,
      "loss": 0.7737,
      "step": 12800
    },
    {
      "epoch": 0.5057843408220476,
      "grad_norm": 0.6141467094421387,
      "learning_rate": 7.47127571366526e-05,
      "loss": 0.7578,
      "step": 12810
    },
    {
      "epoch": 0.5061791763730407,
      "grad_norm": 0.7163729667663574,
      "learning_rate": 7.469301535910293e-05,
      "loss": 0.8131,
      "step": 12820
    },
    {
      "epoch": 0.5065740119240336,
      "grad_norm": 0.7375092506408691,
      "learning_rate": 7.467327358155328e-05,
      "loss": 0.7736,
      "step": 12830
    },
    {
      "epoch": 0.5069688474750267,
      "grad_norm": 0.6237730383872986,
      "learning_rate": 7.465353180400364e-05,
      "loss": 0.8007,
      "step": 12840
    },
    {
      "epoch": 0.5073636830260196,
      "grad_norm": 0.6124526262283325,
      "learning_rate": 7.463379002645398e-05,
      "loss": 0.7815,
      "step": 12850
    },
    {
      "epoch": 0.5077585185770127,
      "grad_norm": 0.6296981573104858,
      "learning_rate": 7.461404824890433e-05,
      "loss": 0.7985,
      "step": 12860
    },
    {
      "epoch": 0.5081533541280057,
      "grad_norm": 0.6749764084815979,
      "learning_rate": 7.459430647135468e-05,
      "loss": 0.748,
      "step": 12870
    },
    {
      "epoch": 0.5085481896789987,
      "grad_norm": 0.5499749183654785,
      "learning_rate": 7.457456469380504e-05,
      "loss": 0.8056,
      "step": 12880
    },
    {
      "epoch": 0.5089430252299917,
      "grad_norm": 0.7193449139595032,
      "learning_rate": 7.455482291625538e-05,
      "loss": 0.7763,
      "step": 12890
    },
    {
      "epoch": 0.5093378607809848,
      "grad_norm": 0.6321723461151123,
      "learning_rate": 7.453508113870573e-05,
      "loss": 0.7844,
      "step": 12900
    },
    {
      "epoch": 0.5097326963319777,
      "grad_norm": 0.8824482560157776,
      "learning_rate": 7.451533936115608e-05,
      "loss": 0.8307,
      "step": 12910
    },
    {
      "epoch": 0.5101275318829708,
      "grad_norm": 0.9199075698852539,
      "learning_rate": 7.449559758360643e-05,
      "loss": 0.7781,
      "step": 12920
    },
    {
      "epoch": 0.5105223674339637,
      "grad_norm": 0.7031727433204651,
      "learning_rate": 7.447585580605678e-05,
      "loss": 0.7556,
      "step": 12930
    },
    {
      "epoch": 0.5109172029849568,
      "grad_norm": 0.6024278402328491,
      "learning_rate": 7.445611402850713e-05,
      "loss": 0.7994,
      "step": 12940
    },
    {
      "epoch": 0.5113120385359498,
      "grad_norm": 0.6138192415237427,
      "learning_rate": 7.443637225095748e-05,
      "loss": 0.7669,
      "step": 12950
    },
    {
      "epoch": 0.5117068740869428,
      "grad_norm": 0.731285810470581,
      "learning_rate": 7.441663047340783e-05,
      "loss": 0.7812,
      "step": 12960
    },
    {
      "epoch": 0.5121017096379358,
      "grad_norm": 0.5669876933097839,
      "learning_rate": 7.439688869585818e-05,
      "loss": 0.7736,
      "step": 12970
    },
    {
      "epoch": 0.5124965451889288,
      "grad_norm": 0.60308438539505,
      "learning_rate": 7.437714691830853e-05,
      "loss": 0.744,
      "step": 12980
    },
    {
      "epoch": 0.5128913807399218,
      "grad_norm": 0.5294771194458008,
      "learning_rate": 7.435740514075888e-05,
      "loss": 0.7488,
      "step": 12990
    },
    {
      "epoch": 0.5132862162909149,
      "grad_norm": 0.5628697872161865,
      "learning_rate": 7.433766336320923e-05,
      "loss": 0.7954,
      "step": 13000
    },
    {
      "epoch": 0.5132862162909149,
      "eval_loss": 0.7955359220504761,
      "eval_runtime": 1215.0448,
      "eval_samples_per_second": 9.265,
      "eval_steps_per_second": 9.265,
      "step": 13000
    },
    {
      "epoch": 0.5136810518419078,
      "grad_norm": 0.9551022052764893,
      "learning_rate": 7.431792158565958e-05,
      "loss": 0.7783,
      "step": 13010
    },
    {
      "epoch": 0.5140758873929009,
      "grad_norm": 0.7871876955032349,
      "learning_rate": 7.429817980810993e-05,
      "loss": 0.769,
      "step": 13020
    },
    {
      "epoch": 0.5144707229438938,
      "grad_norm": 0.7280641198158264,
      "learning_rate": 7.427843803056027e-05,
      "loss": 0.7167,
      "step": 13030
    },
    {
      "epoch": 0.5148655584948869,
      "grad_norm": 0.7100977301597595,
      "learning_rate": 7.425869625301063e-05,
      "loss": 0.803,
      "step": 13040
    },
    {
      "epoch": 0.5152603940458799,
      "grad_norm": 0.5931075811386108,
      "learning_rate": 7.423895447546098e-05,
      "loss": 0.7686,
      "step": 13050
    },
    {
      "epoch": 0.5156552295968729,
      "grad_norm": 0.7398943901062012,
      "learning_rate": 7.421921269791133e-05,
      "loss": 0.7578,
      "step": 13060
    },
    {
      "epoch": 0.5160500651478659,
      "grad_norm": 0.6550546884536743,
      "learning_rate": 7.419947092036166e-05,
      "loss": 0.7529,
      "step": 13070
    },
    {
      "epoch": 0.516444900698859,
      "grad_norm": 0.709782063961029,
      "learning_rate": 7.417972914281203e-05,
      "loss": 0.7853,
      "step": 13080
    },
    {
      "epoch": 0.5168397362498519,
      "grad_norm": 0.5918842554092407,
      "learning_rate": 7.415998736526238e-05,
      "loss": 0.7934,
      "step": 13090
    },
    {
      "epoch": 0.517234571800845,
      "grad_norm": 0.6771364808082581,
      "learning_rate": 7.414024558771271e-05,
      "loss": 0.7685,
      "step": 13100
    },
    {
      "epoch": 0.5176294073518379,
      "grad_norm": 0.687734067440033,
      "learning_rate": 7.412050381016306e-05,
      "loss": 0.8079,
      "step": 13110
    },
    {
      "epoch": 0.518024242902831,
      "grad_norm": 0.5731626152992249,
      "learning_rate": 7.410076203261343e-05,
      "loss": 0.7586,
      "step": 13120
    },
    {
      "epoch": 0.518419078453824,
      "grad_norm": 0.9109025597572327,
      "learning_rate": 7.408102025506378e-05,
      "loss": 0.7217,
      "step": 13130
    },
    {
      "epoch": 0.518813914004817,
      "grad_norm": 0.598689079284668,
      "learning_rate": 7.406127847751411e-05,
      "loss": 0.7993,
      "step": 13140
    },
    {
      "epoch": 0.51920874955581,
      "grad_norm": 0.6768746972084045,
      "learning_rate": 7.404153669996446e-05,
      "loss": 0.7455,
      "step": 13150
    },
    {
      "epoch": 0.519603585106803,
      "grad_norm": 0.5962754487991333,
      "learning_rate": 7.402179492241483e-05,
      "loss": 0.7573,
      "step": 13160
    },
    {
      "epoch": 0.519998420657796,
      "grad_norm": 0.6996914744377136,
      "learning_rate": 7.400205314486516e-05,
      "loss": 0.7356,
      "step": 13170
    },
    {
      "epoch": 0.5203932562087891,
      "grad_norm": 0.663115918636322,
      "learning_rate": 7.398231136731551e-05,
      "loss": 0.7736,
      "step": 13180
    },
    {
      "epoch": 0.520788091759782,
      "grad_norm": 0.5864531993865967,
      "learning_rate": 7.396256958976586e-05,
      "loss": 0.7745,
      "step": 13190
    },
    {
      "epoch": 0.5211829273107751,
      "grad_norm": 0.7106543779373169,
      "learning_rate": 7.394282781221621e-05,
      "loss": 0.7623,
      "step": 13200
    },
    {
      "epoch": 0.521577762861768,
      "grad_norm": 0.5678426027297974,
      "learning_rate": 7.392308603466656e-05,
      "loss": 0.7542,
      "step": 13210
    },
    {
      "epoch": 0.5219725984127611,
      "grad_norm": 0.5455649495124817,
      "learning_rate": 7.390334425711691e-05,
      "loss": 0.7372,
      "step": 13220
    },
    {
      "epoch": 0.5223674339637541,
      "grad_norm": 0.5996615886688232,
      "learning_rate": 7.388360247956726e-05,
      "loss": 0.7206,
      "step": 13230
    },
    {
      "epoch": 0.5227622695147471,
      "grad_norm": 0.5832385420799255,
      "learning_rate": 7.386386070201761e-05,
      "loss": 0.7843,
      "step": 13240
    },
    {
      "epoch": 0.5231571050657401,
      "grad_norm": 0.6970958113670349,
      "learning_rate": 7.384411892446796e-05,
      "loss": 0.7552,
      "step": 13250
    },
    {
      "epoch": 0.5235519406167332,
      "grad_norm": 0.7596831321716309,
      "learning_rate": 7.382437714691831e-05,
      "loss": 0.7893,
      "step": 13260
    },
    {
      "epoch": 0.5239467761677261,
      "grad_norm": 0.7622919678688049,
      "learning_rate": 7.380463536936866e-05,
      "loss": 0.7751,
      "step": 13270
    },
    {
      "epoch": 0.5243416117187192,
      "grad_norm": 0.7208912372589111,
      "learning_rate": 7.378489359181901e-05,
      "loss": 0.7687,
      "step": 13280
    },
    {
      "epoch": 0.5247364472697121,
      "grad_norm": 0.7350472807884216,
      "learning_rate": 7.376515181426936e-05,
      "loss": 0.7939,
      "step": 13290
    },
    {
      "epoch": 0.5251312828207052,
      "grad_norm": 0.6275180578231812,
      "learning_rate": 7.374541003671971e-05,
      "loss": 0.7484,
      "step": 13300
    },
    {
      "epoch": 0.5255261183716982,
      "grad_norm": 0.4537164568901062,
      "learning_rate": 7.372566825917005e-05,
      "loss": 0.7548,
      "step": 13310
    },
    {
      "epoch": 0.5259209539226912,
      "grad_norm": 0.6810120344161987,
      "learning_rate": 7.370592648162041e-05,
      "loss": 0.8255,
      "step": 13320
    },
    {
      "epoch": 0.5263157894736842,
      "grad_norm": 0.6751253008842468,
      "learning_rate": 7.368618470407076e-05,
      "loss": 0.7544,
      "step": 13330
    },
    {
      "epoch": 0.5267106250246772,
      "grad_norm": 0.6507323384284973,
      "learning_rate": 7.366644292652111e-05,
      "loss": 0.7378,
      "step": 13340
    },
    {
      "epoch": 0.5271054605756702,
      "grad_norm": 0.6308696269989014,
      "learning_rate": 7.364670114897145e-05,
      "loss": 0.7689,
      "step": 13350
    },
    {
      "epoch": 0.5275002961266633,
      "grad_norm": 0.9915522336959839,
      "learning_rate": 7.362695937142181e-05,
      "loss": 0.75,
      "step": 13360
    },
    {
      "epoch": 0.5278951316776562,
      "grad_norm": 0.6082946062088013,
      "learning_rate": 7.360721759387216e-05,
      "loss": 0.7619,
      "step": 13370
    },
    {
      "epoch": 0.5282899672286493,
      "grad_norm": 0.6330448985099792,
      "learning_rate": 7.35874758163225e-05,
      "loss": 0.7489,
      "step": 13380
    },
    {
      "epoch": 0.5286848027796422,
      "grad_norm": 0.6209728717803955,
      "learning_rate": 7.356773403877285e-05,
      "loss": 0.7547,
      "step": 13390
    },
    {
      "epoch": 0.5290796383306353,
      "grad_norm": 0.8085650205612183,
      "learning_rate": 7.354799226122321e-05,
      "loss": 0.7633,
      "step": 13400
    },
    {
      "epoch": 0.5294744738816283,
      "grad_norm": 0.6015084981918335,
      "learning_rate": 7.352825048367356e-05,
      "loss": 0.7662,
      "step": 13410
    },
    {
      "epoch": 0.5298693094326213,
      "grad_norm": 0.5771582126617432,
      "learning_rate": 7.35085087061239e-05,
      "loss": 0.7554,
      "step": 13420
    },
    {
      "epoch": 0.5302641449836143,
      "grad_norm": 0.634823203086853,
      "learning_rate": 7.348876692857425e-05,
      "loss": 0.7745,
      "step": 13430
    },
    {
      "epoch": 0.5306589805346074,
      "grad_norm": 0.5511689186096191,
      "learning_rate": 7.346902515102461e-05,
      "loss": 0.7556,
      "step": 13440
    },
    {
      "epoch": 0.5310538160856003,
      "grad_norm": 0.6507060527801514,
      "learning_rate": 7.344928337347495e-05,
      "loss": 0.7509,
      "step": 13450
    },
    {
      "epoch": 0.5314486516365934,
      "grad_norm": 0.6349825859069824,
      "learning_rate": 7.34295415959253e-05,
      "loss": 0.7998,
      "step": 13460
    },
    {
      "epoch": 0.5318434871875863,
      "grad_norm": 0.622866690158844,
      "learning_rate": 7.340979981837565e-05,
      "loss": 0.7615,
      "step": 13470
    },
    {
      "epoch": 0.5322383227385794,
      "grad_norm": 0.745326042175293,
      "learning_rate": 7.3390058040826e-05,
      "loss": 0.7892,
      "step": 13480
    },
    {
      "epoch": 0.5326331582895724,
      "grad_norm": 0.753853976726532,
      "learning_rate": 7.337031626327635e-05,
      "loss": 0.7801,
      "step": 13490
    },
    {
      "epoch": 0.5330279938405654,
      "grad_norm": 0.6509620547294617,
      "learning_rate": 7.33505744857267e-05,
      "loss": 0.7862,
      "step": 13500
    },
    {
      "epoch": 0.5334228293915584,
      "grad_norm": 0.6786113381385803,
      "learning_rate": 7.333083270817705e-05,
      "loss": 0.7851,
      "step": 13510
    },
    {
      "epoch": 0.5338176649425515,
      "grad_norm": 0.7568014860153198,
      "learning_rate": 7.33110909306274e-05,
      "loss": 0.7741,
      "step": 13520
    },
    {
      "epoch": 0.5342125004935444,
      "grad_norm": 0.6065242290496826,
      "learning_rate": 7.329134915307775e-05,
      "loss": 0.7611,
      "step": 13530
    },
    {
      "epoch": 0.5346073360445375,
      "grad_norm": 0.8368576169013977,
      "learning_rate": 7.32716073755281e-05,
      "loss": 0.7964,
      "step": 13540
    },
    {
      "epoch": 0.5350021715955304,
      "grad_norm": 0.8165404200553894,
      "learning_rate": 7.325186559797845e-05,
      "loss": 0.7983,
      "step": 13550
    },
    {
      "epoch": 0.5353970071465235,
      "grad_norm": 0.6715502142906189,
      "learning_rate": 7.32321238204288e-05,
      "loss": 0.7728,
      "step": 13560
    },
    {
      "epoch": 0.5357918426975165,
      "grad_norm": 0.5659531354904175,
      "learning_rate": 7.321238204287915e-05,
      "loss": 0.7786,
      "step": 13570
    },
    {
      "epoch": 0.5361866782485095,
      "grad_norm": 0.6866711974143982,
      "learning_rate": 7.31926402653295e-05,
      "loss": 0.7862,
      "step": 13580
    },
    {
      "epoch": 0.5365815137995025,
      "grad_norm": 0.5821740031242371,
      "learning_rate": 7.317289848777983e-05,
      "loss": 0.8031,
      "step": 13590
    },
    {
      "epoch": 0.5369763493504955,
      "grad_norm": 0.7656296491622925,
      "learning_rate": 7.31531567102302e-05,
      "loss": 0.7566,
      "step": 13600
    },
    {
      "epoch": 0.5373711849014885,
      "grad_norm": 0.6671348810195923,
      "learning_rate": 7.313341493268055e-05,
      "loss": 0.7525,
      "step": 13610
    },
    {
      "epoch": 0.5377660204524816,
      "grad_norm": 0.711357057094574,
      "learning_rate": 7.31136731551309e-05,
      "loss": 0.7504,
      "step": 13620
    },
    {
      "epoch": 0.5381608560034745,
      "grad_norm": 0.6252645254135132,
      "learning_rate": 7.309393137758123e-05,
      "loss": 0.8288,
      "step": 13630
    },
    {
      "epoch": 0.5385556915544676,
      "grad_norm": 0.5441193580627441,
      "learning_rate": 7.30741896000316e-05,
      "loss": 0.7306,
      "step": 13640
    },
    {
      "epoch": 0.5389505271054605,
      "grad_norm": 0.5740919709205627,
      "learning_rate": 7.305444782248195e-05,
      "loss": 0.8013,
      "step": 13650
    },
    {
      "epoch": 0.5393453626564536,
      "grad_norm": 0.6121788024902344,
      "learning_rate": 7.303470604493228e-05,
      "loss": 0.7734,
      "step": 13660
    },
    {
      "epoch": 0.5397401982074466,
      "grad_norm": 0.5166116952896118,
      "learning_rate": 7.301496426738263e-05,
      "loss": 0.785,
      "step": 13670
    },
    {
      "epoch": 0.5401350337584396,
      "grad_norm": 0.6405220627784729,
      "learning_rate": 7.2995222489833e-05,
      "loss": 0.7525,
      "step": 13680
    },
    {
      "epoch": 0.5405298693094326,
      "grad_norm": 0.7627921104431152,
      "learning_rate": 7.297548071228335e-05,
      "loss": 0.7772,
      "step": 13690
    },
    {
      "epoch": 0.5409247048604257,
      "grad_norm": 0.6356509327888489,
      "learning_rate": 7.295573893473368e-05,
      "loss": 0.8118,
      "step": 13700
    },
    {
      "epoch": 0.5413195404114186,
      "grad_norm": 0.6549410223960876,
      "learning_rate": 7.293599715718403e-05,
      "loss": 0.8113,
      "step": 13710
    },
    {
      "epoch": 0.5417143759624117,
      "grad_norm": 0.572701632976532,
      "learning_rate": 7.29162553796344e-05,
      "loss": 0.7751,
      "step": 13720
    },
    {
      "epoch": 0.5421092115134046,
      "grad_norm": 0.6931018829345703,
      "learning_rate": 7.289651360208473e-05,
      "loss": 0.774,
      "step": 13730
    },
    {
      "epoch": 0.5425040470643977,
      "grad_norm": 0.7711584568023682,
      "learning_rate": 7.287677182453508e-05,
      "loss": 0.7828,
      "step": 13740
    },
    {
      "epoch": 0.5428988826153907,
      "grad_norm": 0.641457736492157,
      "learning_rate": 7.285703004698543e-05,
      "loss": 0.776,
      "step": 13750
    },
    {
      "epoch": 0.5432937181663837,
      "grad_norm": 0.7335061430931091,
      "learning_rate": 7.283728826943578e-05,
      "loss": 0.7717,
      "step": 13760
    },
    {
      "epoch": 0.5436885537173767,
      "grad_norm": 0.679757833480835,
      "learning_rate": 7.281754649188613e-05,
      "loss": 0.7812,
      "step": 13770
    },
    {
      "epoch": 0.5440833892683697,
      "grad_norm": 0.8460711240768433,
      "learning_rate": 7.279780471433648e-05,
      "loss": 0.8206,
      "step": 13780
    },
    {
      "epoch": 0.5444782248193627,
      "grad_norm": 0.7468332052230835,
      "learning_rate": 7.277806293678683e-05,
      "loss": 0.8361,
      "step": 13790
    },
    {
      "epoch": 0.5448730603703558,
      "grad_norm": 0.8778299689292908,
      "learning_rate": 7.275832115923718e-05,
      "loss": 0.754,
      "step": 13800
    },
    {
      "epoch": 0.5452678959213487,
      "grad_norm": 0.8273650407791138,
      "learning_rate": 7.273857938168753e-05,
      "loss": 0.8127,
      "step": 13810
    },
    {
      "epoch": 0.5456627314723418,
      "grad_norm": 0.5813717246055603,
      "learning_rate": 7.271883760413788e-05,
      "loss": 0.787,
      "step": 13820
    },
    {
      "epoch": 0.5460575670233347,
      "grad_norm": 0.5519154071807861,
      "learning_rate": 7.269909582658823e-05,
      "loss": 0.7427,
      "step": 13830
    },
    {
      "epoch": 0.5464524025743278,
      "grad_norm": 0.7366795539855957,
      "learning_rate": 7.267935404903858e-05,
      "loss": 0.8032,
      "step": 13840
    },
    {
      "epoch": 0.5468472381253208,
      "grad_norm": 0.7372687458992004,
      "learning_rate": 7.265961227148893e-05,
      "loss": 0.7947,
      "step": 13850
    },
    {
      "epoch": 0.5472420736763138,
      "grad_norm": 0.6373666524887085,
      "learning_rate": 7.263987049393928e-05,
      "loss": 0.7891,
      "step": 13860
    },
    {
      "epoch": 0.5476369092273068,
      "grad_norm": 0.6657714247703552,
      "learning_rate": 7.262012871638962e-05,
      "loss": 0.7606,
      "step": 13870
    },
    {
      "epoch": 0.5480317447782999,
      "grad_norm": 0.6291369795799255,
      "learning_rate": 7.260038693883998e-05,
      "loss": 0.7548,
      "step": 13880
    },
    {
      "epoch": 0.5484265803292928,
      "grad_norm": 0.6320908069610596,
      "learning_rate": 7.258064516129033e-05,
      "loss": 0.7585,
      "step": 13890
    },
    {
      "epoch": 0.5488214158802859,
      "grad_norm": 0.71646648645401,
      "learning_rate": 7.256090338374068e-05,
      "loss": 0.7689,
      "step": 13900
    },
    {
      "epoch": 0.5492162514312788,
      "grad_norm": 0.7555578351020813,
      "learning_rate": 7.254116160619102e-05,
      "loss": 0.8028,
      "step": 13910
    },
    {
      "epoch": 0.5496110869822719,
      "grad_norm": 0.9589332938194275,
      "learning_rate": 7.252141982864138e-05,
      "loss": 0.748,
      "step": 13920
    },
    {
      "epoch": 0.5500059225332649,
      "grad_norm": 0.5806211233139038,
      "learning_rate": 7.250167805109173e-05,
      "loss": 0.7942,
      "step": 13930
    },
    {
      "epoch": 0.5504007580842579,
      "grad_norm": 0.7134691476821899,
      "learning_rate": 7.248193627354207e-05,
      "loss": 0.8135,
      "step": 13940
    },
    {
      "epoch": 0.5507955936352509,
      "grad_norm": 0.6419457793235779,
      "learning_rate": 7.246219449599242e-05,
      "loss": 0.7799,
      "step": 13950
    },
    {
      "epoch": 0.551190429186244,
      "grad_norm": 0.7459760904312134,
      "learning_rate": 7.244245271844278e-05,
      "loss": 0.7633,
      "step": 13960
    },
    {
      "epoch": 0.5515852647372369,
      "grad_norm": 0.7295992970466614,
      "learning_rate": 7.242271094089313e-05,
      "loss": 0.7171,
      "step": 13970
    },
    {
      "epoch": 0.55198010028823,
      "grad_norm": 0.7894341945648193,
      "learning_rate": 7.240296916334347e-05,
      "loss": 0.7893,
      "step": 13980
    },
    {
      "epoch": 0.5523749358392229,
      "grad_norm": 0.6388062834739685,
      "learning_rate": 7.238322738579382e-05,
      "loss": 0.7705,
      "step": 13990
    },
    {
      "epoch": 0.552769771390216,
      "grad_norm": 0.5422223806381226,
      "learning_rate": 7.236348560824418e-05,
      "loss": 0.7678,
      "step": 14000
    },
    {
      "epoch": 0.552769771390216,
      "eval_loss": 0.7948129177093506,
      "eval_runtime": 1214.8779,
      "eval_samples_per_second": 9.266,
      "eval_steps_per_second": 9.266,
      "step": 14000
    },
    {
      "epoch": 0.553164606941209,
      "grad_norm": 0.7596111297607422,
      "learning_rate": 7.234374383069452e-05,
      "loss": 0.7538,
      "step": 14010
    },
    {
      "epoch": 0.553559442492202,
      "grad_norm": 0.6135753393173218,
      "learning_rate": 7.232400205314487e-05,
      "loss": 0.8356,
      "step": 14020
    },
    {
      "epoch": 0.553954278043195,
      "grad_norm": 0.6341413259506226,
      "learning_rate": 7.230426027559522e-05,
      "loss": 0.7832,
      "step": 14030
    },
    {
      "epoch": 0.554349113594188,
      "grad_norm": 0.6083695888519287,
      "learning_rate": 7.228451849804557e-05,
      "loss": 0.774,
      "step": 14040
    },
    {
      "epoch": 0.554743949145181,
      "grad_norm": 0.712217390537262,
      "learning_rate": 7.226477672049592e-05,
      "loss": 0.7689,
      "step": 14050
    },
    {
      "epoch": 0.5551387846961741,
      "grad_norm": 0.6048699617385864,
      "learning_rate": 7.224503494294627e-05,
      "loss": 0.7817,
      "step": 14060
    },
    {
      "epoch": 0.555533620247167,
      "grad_norm": 0.6198461651802063,
      "learning_rate": 7.222529316539662e-05,
      "loss": 0.757,
      "step": 14070
    },
    {
      "epoch": 0.5559284557981601,
      "grad_norm": 0.5835891962051392,
      "learning_rate": 7.220555138784697e-05,
      "loss": 0.8146,
      "step": 14080
    },
    {
      "epoch": 0.556323291349153,
      "grad_norm": 0.7053143382072449,
      "learning_rate": 7.218580961029732e-05,
      "loss": 0.7539,
      "step": 14090
    },
    {
      "epoch": 0.5567181269001461,
      "grad_norm": 0.8739808201789856,
      "learning_rate": 7.216606783274767e-05,
      "loss": 0.7546,
      "step": 14100
    },
    {
      "epoch": 0.5571129624511391,
      "grad_norm": 0.7880245447158813,
      "learning_rate": 7.214632605519802e-05,
      "loss": 0.7848,
      "step": 14110
    },
    {
      "epoch": 0.5575077980021321,
      "grad_norm": 0.7356258034706116,
      "learning_rate": 7.212658427764837e-05,
      "loss": 0.789,
      "step": 14120
    },
    {
      "epoch": 0.5579026335531251,
      "grad_norm": 0.6490318775177002,
      "learning_rate": 7.210684250009872e-05,
      "loss": 0.8381,
      "step": 14130
    },
    {
      "epoch": 0.5582974691041181,
      "grad_norm": 0.5564206838607788,
      "learning_rate": 7.208710072254906e-05,
      "loss": 0.7533,
      "step": 14140
    },
    {
      "epoch": 0.5586923046551111,
      "grad_norm": 0.6046084761619568,
      "learning_rate": 7.20673589449994e-05,
      "loss": 0.7886,
      "step": 14150
    },
    {
      "epoch": 0.5590871402061042,
      "grad_norm": 0.7117812037467957,
      "learning_rate": 7.204761716744976e-05,
      "loss": 0.7528,
      "step": 14160
    },
    {
      "epoch": 0.5594819757570971,
      "grad_norm": 0.6262540221214294,
      "learning_rate": 7.202787538990011e-05,
      "loss": 0.7701,
      "step": 14170
    },
    {
      "epoch": 0.5598768113080902,
      "grad_norm": 0.64516282081604,
      "learning_rate": 7.200813361235046e-05,
      "loss": 0.8159,
      "step": 14180
    },
    {
      "epoch": 0.5602716468590832,
      "grad_norm": 0.6938239932060242,
      "learning_rate": 7.19883918348008e-05,
      "loss": 0.7905,
      "step": 14190
    },
    {
      "epoch": 0.5606664824100762,
      "grad_norm": 0.6857880353927612,
      "learning_rate": 7.196865005725116e-05,
      "loss": 0.7452,
      "step": 14200
    },
    {
      "epoch": 0.5610613179610692,
      "grad_norm": 0.7705075144767761,
      "learning_rate": 7.194890827970151e-05,
      "loss": 0.7317,
      "step": 14210
    },
    {
      "epoch": 0.5614561535120622,
      "grad_norm": 0.7003137469291687,
      "learning_rate": 7.192916650215185e-05,
      "loss": 0.7937,
      "step": 14220
    },
    {
      "epoch": 0.5618509890630552,
      "grad_norm": 0.6329069137573242,
      "learning_rate": 7.19094247246022e-05,
      "loss": 0.7374,
      "step": 14230
    },
    {
      "epoch": 0.5622458246140483,
      "grad_norm": 0.6408053636550903,
      "learning_rate": 7.188968294705256e-05,
      "loss": 0.7532,
      "step": 14240
    },
    {
      "epoch": 0.5626406601650412,
      "grad_norm": 0.7452744245529175,
      "learning_rate": 7.186994116950291e-05,
      "loss": 0.7269,
      "step": 14250
    },
    {
      "epoch": 0.5630354957160343,
      "grad_norm": 0.5069535374641418,
      "learning_rate": 7.185019939195325e-05,
      "loss": 0.8119,
      "step": 14260
    },
    {
      "epoch": 0.5634303312670272,
      "grad_norm": 0.7287333607673645,
      "learning_rate": 7.18304576144036e-05,
      "loss": 0.7491,
      "step": 14270
    },
    {
      "epoch": 0.5638251668180203,
      "grad_norm": 0.5919630527496338,
      "learning_rate": 7.181071583685396e-05,
      "loss": 0.7657,
      "step": 14280
    },
    {
      "epoch": 0.5642200023690133,
      "grad_norm": 0.47259870171546936,
      "learning_rate": 7.17909740593043e-05,
      "loss": 0.7641,
      "step": 14290
    },
    {
      "epoch": 0.5646148379200063,
      "grad_norm": 0.6691468358039856,
      "learning_rate": 7.177123228175465e-05,
      "loss": 0.7573,
      "step": 14300
    },
    {
      "epoch": 0.5650096734709993,
      "grad_norm": 0.674299955368042,
      "learning_rate": 7.1751490504205e-05,
      "loss": 0.7982,
      "step": 14310
    },
    {
      "epoch": 0.5654045090219924,
      "grad_norm": 0.6759563088417053,
      "learning_rate": 7.173174872665536e-05,
      "loss": 0.7885,
      "step": 14320
    },
    {
      "epoch": 0.5657993445729853,
      "grad_norm": 0.6466256380081177,
      "learning_rate": 7.17120069491057e-05,
      "loss": 0.7719,
      "step": 14330
    },
    {
      "epoch": 0.5661941801239784,
      "grad_norm": 0.7833012938499451,
      "learning_rate": 7.169226517155605e-05,
      "loss": 0.7939,
      "step": 14340
    },
    {
      "epoch": 0.5665890156749713,
      "grad_norm": 0.6028810739517212,
      "learning_rate": 7.16725233940064e-05,
      "loss": 0.7802,
      "step": 14350
    },
    {
      "epoch": 0.5669838512259644,
      "grad_norm": 0.6006585359573364,
      "learning_rate": 7.165278161645675e-05,
      "loss": 0.7748,
      "step": 14360
    },
    {
      "epoch": 0.5673786867769574,
      "grad_norm": 0.7222571969032288,
      "learning_rate": 7.16330398389071e-05,
      "loss": 0.7969,
      "step": 14370
    },
    {
      "epoch": 0.5677735223279504,
      "grad_norm": 0.6227672100067139,
      "learning_rate": 7.161329806135745e-05,
      "loss": 0.7748,
      "step": 14380
    },
    {
      "epoch": 0.5681683578789434,
      "grad_norm": 0.9677215218544006,
      "learning_rate": 7.15935562838078e-05,
      "loss": 0.8006,
      "step": 14390
    },
    {
      "epoch": 0.5685631934299364,
      "grad_norm": 0.8604849576950073,
      "learning_rate": 7.157381450625815e-05,
      "loss": 0.7703,
      "step": 14400
    },
    {
      "epoch": 0.5689580289809294,
      "grad_norm": 0.7032308578491211,
      "learning_rate": 7.15540727287085e-05,
      "loss": 0.7675,
      "step": 14410
    },
    {
      "epoch": 0.5693528645319225,
      "grad_norm": 0.7082411050796509,
      "learning_rate": 7.153433095115885e-05,
      "loss": 0.7743,
      "step": 14420
    },
    {
      "epoch": 0.5697477000829154,
      "grad_norm": 0.6839239001274109,
      "learning_rate": 7.15145891736092e-05,
      "loss": 0.8064,
      "step": 14430
    },
    {
      "epoch": 0.5701425356339085,
      "grad_norm": 0.7470941543579102,
      "learning_rate": 7.149484739605955e-05,
      "loss": 0.7802,
      "step": 14440
    },
    {
      "epoch": 0.5705373711849014,
      "grad_norm": 0.49676936864852905,
      "learning_rate": 7.14751056185099e-05,
      "loss": 0.7305,
      "step": 14450
    },
    {
      "epoch": 0.5709322067358945,
      "grad_norm": 0.9511357545852661,
      "learning_rate": 7.145536384096025e-05,
      "loss": 0.7783,
      "step": 14460
    },
    {
      "epoch": 0.5713270422868875,
      "grad_norm": 0.7058945298194885,
      "learning_rate": 7.143562206341058e-05,
      "loss": 0.8353,
      "step": 14470
    },
    {
      "epoch": 0.5717218778378805,
      "grad_norm": 0.6164979934692383,
      "learning_rate": 7.141588028586095e-05,
      "loss": 0.847,
      "step": 14480
    },
    {
      "epoch": 0.5721167133888735,
      "grad_norm": 0.5220000743865967,
      "learning_rate": 7.13961385083113e-05,
      "loss": 0.766,
      "step": 14490
    },
    {
      "epoch": 0.5725115489398666,
      "grad_norm": 0.6377025842666626,
      "learning_rate": 7.137639673076163e-05,
      "loss": 0.7464,
      "step": 14500
    },
    {
      "epoch": 0.5729063844908595,
      "grad_norm": 0.7515997290611267,
      "learning_rate": 7.135665495321198e-05,
      "loss": 0.756,
      "step": 14510
    },
    {
      "epoch": 0.5733012200418526,
      "grad_norm": 0.6282485723495483,
      "learning_rate": 7.133691317566235e-05,
      "loss": 0.7625,
      "step": 14520
    },
    {
      "epoch": 0.5736960555928455,
      "grad_norm": 0.7087286114692688,
      "learning_rate": 7.13171713981127e-05,
      "loss": 0.7626,
      "step": 14530
    },
    {
      "epoch": 0.5740908911438386,
      "grad_norm": 0.7153228521347046,
      "learning_rate": 7.129742962056303e-05,
      "loss": 0.7905,
      "step": 14540
    },
    {
      "epoch": 0.5744857266948316,
      "grad_norm": 0.5892878770828247,
      "learning_rate": 7.127768784301338e-05,
      "loss": 0.8248,
      "step": 14550
    },
    {
      "epoch": 0.5748805622458246,
      "grad_norm": 0.5094887614250183,
      "learning_rate": 7.125794606546375e-05,
      "loss": 0.7734,
      "step": 14560
    },
    {
      "epoch": 0.5752753977968176,
      "grad_norm": 0.5365606546401978,
      "learning_rate": 7.123820428791408e-05,
      "loss": 0.7319,
      "step": 14570
    },
    {
      "epoch": 0.5756702333478106,
      "grad_norm": 0.582985520362854,
      "learning_rate": 7.121846251036443e-05,
      "loss": 0.7257,
      "step": 14580
    },
    {
      "epoch": 0.5760650688988036,
      "grad_norm": 0.5448906421661377,
      "learning_rate": 7.119872073281478e-05,
      "loss": 0.7853,
      "step": 14590
    },
    {
      "epoch": 0.5764599044497967,
      "grad_norm": 0.5308764576911926,
      "learning_rate": 7.117897895526515e-05,
      "loss": 0.7747,
      "step": 14600
    },
    {
      "epoch": 0.5768547400007896,
      "grad_norm": 0.6589566469192505,
      "learning_rate": 7.115923717771548e-05,
      "loss": 0.7725,
      "step": 14610
    },
    {
      "epoch": 0.5772495755517827,
      "grad_norm": 0.4864540994167328,
      "learning_rate": 7.113949540016583e-05,
      "loss": 0.7616,
      "step": 14620
    },
    {
      "epoch": 0.5776444111027756,
      "grad_norm": 0.6333487033843994,
      "learning_rate": 7.111975362261618e-05,
      "loss": 0.7741,
      "step": 14630
    },
    {
      "epoch": 0.5780392466537687,
      "grad_norm": 0.5572456121444702,
      "learning_rate": 7.110001184506653e-05,
      "loss": 0.8255,
      "step": 14640
    },
    {
      "epoch": 0.5784340822047617,
      "grad_norm": 0.6229902505874634,
      "learning_rate": 7.108027006751688e-05,
      "loss": 0.7262,
      "step": 14650
    },
    {
      "epoch": 0.5788289177557547,
      "grad_norm": 0.8674890398979187,
      "learning_rate": 7.106052828996723e-05,
      "loss": 0.7837,
      "step": 14660
    },
    {
      "epoch": 0.5792237533067477,
      "grad_norm": 0.6693827509880066,
      "learning_rate": 7.104078651241758e-05,
      "loss": 0.7499,
      "step": 14670
    },
    {
      "epoch": 0.5796185888577408,
      "grad_norm": 0.6607065796852112,
      "learning_rate": 7.102104473486793e-05,
      "loss": 0.8232,
      "step": 14680
    },
    {
      "epoch": 0.5800134244087337,
      "grad_norm": 0.5719494819641113,
      "learning_rate": 7.100130295731828e-05,
      "loss": 0.7599,
      "step": 14690
    },
    {
      "epoch": 0.5804082599597268,
      "grad_norm": 0.7747814059257507,
      "learning_rate": 7.098156117976863e-05,
      "loss": 0.828,
      "step": 14700
    },
    {
      "epoch": 0.5808030955107197,
      "grad_norm": 0.6684888601303101,
      "learning_rate": 7.096181940221898e-05,
      "loss": 0.7377,
      "step": 14710
    },
    {
      "epoch": 0.5811979310617128,
      "grad_norm": 0.7558841109275818,
      "learning_rate": 7.094207762466933e-05,
      "loss": 0.7873,
      "step": 14720
    },
    {
      "epoch": 0.5815927666127058,
      "grad_norm": 0.6949185729026794,
      "learning_rate": 7.092233584711968e-05,
      "loss": 0.7642,
      "step": 14730
    },
    {
      "epoch": 0.5819876021636988,
      "grad_norm": 0.6669390797615051,
      "learning_rate": 7.090259406957003e-05,
      "loss": 0.7448,
      "step": 14740
    },
    {
      "epoch": 0.5823824377146918,
      "grad_norm": 0.859322726726532,
      "learning_rate": 7.088285229202037e-05,
      "loss": 0.7808,
      "step": 14750
    },
    {
      "epoch": 0.5827772732656848,
      "grad_norm": 0.63298100233078,
      "learning_rate": 7.086311051447072e-05,
      "loss": 0.756,
      "step": 14760
    },
    {
      "epoch": 0.5831721088166778,
      "grad_norm": 0.8312320113182068,
      "learning_rate": 7.084336873692108e-05,
      "loss": 0.7746,
      "step": 14770
    },
    {
      "epoch": 0.5835669443676709,
      "grad_norm": 0.6284334063529968,
      "learning_rate": 7.082362695937142e-05,
      "loss": 0.8063,
      "step": 14780
    },
    {
      "epoch": 0.5839617799186638,
      "grad_norm": 0.5904873609542847,
      "learning_rate": 7.080388518182177e-05,
      "loss": 0.7068,
      "step": 14790
    },
    {
      "epoch": 0.5843566154696569,
      "grad_norm": 0.6696949005126953,
      "learning_rate": 7.078414340427212e-05,
      "loss": 0.8285,
      "step": 14800
    },
    {
      "epoch": 0.5847514510206498,
      "grad_norm": 0.5489307045936584,
      "learning_rate": 7.076440162672248e-05,
      "loss": 0.7641,
      "step": 14810
    },
    {
      "epoch": 0.5851462865716429,
      "grad_norm": 0.7772567868232727,
      "learning_rate": 7.074465984917282e-05,
      "loss": 0.761,
      "step": 14820
    },
    {
      "epoch": 0.5855411221226359,
      "grad_norm": 0.4830676019191742,
      "learning_rate": 7.072491807162317e-05,
      "loss": 0.7555,
      "step": 14830
    },
    {
      "epoch": 0.5859359576736289,
      "grad_norm": 0.600470781326294,
      "learning_rate": 7.070517629407352e-05,
      "loss": 0.7938,
      "step": 14840
    },
    {
      "epoch": 0.5863307932246219,
      "grad_norm": 0.9462676644325256,
      "learning_rate": 7.068543451652387e-05,
      "loss": 0.7914,
      "step": 14850
    },
    {
      "epoch": 0.586725628775615,
      "grad_norm": 0.6031820774078369,
      "learning_rate": 7.066569273897422e-05,
      "loss": 0.7485,
      "step": 14860
    },
    {
      "epoch": 0.5871204643266079,
      "grad_norm": 0.5999231934547424,
      "learning_rate": 7.064595096142457e-05,
      "loss": 0.7685,
      "step": 14870
    },
    {
      "epoch": 0.587515299877601,
      "grad_norm": 0.6746068596839905,
      "learning_rate": 7.062620918387492e-05,
      "loss": 0.7664,
      "step": 14880
    },
    {
      "epoch": 0.5879101354285939,
      "grad_norm": 0.5502045154571533,
      "learning_rate": 7.060646740632527e-05,
      "loss": 0.7314,
      "step": 14890
    },
    {
      "epoch": 0.588304970979587,
      "grad_norm": 0.5857179760932922,
      "learning_rate": 7.058672562877562e-05,
      "loss": 0.7626,
      "step": 14900
    },
    {
      "epoch": 0.58869980653058,
      "grad_norm": 0.7914093136787415,
      "learning_rate": 7.056698385122597e-05,
      "loss": 0.7734,
      "step": 14910
    },
    {
      "epoch": 0.589094642081573,
      "grad_norm": 0.8161295056343079,
      "learning_rate": 7.054724207367632e-05,
      "loss": 0.7226,
      "step": 14920
    },
    {
      "epoch": 0.589489477632566,
      "grad_norm": 0.58575838804245,
      "learning_rate": 7.052750029612667e-05,
      "loss": 0.7362,
      "step": 14930
    },
    {
      "epoch": 0.589884313183559,
      "grad_norm": 0.6129579544067383,
      "learning_rate": 7.050775851857702e-05,
      "loss": 0.7558,
      "step": 14940
    },
    {
      "epoch": 0.590279148734552,
      "grad_norm": 0.7330158948898315,
      "learning_rate": 7.048801674102737e-05,
      "loss": 0.7697,
      "step": 14950
    },
    {
      "epoch": 0.5906739842855451,
      "grad_norm": 0.7233977913856506,
      "learning_rate": 7.04682749634777e-05,
      "loss": 0.7074,
      "step": 14960
    },
    {
      "epoch": 0.591068819836538,
      "grad_norm": 0.8432481288909912,
      "learning_rate": 7.044853318592807e-05,
      "loss": 0.7843,
      "step": 14970
    },
    {
      "epoch": 0.5914636553875311,
      "grad_norm": 0.5931739807128906,
      "learning_rate": 7.042879140837842e-05,
      "loss": 0.7616,
      "step": 14980
    },
    {
      "epoch": 0.591858490938524,
      "grad_norm": 0.6545963287353516,
      "learning_rate": 7.040904963082877e-05,
      "loss": 0.8138,
      "step": 14990
    },
    {
      "epoch": 0.5922533264895171,
      "grad_norm": 0.5387380719184875,
      "learning_rate": 7.03893078532791e-05,
      "loss": 0.7581,
      "step": 15000
    },
    {
      "epoch": 0.5922533264895171,
      "eval_loss": 0.7917139530181885,
      "eval_runtime": 1215.0828,
      "eval_samples_per_second": 9.264,
      "eval_steps_per_second": 9.264,
      "step": 15000
    },
    {
      "epoch": 0.5926481620405101,
      "grad_norm": 0.6550899147987366,
      "learning_rate": 7.036956607572947e-05,
      "loss": 0.7991,
      "step": 15010
    },
    {
      "epoch": 0.5930429975915031,
      "grad_norm": 0.7894794940948486,
      "learning_rate": 7.034982429817982e-05,
      "loss": 0.7425,
      "step": 15020
    },
    {
      "epoch": 0.5934378331424961,
      "grad_norm": 0.5271204710006714,
      "learning_rate": 7.033008252063015e-05,
      "loss": 0.7833,
      "step": 15030
    },
    {
      "epoch": 0.5938326686934892,
      "grad_norm": 0.6322628259658813,
      "learning_rate": 7.03103407430805e-05,
      "loss": 0.7206,
      "step": 15040
    },
    {
      "epoch": 0.5942275042444821,
      "grad_norm": 0.7340500950813293,
      "learning_rate": 7.029059896553087e-05,
      "loss": 0.7824,
      "step": 15050
    },
    {
      "epoch": 0.5946223397954752,
      "grad_norm": 0.5949521064758301,
      "learning_rate": 7.02708571879812e-05,
      "loss": 0.7242,
      "step": 15060
    },
    {
      "epoch": 0.5950171753464681,
      "grad_norm": 0.6466352939605713,
      "learning_rate": 7.025111541043155e-05,
      "loss": 0.7317,
      "step": 15070
    },
    {
      "epoch": 0.5954120108974612,
      "grad_norm": 0.5655555725097656,
      "learning_rate": 7.02313736328819e-05,
      "loss": 0.7614,
      "step": 15080
    },
    {
      "epoch": 0.5958068464484542,
      "grad_norm": 0.5020460486412048,
      "learning_rate": 7.021163185533227e-05,
      "loss": 0.7915,
      "step": 15090
    },
    {
      "epoch": 0.5962016819994472,
      "grad_norm": 0.5459046363830566,
      "learning_rate": 7.01918900777826e-05,
      "loss": 0.7929,
      "step": 15100
    },
    {
      "epoch": 0.5965965175504402,
      "grad_norm": 0.6063904762268066,
      "learning_rate": 7.017214830023295e-05,
      "loss": 0.7832,
      "step": 15110
    },
    {
      "epoch": 0.5969913531014333,
      "grad_norm": 0.5803920030593872,
      "learning_rate": 7.01524065226833e-05,
      "loss": 0.7882,
      "step": 15120
    },
    {
      "epoch": 0.5973861886524263,
      "grad_norm": 0.7533361911773682,
      "learning_rate": 7.013266474513365e-05,
      "loss": 0.7711,
      "step": 15130
    },
    {
      "epoch": 0.5977810242034193,
      "grad_norm": 0.7209520936012268,
      "learning_rate": 7.0112922967584e-05,
      "loss": 0.8115,
      "step": 15140
    },
    {
      "epoch": 0.5981758597544123,
      "grad_norm": 0.7162090539932251,
      "learning_rate": 7.009318119003435e-05,
      "loss": 0.7692,
      "step": 15150
    },
    {
      "epoch": 0.5985706953054053,
      "grad_norm": 0.6279639005661011,
      "learning_rate": 7.00734394124847e-05,
      "loss": 0.7515,
      "step": 15160
    },
    {
      "epoch": 0.5989655308563984,
      "grad_norm": 0.7001605033874512,
      "learning_rate": 7.005369763493505e-05,
      "loss": 0.7569,
      "step": 15170
    },
    {
      "epoch": 0.5993603664073913,
      "grad_norm": 0.6379891633987427,
      "learning_rate": 7.00339558573854e-05,
      "loss": 0.7584,
      "step": 15180
    },
    {
      "epoch": 0.5997552019583844,
      "grad_norm": 0.7316511869430542,
      "learning_rate": 7.001421407983575e-05,
      "loss": 0.7843,
      "step": 15190
    },
    {
      "epoch": 0.6001500375093773,
      "grad_norm": 0.6476930975914001,
      "learning_rate": 6.99944723022861e-05,
      "loss": 0.7544,
      "step": 15200
    },
    {
      "epoch": 0.6005448730603704,
      "grad_norm": 0.7773258686065674,
      "learning_rate": 6.997473052473645e-05,
      "loss": 0.8179,
      "step": 15210
    },
    {
      "epoch": 0.6009397086113634,
      "grad_norm": 0.5286266207695007,
      "learning_rate": 6.99549887471868e-05,
      "loss": 0.7868,
      "step": 15220
    },
    {
      "epoch": 0.6013345441623564,
      "grad_norm": 0.7698918581008911,
      "learning_rate": 6.993524696963715e-05,
      "loss": 0.7632,
      "step": 15230
    },
    {
      "epoch": 0.6017293797133494,
      "grad_norm": 0.6421895623207092,
      "learning_rate": 6.991550519208749e-05,
      "loss": 0.7425,
      "step": 15240
    },
    {
      "epoch": 0.6021242152643425,
      "grad_norm": 0.7675660252571106,
      "learning_rate": 6.989576341453785e-05,
      "loss": 0.7719,
      "step": 15250
    },
    {
      "epoch": 0.6025190508153354,
      "grad_norm": 0.5695964694023132,
      "learning_rate": 6.98760216369882e-05,
      "loss": 0.7392,
      "step": 15260
    },
    {
      "epoch": 0.6029138863663285,
      "grad_norm": 0.5332979559898376,
      "learning_rate": 6.985627985943855e-05,
      "loss": 0.7534,
      "step": 15270
    },
    {
      "epoch": 0.6033087219173214,
      "grad_norm": 0.6687954068183899,
      "learning_rate": 6.983653808188889e-05,
      "loss": 0.7512,
      "step": 15280
    },
    {
      "epoch": 0.6037035574683145,
      "grad_norm": 0.5503205060958862,
      "learning_rate": 6.981679630433925e-05,
      "loss": 0.7492,
      "step": 15290
    },
    {
      "epoch": 0.6040983930193075,
      "grad_norm": 0.650392472743988,
      "learning_rate": 6.97970545267896e-05,
      "loss": 0.7718,
      "step": 15300
    },
    {
      "epoch": 0.6044932285703005,
      "grad_norm": 0.7663604617118835,
      "learning_rate": 6.977731274923994e-05,
      "loss": 0.7784,
      "step": 15310
    },
    {
      "epoch": 0.6048880641212935,
      "grad_norm": 0.8143925070762634,
      "learning_rate": 6.975757097169029e-05,
      "loss": 0.7853,
      "step": 15320
    },
    {
      "epoch": 0.6052828996722865,
      "grad_norm": 0.6875696778297424,
      "learning_rate": 6.973782919414065e-05,
      "loss": 0.8077,
      "step": 15330
    },
    {
      "epoch": 0.6056777352232795,
      "grad_norm": 0.841258704662323,
      "learning_rate": 6.9718087416591e-05,
      "loss": 0.7029,
      "step": 15340
    },
    {
      "epoch": 0.6060725707742726,
      "grad_norm": 0.6731675267219543,
      "learning_rate": 6.969834563904134e-05,
      "loss": 0.7275,
      "step": 15350
    },
    {
      "epoch": 0.6064674063252655,
      "grad_norm": 0.6496155261993408,
      "learning_rate": 6.967860386149169e-05,
      "loss": 0.7717,
      "step": 15360
    },
    {
      "epoch": 0.6068622418762586,
      "grad_norm": 0.5873066186904907,
      "learning_rate": 6.965886208394205e-05,
      "loss": 0.8207,
      "step": 15370
    },
    {
      "epoch": 0.6072570774272515,
      "grad_norm": 0.7570459246635437,
      "learning_rate": 6.963912030639239e-05,
      "loss": 0.7449,
      "step": 15380
    },
    {
      "epoch": 0.6076519129782446,
      "grad_norm": 0.9346799254417419,
      "learning_rate": 6.961937852884274e-05,
      "loss": 0.8378,
      "step": 15390
    },
    {
      "epoch": 0.6080467485292376,
      "grad_norm": 0.7119365930557251,
      "learning_rate": 6.959963675129309e-05,
      "loss": 0.7764,
      "step": 15400
    },
    {
      "epoch": 0.6084415840802306,
      "grad_norm": 0.5643101930618286,
      "learning_rate": 6.957989497374344e-05,
      "loss": 0.7502,
      "step": 15410
    },
    {
      "epoch": 0.6088364196312236,
      "grad_norm": 0.560299277305603,
      "learning_rate": 6.956015319619379e-05,
      "loss": 0.7869,
      "step": 15420
    },
    {
      "epoch": 0.6092312551822167,
      "grad_norm": 0.666294515132904,
      "learning_rate": 6.954041141864414e-05,
      "loss": 0.7764,
      "step": 15430
    },
    {
      "epoch": 0.6096260907332096,
      "grad_norm": 0.7038396596908569,
      "learning_rate": 6.952066964109449e-05,
      "loss": 0.7635,
      "step": 15440
    },
    {
      "epoch": 0.6100209262842027,
      "grad_norm": 0.5924454927444458,
      "learning_rate": 6.950092786354484e-05,
      "loss": 0.7731,
      "step": 15450
    },
    {
      "epoch": 0.6104157618351956,
      "grad_norm": 0.74095618724823,
      "learning_rate": 6.948118608599519e-05,
      "loss": 0.769,
      "step": 15460
    },
    {
      "epoch": 0.6108105973861887,
      "grad_norm": 0.6099414229393005,
      "learning_rate": 6.946144430844554e-05,
      "loss": 0.7477,
      "step": 15470
    },
    {
      "epoch": 0.6112054329371817,
      "grad_norm": 0.7386201024055481,
      "learning_rate": 6.944170253089589e-05,
      "loss": 0.737,
      "step": 15480
    },
    {
      "epoch": 0.6116002684881747,
      "grad_norm": 0.665571928024292,
      "learning_rate": 6.942196075334624e-05,
      "loss": 0.7872,
      "step": 15490
    },
    {
      "epoch": 0.6119951040391677,
      "grad_norm": 0.9050758481025696,
      "learning_rate": 6.940221897579659e-05,
      "loss": 0.8493,
      "step": 15500
    },
    {
      "epoch": 0.6123899395901607,
      "grad_norm": 0.705315887928009,
      "learning_rate": 6.938247719824694e-05,
      "loss": 0.7518,
      "step": 15510
    },
    {
      "epoch": 0.6127847751411537,
      "grad_norm": 0.8458222150802612,
      "learning_rate": 6.936273542069727e-05,
      "loss": 0.7644,
      "step": 15520
    },
    {
      "epoch": 0.6131796106921468,
      "grad_norm": 1.059733510017395,
      "learning_rate": 6.934299364314764e-05,
      "loss": 0.7724,
      "step": 15530
    },
    {
      "epoch": 0.6135744462431397,
      "grad_norm": 1.0144909620285034,
      "learning_rate": 6.932325186559799e-05,
      "loss": 0.8006,
      "step": 15540
    },
    {
      "epoch": 0.6139692817941328,
      "grad_norm": 0.7890713214874268,
      "learning_rate": 6.930351008804834e-05,
      "loss": 0.817,
      "step": 15550
    },
    {
      "epoch": 0.6143641173451257,
      "grad_norm": 0.596610963344574,
      "learning_rate": 6.928376831049867e-05,
      "loss": 0.8019,
      "step": 15560
    },
    {
      "epoch": 0.6147589528961188,
      "grad_norm": 0.8269368410110474,
      "learning_rate": 6.926402653294903e-05,
      "loss": 0.8199,
      "step": 15570
    },
    {
      "epoch": 0.6151537884471118,
      "grad_norm": 0.5524453520774841,
      "learning_rate": 6.924428475539938e-05,
      "loss": 0.7783,
      "step": 15580
    },
    {
      "epoch": 0.6155486239981048,
      "grad_norm": 1.0756877660751343,
      "learning_rate": 6.922454297784972e-05,
      "loss": 0.8172,
      "step": 15590
    },
    {
      "epoch": 0.6159434595490978,
      "grad_norm": 0.6570518612861633,
      "learning_rate": 6.920480120030007e-05,
      "loss": 0.7651,
      "step": 15600
    },
    {
      "epoch": 0.6163382951000909,
      "grad_norm": 0.7987695336341858,
      "learning_rate": 6.918505942275043e-05,
      "loss": 0.7549,
      "step": 15610
    },
    {
      "epoch": 0.6167331306510838,
      "grad_norm": 0.8222894072532654,
      "learning_rate": 6.916531764520078e-05,
      "loss": 0.7987,
      "step": 15620
    },
    {
      "epoch": 0.6171279662020769,
      "grad_norm": 0.8072043061256409,
      "learning_rate": 6.914557586765112e-05,
      "loss": 0.7494,
      "step": 15630
    },
    {
      "epoch": 0.6175228017530698,
      "grad_norm": 0.7552981972694397,
      "learning_rate": 6.912583409010147e-05,
      "loss": 0.8061,
      "step": 15640
    },
    {
      "epoch": 0.6179176373040629,
      "grad_norm": 0.6170778274536133,
      "learning_rate": 6.910609231255183e-05,
      "loss": 0.765,
      "step": 15650
    },
    {
      "epoch": 0.6183124728550559,
      "grad_norm": 0.9242129921913147,
      "learning_rate": 6.908635053500217e-05,
      "loss": 0.7697,
      "step": 15660
    },
    {
      "epoch": 0.6187073084060489,
      "grad_norm": 0.8242626786231995,
      "learning_rate": 6.906660875745252e-05,
      "loss": 0.7683,
      "step": 15670
    },
    {
      "epoch": 0.6191021439570419,
      "grad_norm": 0.7256656885147095,
      "learning_rate": 6.904686697990287e-05,
      "loss": 0.7732,
      "step": 15680
    },
    {
      "epoch": 0.619496979508035,
      "grad_norm": 0.6916788220405579,
      "learning_rate": 6.902712520235322e-05,
      "loss": 0.7494,
      "step": 15690
    },
    {
      "epoch": 0.6198918150590279,
      "grad_norm": 0.6481537818908691,
      "learning_rate": 6.900738342480357e-05,
      "loss": 0.7776,
      "step": 15700
    },
    {
      "epoch": 0.620286650610021,
      "grad_norm": 0.545897901058197,
      "learning_rate": 6.898764164725392e-05,
      "loss": 0.8013,
      "step": 15710
    },
    {
      "epoch": 0.6206814861610139,
      "grad_norm": 0.5743032097816467,
      "learning_rate": 6.896789986970427e-05,
      "loss": 0.7701,
      "step": 15720
    },
    {
      "epoch": 0.621076321712007,
      "grad_norm": 0.5644164085388184,
      "learning_rate": 6.894815809215462e-05,
      "loss": 0.7878,
      "step": 15730
    },
    {
      "epoch": 0.621471157263,
      "grad_norm": 0.9686983823776245,
      "learning_rate": 6.892841631460497e-05,
      "loss": 0.6999,
      "step": 15740
    },
    {
      "epoch": 0.621865992813993,
      "grad_norm": 0.5605336427688599,
      "learning_rate": 6.890867453705532e-05,
      "loss": 0.7523,
      "step": 15750
    },
    {
      "epoch": 0.622260828364986,
      "grad_norm": 0.6378804445266724,
      "learning_rate": 6.888893275950567e-05,
      "loss": 0.7655,
      "step": 15760
    },
    {
      "epoch": 0.622655663915979,
      "grad_norm": 0.7251653671264648,
      "learning_rate": 6.886919098195602e-05,
      "loss": 0.7687,
      "step": 15770
    },
    {
      "epoch": 0.623050499466972,
      "grad_norm": 0.5370622873306274,
      "learning_rate": 6.884944920440637e-05,
      "loss": 0.8157,
      "step": 15780
    },
    {
      "epoch": 0.6234453350179651,
      "grad_norm": 0.6600772738456726,
      "learning_rate": 6.882970742685672e-05,
      "loss": 0.7592,
      "step": 15790
    },
    {
      "epoch": 0.623840170568958,
      "grad_norm": 0.6747241020202637,
      "learning_rate": 6.880996564930706e-05,
      "loss": 0.7507,
      "step": 15800
    },
    {
      "epoch": 0.6242350061199511,
      "grad_norm": 0.5568457841873169,
      "learning_rate": 6.879022387175742e-05,
      "loss": 0.7305,
      "step": 15810
    },
    {
      "epoch": 0.624629841670944,
      "grad_norm": 0.6351909637451172,
      "learning_rate": 6.877048209420777e-05,
      "loss": 0.754,
      "step": 15820
    },
    {
      "epoch": 0.6250246772219371,
      "grad_norm": 0.5560494661331177,
      "learning_rate": 6.875074031665812e-05,
      "loss": 0.7131,
      "step": 15830
    },
    {
      "epoch": 0.6254195127729301,
      "grad_norm": 0.5975151658058167,
      "learning_rate": 6.873099853910846e-05,
      "loss": 0.7827,
      "step": 15840
    },
    {
      "epoch": 0.6258143483239231,
      "grad_norm": 0.6197441816329956,
      "learning_rate": 6.871125676155882e-05,
      "loss": 0.7492,
      "step": 15850
    },
    {
      "epoch": 0.6262091838749161,
      "grad_norm": 0.7439727783203125,
      "learning_rate": 6.869151498400917e-05,
      "loss": 0.767,
      "step": 15860
    },
    {
      "epoch": 0.6266040194259092,
      "grad_norm": 0.648250162601471,
      "learning_rate": 6.86717732064595e-05,
      "loss": 0.775,
      "step": 15870
    },
    {
      "epoch": 0.6269988549769021,
      "grad_norm": 1.1372383832931519,
      "learning_rate": 6.865203142890986e-05,
      "loss": 0.7725,
      "step": 15880
    },
    {
      "epoch": 0.6273936905278952,
      "grad_norm": 1.0061287879943848,
      "learning_rate": 6.863228965136022e-05,
      "loss": 0.7531,
      "step": 15890
    },
    {
      "epoch": 0.6277885260788881,
      "grad_norm": 0.9141510725021362,
      "learning_rate": 6.861254787381057e-05,
      "loss": 0.7526,
      "step": 15900
    },
    {
      "epoch": 0.6281833616298812,
      "grad_norm": 0.5772908329963684,
      "learning_rate": 6.85928060962609e-05,
      "loss": 0.7681,
      "step": 15910
    },
    {
      "epoch": 0.6285781971808742,
      "grad_norm": 0.756662905216217,
      "learning_rate": 6.857306431871125e-05,
      "loss": 0.738,
      "step": 15920
    },
    {
      "epoch": 0.6289730327318672,
      "grad_norm": 0.6585360169410706,
      "learning_rate": 6.855332254116162e-05,
      "loss": 0.8019,
      "step": 15930
    },
    {
      "epoch": 0.6293678682828602,
      "grad_norm": 0.5938853025436401,
      "learning_rate": 6.853358076361195e-05,
      "loss": 0.7555,
      "step": 15940
    },
    {
      "epoch": 0.6297627038338532,
      "grad_norm": 0.7161555886268616,
      "learning_rate": 6.85138389860623e-05,
      "loss": 0.7943,
      "step": 15950
    },
    {
      "epoch": 0.6301575393848462,
      "grad_norm": 0.5534514784812927,
      "learning_rate": 6.849409720851265e-05,
      "loss": 0.74,
      "step": 15960
    },
    {
      "epoch": 0.6305523749358393,
      "grad_norm": 0.5478984117507935,
      "learning_rate": 6.8474355430963e-05,
      "loss": 0.7715,
      "step": 15970
    },
    {
      "epoch": 0.6309472104868322,
      "grad_norm": 0.7306235432624817,
      "learning_rate": 6.845461365341335e-05,
      "loss": 0.757,
      "step": 15980
    },
    {
      "epoch": 0.6313420460378253,
      "grad_norm": 0.7439174652099609,
      "learning_rate": 6.84348718758637e-05,
      "loss": 0.7427,
      "step": 15990
    },
    {
      "epoch": 0.6317368815888182,
      "grad_norm": 0.6743655800819397,
      "learning_rate": 6.841513009831405e-05,
      "loss": 0.7702,
      "step": 16000
    },
    {
      "epoch": 0.6317368815888182,
      "eval_loss": 0.789116382598877,
      "eval_runtime": 1215.0753,
      "eval_samples_per_second": 9.264,
      "eval_steps_per_second": 9.264,
      "step": 16000
    },
    {
      "epoch": 0.6321317171398113,
      "grad_norm": 0.8135437369346619,
      "learning_rate": 6.83953883207644e-05,
      "loss": 0.7486,
      "step": 16010
    },
    {
      "epoch": 0.6325265526908043,
      "grad_norm": 0.591954231262207,
      "learning_rate": 6.837564654321475e-05,
      "loss": 0.8058,
      "step": 16020
    },
    {
      "epoch": 0.6329213882417973,
      "grad_norm": 1.0758217573165894,
      "learning_rate": 6.83559047656651e-05,
      "loss": 0.8093,
      "step": 16030
    },
    {
      "epoch": 0.6333162237927903,
      "grad_norm": 0.6133176684379578,
      "learning_rate": 6.833616298811545e-05,
      "loss": 0.767,
      "step": 16040
    },
    {
      "epoch": 0.6337110593437834,
      "grad_norm": 0.7464852333068848,
      "learning_rate": 6.83164212105658e-05,
      "loss": 0.7515,
      "step": 16050
    },
    {
      "epoch": 0.6341058948947763,
      "grad_norm": 0.60821133852005,
      "learning_rate": 6.829667943301615e-05,
      "loss": 0.7277,
      "step": 16060
    },
    {
      "epoch": 0.6345007304457694,
      "grad_norm": 0.6696183681488037,
      "learning_rate": 6.82769376554665e-05,
      "loss": 0.793,
      "step": 16070
    },
    {
      "epoch": 0.6348955659967623,
      "grad_norm": 0.705920934677124,
      "learning_rate": 6.825719587791684e-05,
      "loss": 0.8047,
      "step": 16080
    },
    {
      "epoch": 0.6352904015477554,
      "grad_norm": 0.6810670495033264,
      "learning_rate": 6.82374541003672e-05,
      "loss": 0.7652,
      "step": 16090
    },
    {
      "epoch": 0.6356852370987484,
      "grad_norm": 0.5768440365791321,
      "learning_rate": 6.821771232281755e-05,
      "loss": 0.7463,
      "step": 16100
    },
    {
      "epoch": 0.6360800726497414,
      "grad_norm": 0.5265468955039978,
      "learning_rate": 6.81979705452679e-05,
      "loss": 0.751,
      "step": 16110
    },
    {
      "epoch": 0.6364749082007344,
      "grad_norm": 0.6853092312812805,
      "learning_rate": 6.817822876771824e-05,
      "loss": 0.8323,
      "step": 16120
    },
    {
      "epoch": 0.6368697437517274,
      "grad_norm": 0.7514411807060242,
      "learning_rate": 6.81584869901686e-05,
      "loss": 0.7963,
      "step": 16130
    },
    {
      "epoch": 0.6372645793027204,
      "grad_norm": 0.7728338241577148,
      "learning_rate": 6.813874521261895e-05,
      "loss": 0.7915,
      "step": 16140
    },
    {
      "epoch": 0.6376594148537135,
      "grad_norm": 0.7520976662635803,
      "learning_rate": 6.811900343506929e-05,
      "loss": 0.74,
      "step": 16150
    },
    {
      "epoch": 0.6380542504047064,
      "grad_norm": 0.6616979837417603,
      "learning_rate": 6.809926165751964e-05,
      "loss": 0.8067,
      "step": 16160
    },
    {
      "epoch": 0.6384490859556995,
      "grad_norm": 0.5098218321800232,
      "learning_rate": 6.807951987997e-05,
      "loss": 0.8044,
      "step": 16170
    },
    {
      "epoch": 0.6388439215066924,
      "grad_norm": 0.6557897925376892,
      "learning_rate": 6.805977810242035e-05,
      "loss": 0.788,
      "step": 16180
    },
    {
      "epoch": 0.6392387570576855,
      "grad_norm": 0.5188423991203308,
      "learning_rate": 6.804003632487069e-05,
      "loss": 0.7797,
      "step": 16190
    },
    {
      "epoch": 0.6396335926086785,
      "grad_norm": 0.7651286721229553,
      "learning_rate": 6.802029454732104e-05,
      "loss": 0.788,
      "step": 16200
    },
    {
      "epoch": 0.6400284281596715,
      "grad_norm": 0.5766180753707886,
      "learning_rate": 6.80005527697714e-05,
      "loss": 0.7686,
      "step": 16210
    },
    {
      "epoch": 0.6404232637106645,
      "grad_norm": 0.8401623964309692,
      "learning_rate": 6.798081099222174e-05,
      "loss": 0.7858,
      "step": 16220
    },
    {
      "epoch": 0.6408180992616576,
      "grad_norm": 0.6977384686470032,
      "learning_rate": 6.796106921467209e-05,
      "loss": 0.744,
      "step": 16230
    },
    {
      "epoch": 0.6412129348126505,
      "grad_norm": 0.482693076133728,
      "learning_rate": 6.794132743712244e-05,
      "loss": 0.7137,
      "step": 16240
    },
    {
      "epoch": 0.6416077703636436,
      "grad_norm": 0.7521809935569763,
      "learning_rate": 6.792158565957279e-05,
      "loss": 0.818,
      "step": 16250
    },
    {
      "epoch": 0.6420026059146365,
      "grad_norm": 0.6221334934234619,
      "learning_rate": 6.790184388202314e-05,
      "loss": 0.758,
      "step": 16260
    },
    {
      "epoch": 0.6423974414656296,
      "grad_norm": 0.8966332077980042,
      "learning_rate": 6.788210210447349e-05,
      "loss": 0.7832,
      "step": 16270
    },
    {
      "epoch": 0.6427922770166226,
      "grad_norm": 0.6503891348838806,
      "learning_rate": 6.786236032692384e-05,
      "loss": 0.75,
      "step": 16280
    },
    {
      "epoch": 0.6431871125676156,
      "grad_norm": 0.6676045060157776,
      "learning_rate": 6.784261854937419e-05,
      "loss": 0.7767,
      "step": 16290
    },
    {
      "epoch": 0.6435819481186086,
      "grad_norm": 0.5581197142601013,
      "learning_rate": 6.782287677182454e-05,
      "loss": 0.7614,
      "step": 16300
    },
    {
      "epoch": 0.6439767836696016,
      "grad_norm": 0.6801439523696899,
      "learning_rate": 6.780313499427489e-05,
      "loss": 0.7646,
      "step": 16310
    },
    {
      "epoch": 0.6443716192205946,
      "grad_norm": 0.7119751572608948,
      "learning_rate": 6.778339321672524e-05,
      "loss": 0.7865,
      "step": 16320
    },
    {
      "epoch": 0.6447664547715877,
      "grad_norm": 0.6250141859054565,
      "learning_rate": 6.776365143917559e-05,
      "loss": 0.7724,
      "step": 16330
    },
    {
      "epoch": 0.6451612903225806,
      "grad_norm": 0.6381964087486267,
      "learning_rate": 6.774390966162594e-05,
      "loss": 0.7816,
      "step": 16340
    },
    {
      "epoch": 0.6455561258735737,
      "grad_norm": 0.867359459400177,
      "learning_rate": 6.772416788407629e-05,
      "loss": 0.7655,
      "step": 16350
    },
    {
      "epoch": 0.6459509614245666,
      "grad_norm": 0.7748587131500244,
      "learning_rate": 6.770442610652662e-05,
      "loss": 0.808,
      "step": 16360
    },
    {
      "epoch": 0.6463457969755597,
      "grad_norm": 0.5868251919746399,
      "learning_rate": 6.768468432897699e-05,
      "loss": 0.7661,
      "step": 16370
    },
    {
      "epoch": 0.6467406325265527,
      "grad_norm": 0.6550024747848511,
      "learning_rate": 6.766494255142734e-05,
      "loss": 0.7087,
      "step": 16380
    },
    {
      "epoch": 0.6471354680775457,
      "grad_norm": 0.6297149658203125,
      "learning_rate": 6.764520077387769e-05,
      "loss": 0.7777,
      "step": 16390
    },
    {
      "epoch": 0.6475303036285387,
      "grad_norm": 0.6585521697998047,
      "learning_rate": 6.762545899632802e-05,
      "loss": 0.7495,
      "step": 16400
    },
    {
      "epoch": 0.6479251391795318,
      "grad_norm": 0.8524125218391418,
      "learning_rate": 6.760571721877839e-05,
      "loss": 0.7708,
      "step": 16410
    },
    {
      "epoch": 0.6483199747305247,
      "grad_norm": 0.6276322603225708,
      "learning_rate": 6.758597544122874e-05,
      "loss": 0.7429,
      "step": 16420
    },
    {
      "epoch": 0.6487148102815178,
      "grad_norm": 0.732085108757019,
      "learning_rate": 6.756623366367907e-05,
      "loss": 0.7553,
      "step": 16430
    },
    {
      "epoch": 0.6491096458325107,
      "grad_norm": 0.5849114060401917,
      "learning_rate": 6.754649188612942e-05,
      "loss": 0.757,
      "step": 16440
    },
    {
      "epoch": 0.6495044813835038,
      "grad_norm": 1.04624342918396,
      "learning_rate": 6.752675010857979e-05,
      "loss": 0.8141,
      "step": 16450
    },
    {
      "epoch": 0.6498993169344968,
      "grad_norm": 0.5785509943962097,
      "learning_rate": 6.750700833103014e-05,
      "loss": 0.7425,
      "step": 16460
    },
    {
      "epoch": 0.6502941524854898,
      "grad_norm": 0.6683269739151001,
      "learning_rate": 6.748726655348047e-05,
      "loss": 0.7311,
      "step": 16470
    },
    {
      "epoch": 0.6506889880364828,
      "grad_norm": 0.5071510672569275,
      "learning_rate": 6.746752477593082e-05,
      "loss": 0.7561,
      "step": 16480
    },
    {
      "epoch": 0.6510838235874759,
      "grad_norm": 0.5828620791435242,
      "learning_rate": 6.744778299838119e-05,
      "loss": 0.7706,
      "step": 16490
    },
    {
      "epoch": 0.6514786591384688,
      "grad_norm": 0.7247415781021118,
      "learning_rate": 6.742804122083152e-05,
      "loss": 0.7128,
      "step": 16500
    },
    {
      "epoch": 0.6518734946894619,
      "grad_norm": 0.5934798717498779,
      "learning_rate": 6.740829944328187e-05,
      "loss": 0.7749,
      "step": 16510
    },
    {
      "epoch": 0.6522683302404548,
      "grad_norm": 0.627058207988739,
      "learning_rate": 6.738855766573222e-05,
      "loss": 0.7291,
      "step": 16520
    },
    {
      "epoch": 0.6526631657914479,
      "grad_norm": 0.7194755673408508,
      "learning_rate": 6.736881588818259e-05,
      "loss": 0.7116,
      "step": 16530
    },
    {
      "epoch": 0.6530580013424409,
      "grad_norm": 0.7428192496299744,
      "learning_rate": 6.734907411063292e-05,
      "loss": 0.7641,
      "step": 16540
    },
    {
      "epoch": 0.6534528368934339,
      "grad_norm": 0.5888144373893738,
      "learning_rate": 6.732933233308327e-05,
      "loss": 0.7535,
      "step": 16550
    },
    {
      "epoch": 0.6538476724444269,
      "grad_norm": 0.6760684847831726,
      "learning_rate": 6.730959055553362e-05,
      "loss": 0.7595,
      "step": 16560
    },
    {
      "epoch": 0.6542425079954199,
      "grad_norm": 0.6023147106170654,
      "learning_rate": 6.728984877798397e-05,
      "loss": 0.771,
      "step": 16570
    },
    {
      "epoch": 0.6546373435464129,
      "grad_norm": 0.6438781023025513,
      "learning_rate": 6.727010700043432e-05,
      "loss": 0.8059,
      "step": 16580
    },
    {
      "epoch": 0.655032179097406,
      "grad_norm": 0.7148849964141846,
      "learning_rate": 6.725036522288467e-05,
      "loss": 0.7847,
      "step": 16590
    },
    {
      "epoch": 0.6554270146483989,
      "grad_norm": 0.6499571204185486,
      "learning_rate": 6.723062344533502e-05,
      "loss": 0.7098,
      "step": 16600
    },
    {
      "epoch": 0.655821850199392,
      "grad_norm": 0.6407788991928101,
      "learning_rate": 6.721088166778537e-05,
      "loss": 0.7418,
      "step": 16610
    },
    {
      "epoch": 0.6562166857503849,
      "grad_norm": 0.6939117312431335,
      "learning_rate": 6.719113989023572e-05,
      "loss": 0.7999,
      "step": 16620
    },
    {
      "epoch": 0.656611521301378,
      "grad_norm": 0.571340799331665,
      "learning_rate": 6.717139811268607e-05,
      "loss": 0.7875,
      "step": 16630
    },
    {
      "epoch": 0.657006356852371,
      "grad_norm": 0.7884671688079834,
      "learning_rate": 6.715165633513642e-05,
      "loss": 0.7797,
      "step": 16640
    },
    {
      "epoch": 0.657401192403364,
      "grad_norm": 0.6350172162055969,
      "learning_rate": 6.713191455758677e-05,
      "loss": 0.7732,
      "step": 16650
    },
    {
      "epoch": 0.657796027954357,
      "grad_norm": 0.5946335196495056,
      "learning_rate": 6.711217278003712e-05,
      "loss": 0.7489,
      "step": 16660
    },
    {
      "epoch": 0.65819086350535,
      "grad_norm": 0.740353524684906,
      "learning_rate": 6.709243100248747e-05,
      "loss": 0.7693,
      "step": 16670
    },
    {
      "epoch": 0.658585699056343,
      "grad_norm": 0.6220085620880127,
      "learning_rate": 6.707268922493781e-05,
      "loss": 0.7893,
      "step": 16680
    },
    {
      "epoch": 0.6589805346073361,
      "grad_norm": 0.7025474905967712,
      "learning_rate": 6.705294744738817e-05,
      "loss": 0.7674,
      "step": 16690
    },
    {
      "epoch": 0.659375370158329,
      "grad_norm": 0.7749021649360657,
      "learning_rate": 6.703320566983852e-05,
      "loss": 0.7874,
      "step": 16700
    },
    {
      "epoch": 0.6597702057093221,
      "grad_norm": 0.7466211318969727,
      "learning_rate": 6.701346389228886e-05,
      "loss": 0.7328,
      "step": 16710
    },
    {
      "epoch": 0.660165041260315,
      "grad_norm": 0.5966317653656006,
      "learning_rate": 6.699372211473921e-05,
      "loss": 0.7529,
      "step": 16720
    },
    {
      "epoch": 0.6605598768113081,
      "grad_norm": 0.7087287306785583,
      "learning_rate": 6.697398033718957e-05,
      "loss": 0.7897,
      "step": 16730
    },
    {
      "epoch": 0.6609547123623011,
      "grad_norm": 0.6385003924369812,
      "learning_rate": 6.695423855963992e-05,
      "loss": 0.777,
      "step": 16740
    },
    {
      "epoch": 0.6613495479132941,
      "grad_norm": 0.6310839056968689,
      "learning_rate": 6.693449678209026e-05,
      "loss": 0.7808,
      "step": 16750
    },
    {
      "epoch": 0.6617443834642871,
      "grad_norm": 0.9261127710342407,
      "learning_rate": 6.691475500454061e-05,
      "loss": 0.7601,
      "step": 16760
    },
    {
      "epoch": 0.6621392190152802,
      "grad_norm": 0.8948690891265869,
      "learning_rate": 6.689501322699097e-05,
      "loss": 0.7869,
      "step": 16770
    },
    {
      "epoch": 0.6625340545662731,
      "grad_norm": 0.7078599333763123,
      "learning_rate": 6.68752714494413e-05,
      "loss": 0.7751,
      "step": 16780
    },
    {
      "epoch": 0.6629288901172662,
      "grad_norm": 0.5582742094993591,
      "learning_rate": 6.685552967189166e-05,
      "loss": 0.7709,
      "step": 16790
    },
    {
      "epoch": 0.6633237256682591,
      "grad_norm": 0.7691472172737122,
      "learning_rate": 6.6835787894342e-05,
      "loss": 0.7982,
      "step": 16800
    },
    {
      "epoch": 0.6637185612192522,
      "grad_norm": 0.7216423749923706,
      "learning_rate": 6.681604611679237e-05,
      "loss": 0.7297,
      "step": 16810
    },
    {
      "epoch": 0.6641133967702452,
      "grad_norm": 0.5864905118942261,
      "learning_rate": 6.67963043392427e-05,
      "loss": 0.7428,
      "step": 16820
    },
    {
      "epoch": 0.6645082323212382,
      "grad_norm": 0.6905734539031982,
      "learning_rate": 6.677656256169306e-05,
      "loss": 0.7417,
      "step": 16830
    },
    {
      "epoch": 0.6649030678722312,
      "grad_norm": 0.8749159574508667,
      "learning_rate": 6.67568207841434e-05,
      "loss": 0.7928,
      "step": 16840
    },
    {
      "epoch": 0.6652979034232243,
      "grad_norm": 0.5857992768287659,
      "learning_rate": 6.673707900659376e-05,
      "loss": 0.7476,
      "step": 16850
    },
    {
      "epoch": 0.6656927389742172,
      "grad_norm": 0.7574391961097717,
      "learning_rate": 6.67173372290441e-05,
      "loss": 0.7824,
      "step": 16860
    },
    {
      "epoch": 0.6660875745252103,
      "grad_norm": 0.6882028579711914,
      "learning_rate": 6.669759545149446e-05,
      "loss": 0.7868,
      "step": 16870
    },
    {
      "epoch": 0.6664824100762032,
      "grad_norm": 0.6031939387321472,
      "learning_rate": 6.66778536739448e-05,
      "loss": 0.7745,
      "step": 16880
    },
    {
      "epoch": 0.6668772456271963,
      "grad_norm": 0.8073292970657349,
      "learning_rate": 6.665811189639516e-05,
      "loss": 0.7404,
      "step": 16890
    },
    {
      "epoch": 0.6672720811781893,
      "grad_norm": 0.6610740423202515,
      "learning_rate": 6.66383701188455e-05,
      "loss": 0.771,
      "step": 16900
    },
    {
      "epoch": 0.6676669167291823,
      "grad_norm": 0.8501059412956238,
      "learning_rate": 6.661862834129586e-05,
      "loss": 0.7327,
      "step": 16910
    },
    {
      "epoch": 0.6680617522801753,
      "grad_norm": 0.5541118383407593,
      "learning_rate": 6.65988865637462e-05,
      "loss": 0.7699,
      "step": 16920
    },
    {
      "epoch": 0.6684565878311683,
      "grad_norm": 0.662867546081543,
      "learning_rate": 6.657914478619656e-05,
      "loss": 0.767,
      "step": 16930
    },
    {
      "epoch": 0.6688514233821613,
      "grad_norm": 0.556549608707428,
      "learning_rate": 6.65594030086469e-05,
      "loss": 0.7811,
      "step": 16940
    },
    {
      "epoch": 0.6692462589331544,
      "grad_norm": 0.5977621078491211,
      "learning_rate": 6.653966123109726e-05,
      "loss": 0.7408,
      "step": 16950
    },
    {
      "epoch": 0.6696410944841473,
      "grad_norm": 0.5916113257408142,
      "learning_rate": 6.651991945354759e-05,
      "loss": 0.7399,
      "step": 16960
    },
    {
      "epoch": 0.6700359300351404,
      "grad_norm": 0.5582310557365417,
      "learning_rate": 6.650017767599796e-05,
      "loss": 0.753,
      "step": 16970
    },
    {
      "epoch": 0.6704307655861333,
      "grad_norm": 0.6345201730728149,
      "learning_rate": 6.64804358984483e-05,
      "loss": 0.7776,
      "step": 16980
    },
    {
      "epoch": 0.6708256011371264,
      "grad_norm": 0.6093125343322754,
      "learning_rate": 6.646069412089864e-05,
      "loss": 0.7689,
      "step": 16990
    },
    {
      "epoch": 0.6712204366881194,
      "grad_norm": 0.6525964140892029,
      "learning_rate": 6.644095234334899e-05,
      "loss": 0.7649,
      "step": 17000
    },
    {
      "epoch": 0.6712204366881194,
      "eval_loss": 0.7867364287376404,
      "eval_runtime": 1219.0694,
      "eval_samples_per_second": 9.234,
      "eval_steps_per_second": 9.234,
      "step": 17000
    },
    {
      "epoch": 0.6716152722391124,
      "grad_norm": 0.7925819754600525,
      "learning_rate": 6.642121056579935e-05,
      "loss": 0.7977,
      "step": 17010
    },
    {
      "epoch": 0.6720101077901054,
      "grad_norm": 0.6670516729354858,
      "learning_rate": 6.64014687882497e-05,
      "loss": 0.7551,
      "step": 17020
    },
    {
      "epoch": 0.6724049433410985,
      "grad_norm": 0.6354155540466309,
      "learning_rate": 6.638172701070004e-05,
      "loss": 0.7817,
      "step": 17030
    },
    {
      "epoch": 0.6727997788920914,
      "grad_norm": 0.6821786165237427,
      "learning_rate": 6.636198523315039e-05,
      "loss": 0.7777,
      "step": 17040
    },
    {
      "epoch": 0.6731946144430845,
      "grad_norm": 0.5779122710227966,
      "learning_rate": 6.634224345560075e-05,
      "loss": 0.7781,
      "step": 17050
    },
    {
      "epoch": 0.6735894499940774,
      "grad_norm": 0.800076425075531,
      "learning_rate": 6.632250167805109e-05,
      "loss": 0.7653,
      "step": 17060
    },
    {
      "epoch": 0.6739842855450705,
      "grad_norm": 0.5957666635513306,
      "learning_rate": 6.630275990050144e-05,
      "loss": 0.793,
      "step": 17070
    },
    {
      "epoch": 0.6743791210960635,
      "grad_norm": 0.5729408264160156,
      "learning_rate": 6.628301812295179e-05,
      "loss": 0.7654,
      "step": 17080
    },
    {
      "epoch": 0.6747739566470565,
      "grad_norm": 0.5675843358039856,
      "learning_rate": 6.626327634540215e-05,
      "loss": 0.7802,
      "step": 17090
    },
    {
      "epoch": 0.6751687921980495,
      "grad_norm": 0.6229000687599182,
      "learning_rate": 6.624353456785249e-05,
      "loss": 0.8479,
      "step": 17100
    },
    {
      "epoch": 0.6755636277490426,
      "grad_norm": 0.6223024129867554,
      "learning_rate": 6.622379279030284e-05,
      "loss": 0.7677,
      "step": 17110
    },
    {
      "epoch": 0.6759584633000355,
      "grad_norm": 0.6400243043899536,
      "learning_rate": 6.620405101275319e-05,
      "loss": 0.7962,
      "step": 17120
    },
    {
      "epoch": 0.6763532988510286,
      "grad_norm": 0.6039110422134399,
      "learning_rate": 6.618430923520354e-05,
      "loss": 0.7889,
      "step": 17130
    },
    {
      "epoch": 0.6767481344020215,
      "grad_norm": 0.7081669569015503,
      "learning_rate": 6.616456745765389e-05,
      "loss": 0.7786,
      "step": 17140
    },
    {
      "epoch": 0.6771429699530146,
      "grad_norm": 0.5935953259468079,
      "learning_rate": 6.614482568010424e-05,
      "loss": 0.7614,
      "step": 17150
    },
    {
      "epoch": 0.6775378055040076,
      "grad_norm": 0.7278865575790405,
      "learning_rate": 6.612508390255459e-05,
      "loss": 0.7348,
      "step": 17160
    },
    {
      "epoch": 0.6779326410550006,
      "grad_norm": 0.626869797706604,
      "learning_rate": 6.610534212500494e-05,
      "loss": 0.7508,
      "step": 17170
    },
    {
      "epoch": 0.6783274766059936,
      "grad_norm": 0.6826003789901733,
      "learning_rate": 6.608560034745529e-05,
      "loss": 0.757,
      "step": 17180
    },
    {
      "epoch": 0.6787223121569866,
      "grad_norm": 0.6002479791641235,
      "learning_rate": 6.606585856990564e-05,
      "loss": 0.7141,
      "step": 17190
    },
    {
      "epoch": 0.6791171477079796,
      "grad_norm": 0.67243492603302,
      "learning_rate": 6.604611679235599e-05,
      "loss": 0.7555,
      "step": 17200
    },
    {
      "epoch": 0.6795119832589727,
      "grad_norm": 0.6578114628791809,
      "learning_rate": 6.602637501480634e-05,
      "loss": 0.743,
      "step": 17210
    },
    {
      "epoch": 0.6799068188099656,
      "grad_norm": 0.6647624969482422,
      "learning_rate": 6.600663323725669e-05,
      "loss": 0.7635,
      "step": 17220
    },
    {
      "epoch": 0.6803016543609587,
      "grad_norm": 0.6973039507865906,
      "learning_rate": 6.598689145970704e-05,
      "loss": 0.7938,
      "step": 17230
    },
    {
      "epoch": 0.6806964899119516,
      "grad_norm": 0.7472714185714722,
      "learning_rate": 6.596714968215738e-05,
      "loss": 0.8122,
      "step": 17240
    },
    {
      "epoch": 0.6810913254629447,
      "grad_norm": 0.5859733819961548,
      "learning_rate": 6.594740790460774e-05,
      "loss": 0.7257,
      "step": 17250
    },
    {
      "epoch": 0.6814861610139377,
      "grad_norm": 0.6637718677520752,
      "learning_rate": 6.592766612705809e-05,
      "loss": 0.7371,
      "step": 17260
    },
    {
      "epoch": 0.6818809965649307,
      "grad_norm": 0.7119947075843811,
      "learning_rate": 6.590792434950843e-05,
      "loss": 0.7885,
      "step": 17270
    },
    {
      "epoch": 0.6822758321159237,
      "grad_norm": 0.6266738176345825,
      "learning_rate": 6.588818257195878e-05,
      "loss": 0.7558,
      "step": 17280
    },
    {
      "epoch": 0.6826706676669168,
      "grad_norm": 0.7508066296577454,
      "learning_rate": 6.586844079440914e-05,
      "loss": 0.7929,
      "step": 17290
    },
    {
      "epoch": 0.6830655032179097,
      "grad_norm": 0.5807524919509888,
      "learning_rate": 6.584869901685949e-05,
      "loss": 0.799,
      "step": 17300
    },
    {
      "epoch": 0.6834603387689028,
      "grad_norm": 0.5789412260055542,
      "learning_rate": 6.582895723930983e-05,
      "loss": 0.7139,
      "step": 17310
    },
    {
      "epoch": 0.6838551743198957,
      "grad_norm": 0.5411487817764282,
      "learning_rate": 6.580921546176017e-05,
      "loss": 0.7598,
      "step": 17320
    },
    {
      "epoch": 0.6842500098708888,
      "grad_norm": 0.7399130463600159,
      "learning_rate": 6.578947368421054e-05,
      "loss": 0.7719,
      "step": 17330
    },
    {
      "epoch": 0.6846448454218818,
      "grad_norm": 0.5960916876792908,
      "learning_rate": 6.576973190666087e-05,
      "loss": 0.7924,
      "step": 17340
    },
    {
      "epoch": 0.6850396809728748,
      "grad_norm": 0.9452070593833923,
      "learning_rate": 6.574999012911122e-05,
      "loss": 0.7444,
      "step": 17350
    },
    {
      "epoch": 0.6854345165238678,
      "grad_norm": 0.5764663815498352,
      "learning_rate": 6.573024835156157e-05,
      "loss": 0.7714,
      "step": 17360
    },
    {
      "epoch": 0.6858293520748608,
      "grad_norm": 0.8341732621192932,
      "learning_rate": 6.571050657401194e-05,
      "loss": 0.7594,
      "step": 17370
    },
    {
      "epoch": 0.6862241876258538,
      "grad_norm": 0.6533732414245605,
      "learning_rate": 6.569076479646227e-05,
      "loss": 0.772,
      "step": 17380
    },
    {
      "epoch": 0.6866190231768469,
      "grad_norm": 0.6126042604446411,
      "learning_rate": 6.567102301891262e-05,
      "loss": 0.7656,
      "step": 17390
    },
    {
      "epoch": 0.6870138587278398,
      "grad_norm": 0.6613826751708984,
      "learning_rate": 6.565128124136297e-05,
      "loss": 0.7795,
      "step": 17400
    },
    {
      "epoch": 0.6874086942788329,
      "grad_norm": 0.5552768707275391,
      "learning_rate": 6.563153946381332e-05,
      "loss": 0.7314,
      "step": 17410
    },
    {
      "epoch": 0.6878035298298258,
      "grad_norm": 0.8095720410346985,
      "learning_rate": 6.561179768626367e-05,
      "loss": 0.7902,
      "step": 17420
    },
    {
      "epoch": 0.6881983653808189,
      "grad_norm": 0.622170090675354,
      "learning_rate": 6.559205590871402e-05,
      "loss": 0.7657,
      "step": 17430
    },
    {
      "epoch": 0.6885932009318119,
      "grad_norm": 0.6938590407371521,
      "learning_rate": 6.557231413116437e-05,
      "loss": 0.7625,
      "step": 17440
    },
    {
      "epoch": 0.6889880364828049,
      "grad_norm": 0.6493036150932312,
      "learning_rate": 6.555257235361472e-05,
      "loss": 0.7452,
      "step": 17450
    },
    {
      "epoch": 0.6893828720337979,
      "grad_norm": 0.7159507870674133,
      "learning_rate": 6.553283057606507e-05,
      "loss": 0.7389,
      "step": 17460
    },
    {
      "epoch": 0.689777707584791,
      "grad_norm": 0.7454602122306824,
      "learning_rate": 6.551308879851542e-05,
      "loss": 0.7779,
      "step": 17470
    },
    {
      "epoch": 0.6901725431357839,
      "grad_norm": 0.6285935640335083,
      "learning_rate": 6.549334702096577e-05,
      "loss": 0.7271,
      "step": 17480
    },
    {
      "epoch": 0.690567378686777,
      "grad_norm": 0.7422964572906494,
      "learning_rate": 6.547360524341612e-05,
      "loss": 0.7898,
      "step": 17490
    },
    {
      "epoch": 0.6909622142377699,
      "grad_norm": 0.7271997928619385,
      "learning_rate": 6.545386346586647e-05,
      "loss": 0.8066,
      "step": 17500
    },
    {
      "epoch": 0.691357049788763,
      "grad_norm": 0.8177445530891418,
      "learning_rate": 6.543412168831682e-05,
      "loss": 0.7742,
      "step": 17510
    },
    {
      "epoch": 0.691751885339756,
      "grad_norm": 0.6140141487121582,
      "learning_rate": 6.541437991076716e-05,
      "loss": 0.8121,
      "step": 17520
    },
    {
      "epoch": 0.692146720890749,
      "grad_norm": 0.7023524045944214,
      "learning_rate": 6.539463813321752e-05,
      "loss": 0.7827,
      "step": 17530
    },
    {
      "epoch": 0.692541556441742,
      "grad_norm": 0.8436951637268066,
      "learning_rate": 6.537489635566787e-05,
      "loss": 0.8014,
      "step": 17540
    },
    {
      "epoch": 0.692936391992735,
      "grad_norm": 0.5703438520431519,
      "learning_rate": 6.535515457811822e-05,
      "loss": 0.7551,
      "step": 17550
    },
    {
      "epoch": 0.693331227543728,
      "grad_norm": 0.7413424253463745,
      "learning_rate": 6.533541280056856e-05,
      "loss": 0.7828,
      "step": 17560
    },
    {
      "epoch": 0.6937260630947211,
      "grad_norm": 0.6388198137283325,
      "learning_rate": 6.531567102301892e-05,
      "loss": 0.7627,
      "step": 17570
    },
    {
      "epoch": 0.694120898645714,
      "grad_norm": 0.6820152997970581,
      "learning_rate": 6.529592924546927e-05,
      "loss": 0.7693,
      "step": 17580
    },
    {
      "epoch": 0.6945157341967071,
      "grad_norm": 1.0213762521743774,
      "learning_rate": 6.527618746791961e-05,
      "loss": 0.7966,
      "step": 17590
    },
    {
      "epoch": 0.6949105697477,
      "grad_norm": 0.622673749923706,
      "learning_rate": 6.525644569036996e-05,
      "loss": 0.7527,
      "step": 17600
    },
    {
      "epoch": 0.6953054052986931,
      "grad_norm": 0.6751917600631714,
      "learning_rate": 6.523670391282032e-05,
      "loss": 0.7485,
      "step": 17610
    },
    {
      "epoch": 0.6957002408496861,
      "grad_norm": 0.7766610383987427,
      "learning_rate": 6.521696213527066e-05,
      "loss": 0.7573,
      "step": 17620
    },
    {
      "epoch": 0.6960950764006791,
      "grad_norm": 0.6093705296516418,
      "learning_rate": 6.519722035772101e-05,
      "loss": 0.755,
      "step": 17630
    },
    {
      "epoch": 0.6964899119516721,
      "grad_norm": 0.6379428505897522,
      "learning_rate": 6.517747858017136e-05,
      "loss": 0.8095,
      "step": 17640
    },
    {
      "epoch": 0.6968847475026652,
      "grad_norm": 0.6279134750366211,
      "learning_rate": 6.515773680262172e-05,
      "loss": 0.7999,
      "step": 17650
    },
    {
      "epoch": 0.6972795830536581,
      "grad_norm": 0.8135433793067932,
      "learning_rate": 6.513799502507206e-05,
      "loss": 0.7467,
      "step": 17660
    },
    {
      "epoch": 0.6976744186046512,
      "grad_norm": 0.6758411526679993,
      "learning_rate": 6.511825324752241e-05,
      "loss": 0.7836,
      "step": 17670
    },
    {
      "epoch": 0.6980692541556441,
      "grad_norm": 0.6904892325401306,
      "learning_rate": 6.509851146997276e-05,
      "loss": 0.7564,
      "step": 17680
    },
    {
      "epoch": 0.6984640897066372,
      "grad_norm": 0.6118103861808777,
      "learning_rate": 6.507876969242311e-05,
      "loss": 0.7401,
      "step": 17690
    },
    {
      "epoch": 0.6988589252576302,
      "grad_norm": 0.6606036424636841,
      "learning_rate": 6.505902791487346e-05,
      "loss": 0.7618,
      "step": 17700
    },
    {
      "epoch": 0.6992537608086232,
      "grad_norm": 0.6415766477584839,
      "learning_rate": 6.503928613732381e-05,
      "loss": 0.7329,
      "step": 17710
    },
    {
      "epoch": 0.6996485963596162,
      "grad_norm": 0.6822984218597412,
      "learning_rate": 6.501954435977416e-05,
      "loss": 0.7586,
      "step": 17720
    },
    {
      "epoch": 0.7000434319106092,
      "grad_norm": 0.6172343492507935,
      "learning_rate": 6.499980258222451e-05,
      "loss": 0.7597,
      "step": 17730
    },
    {
      "epoch": 0.7004382674616022,
      "grad_norm": 0.6196884512901306,
      "learning_rate": 6.498006080467486e-05,
      "loss": 0.7423,
      "step": 17740
    },
    {
      "epoch": 0.7008331030125953,
      "grad_norm": 0.75420743227005,
      "learning_rate": 6.496031902712521e-05,
      "loss": 0.7453,
      "step": 17750
    },
    {
      "epoch": 0.7012279385635882,
      "grad_norm": 0.6596104502677917,
      "learning_rate": 6.494057724957556e-05,
      "loss": 0.7688,
      "step": 17760
    },
    {
      "epoch": 0.7016227741145813,
      "grad_norm": 0.7179462909698486,
      "learning_rate": 6.492083547202591e-05,
      "loss": 0.7908,
      "step": 17770
    },
    {
      "epoch": 0.7020176096655742,
      "grad_norm": 0.5742068290710449,
      "learning_rate": 6.490109369447626e-05,
      "loss": 0.7461,
      "step": 17780
    },
    {
      "epoch": 0.7024124452165673,
      "grad_norm": 0.73685222864151,
      "learning_rate": 6.488135191692661e-05,
      "loss": 0.8017,
      "step": 17790
    },
    {
      "epoch": 0.7028072807675603,
      "grad_norm": 0.6375583410263062,
      "learning_rate": 6.486161013937694e-05,
      "loss": 0.7551,
      "step": 17800
    },
    {
      "epoch": 0.7032021163185533,
      "grad_norm": 0.6169838905334473,
      "learning_rate": 6.484186836182731e-05,
      "loss": 0.7275,
      "step": 17810
    },
    {
      "epoch": 0.7035969518695463,
      "grad_norm": 0.58539879322052,
      "learning_rate": 6.482212658427766e-05,
      "loss": 0.753,
      "step": 17820
    },
    {
      "epoch": 0.7039917874205394,
      "grad_norm": 0.5705277919769287,
      "learning_rate": 6.480238480672801e-05,
      "loss": 0.7993,
      "step": 17830
    },
    {
      "epoch": 0.7043866229715323,
      "grad_norm": 0.5821515917778015,
      "learning_rate": 6.478264302917834e-05,
      "loss": 0.7731,
      "step": 17840
    },
    {
      "epoch": 0.7047814585225254,
      "grad_norm": 0.6159569025039673,
      "learning_rate": 6.476290125162871e-05,
      "loss": 0.7477,
      "step": 17850
    },
    {
      "epoch": 0.7051762940735183,
      "grad_norm": 0.8114421963691711,
      "learning_rate": 6.474315947407906e-05,
      "loss": 0.7584,
      "step": 17860
    },
    {
      "epoch": 0.7055711296245114,
      "grad_norm": 0.761249840259552,
      "learning_rate": 6.472341769652939e-05,
      "loss": 0.7863,
      "step": 17870
    },
    {
      "epoch": 0.7059659651755044,
      "grad_norm": 0.6976077556610107,
      "learning_rate": 6.470367591897974e-05,
      "loss": 0.7628,
      "step": 17880
    },
    {
      "epoch": 0.7063608007264974,
      "grad_norm": 0.679200291633606,
      "learning_rate": 6.46839341414301e-05,
      "loss": 0.7212,
      "step": 17890
    },
    {
      "epoch": 0.7067556362774904,
      "grad_norm": 0.6866273283958435,
      "learning_rate": 6.466419236388044e-05,
      "loss": 0.787,
      "step": 17900
    },
    {
      "epoch": 0.7071504718284835,
      "grad_norm": 0.5398581027984619,
      "learning_rate": 6.464445058633079e-05,
      "loss": 0.7927,
      "step": 17910
    },
    {
      "epoch": 0.7075453073794764,
      "grad_norm": 0.7390565276145935,
      "learning_rate": 6.462470880878114e-05,
      "loss": 0.743,
      "step": 17920
    },
    {
      "epoch": 0.7079401429304695,
      "grad_norm": 0.6303054094314575,
      "learning_rate": 6.46049670312315e-05,
      "loss": 0.8099,
      "step": 17930
    },
    {
      "epoch": 0.7083349784814624,
      "grad_norm": 0.6801662445068359,
      "learning_rate": 6.458522525368184e-05,
      "loss": 0.7367,
      "step": 17940
    },
    {
      "epoch": 0.7087298140324555,
      "grad_norm": 0.590650737285614,
      "learning_rate": 6.456548347613219e-05,
      "loss": 0.7867,
      "step": 17950
    },
    {
      "epoch": 0.7091246495834485,
      "grad_norm": 0.6457818150520325,
      "learning_rate": 6.454574169858254e-05,
      "loss": 0.7896,
      "step": 17960
    },
    {
      "epoch": 0.7095194851344415,
      "grad_norm": 0.853594183921814,
      "learning_rate": 6.452599992103289e-05,
      "loss": 0.7965,
      "step": 17970
    },
    {
      "epoch": 0.7099143206854345,
      "grad_norm": 0.7668795585632324,
      "learning_rate": 6.450625814348324e-05,
      "loss": 0.7198,
      "step": 17980
    },
    {
      "epoch": 0.7103091562364275,
      "grad_norm": 0.7476910948753357,
      "learning_rate": 6.448651636593359e-05,
      "loss": 0.7656,
      "step": 17990
    },
    {
      "epoch": 0.7107039917874205,
      "grad_norm": 0.683815062046051,
      "learning_rate": 6.446677458838394e-05,
      "loss": 0.7518,
      "step": 18000
    },
    {
      "epoch": 0.7107039917874205,
      "eval_loss": 0.7861260175704956,
      "eval_runtime": 1218.4871,
      "eval_samples_per_second": 9.239,
      "eval_steps_per_second": 9.239,
      "step": 18000
    },
    {
      "epoch": 0.7110988273384136,
      "grad_norm": 1.095716953277588,
      "learning_rate": 6.444703281083429e-05,
      "loss": 0.7763,
      "step": 18010
    },
    {
      "epoch": 0.7114936628894065,
      "grad_norm": 0.5966769456863403,
      "learning_rate": 6.442729103328464e-05,
      "loss": 0.758,
      "step": 18020
    },
    {
      "epoch": 0.7118884984403996,
      "grad_norm": 0.7373835444450378,
      "learning_rate": 6.440754925573499e-05,
      "loss": 0.7637,
      "step": 18030
    },
    {
      "epoch": 0.7122833339913925,
      "grad_norm": 0.9630817770957947,
      "learning_rate": 6.438780747818534e-05,
      "loss": 0.8025,
      "step": 18040
    },
    {
      "epoch": 0.7126781695423856,
      "grad_norm": 0.6496232151985168,
      "learning_rate": 6.436806570063569e-05,
      "loss": 0.7589,
      "step": 18050
    },
    {
      "epoch": 0.7130730050933786,
      "grad_norm": 0.6424559354782104,
      "learning_rate": 6.434832392308604e-05,
      "loss": 0.7689,
      "step": 18060
    },
    {
      "epoch": 0.7134678406443716,
      "grad_norm": 0.7560884952545166,
      "learning_rate": 6.432858214553639e-05,
      "loss": 0.7927,
      "step": 18070
    },
    {
      "epoch": 0.7138626761953646,
      "grad_norm": 0.7342326045036316,
      "learning_rate": 6.430884036798673e-05,
      "loss": 0.7137,
      "step": 18080
    },
    {
      "epoch": 0.7142575117463577,
      "grad_norm": 0.9626564383506775,
      "learning_rate": 6.428909859043709e-05,
      "loss": 0.785,
      "step": 18090
    },
    {
      "epoch": 0.7146523472973506,
      "grad_norm": 0.6173802614212036,
      "learning_rate": 6.426935681288744e-05,
      "loss": 0.7467,
      "step": 18100
    },
    {
      "epoch": 0.7150471828483437,
      "grad_norm": 0.6491531133651733,
      "learning_rate": 6.424961503533779e-05,
      "loss": 0.806,
      "step": 18110
    },
    {
      "epoch": 0.7154420183993366,
      "grad_norm": 0.6458175182342529,
      "learning_rate": 6.422987325778813e-05,
      "loss": 0.7835,
      "step": 18120
    },
    {
      "epoch": 0.7158368539503297,
      "grad_norm": 0.7415094375610352,
      "learning_rate": 6.421013148023849e-05,
      "loss": 0.7618,
      "step": 18130
    },
    {
      "epoch": 0.7162316895013227,
      "grad_norm": 0.7003722190856934,
      "learning_rate": 6.419038970268884e-05,
      "loss": 0.7861,
      "step": 18140
    },
    {
      "epoch": 0.7166265250523157,
      "grad_norm": 0.6908542513847351,
      "learning_rate": 6.417064792513918e-05,
      "loss": 0.7469,
      "step": 18150
    },
    {
      "epoch": 0.7170213606033087,
      "grad_norm": 0.6911394000053406,
      "learning_rate": 6.415090614758953e-05,
      "loss": 0.7681,
      "step": 18160
    },
    {
      "epoch": 0.7174161961543017,
      "grad_norm": 0.6577691435813904,
      "learning_rate": 6.413116437003989e-05,
      "loss": 0.7273,
      "step": 18170
    },
    {
      "epoch": 0.7178110317052947,
      "grad_norm": 0.5489096641540527,
      "learning_rate": 6.411142259249023e-05,
      "loss": 0.7554,
      "step": 18180
    },
    {
      "epoch": 0.7182058672562878,
      "grad_norm": 0.6111535429954529,
      "learning_rate": 6.409168081494058e-05,
      "loss": 0.7585,
      "step": 18190
    },
    {
      "epoch": 0.7186007028072807,
      "grad_norm": 0.9070587158203125,
      "learning_rate": 6.407193903739093e-05,
      "loss": 0.7259,
      "step": 18200
    },
    {
      "epoch": 0.7189955383582738,
      "grad_norm": 0.5707655549049377,
      "learning_rate": 6.405219725984129e-05,
      "loss": 0.759,
      "step": 18210
    },
    {
      "epoch": 0.7193903739092667,
      "grad_norm": 0.6399791240692139,
      "learning_rate": 6.403245548229163e-05,
      "loss": 0.7609,
      "step": 18220
    },
    {
      "epoch": 0.7197852094602598,
      "grad_norm": 0.5706924200057983,
      "learning_rate": 6.401271370474198e-05,
      "loss": 0.7415,
      "step": 18230
    },
    {
      "epoch": 0.7201800450112528,
      "grad_norm": 0.8226593136787415,
      "learning_rate": 6.399297192719233e-05,
      "loss": 0.7219,
      "step": 18240
    },
    {
      "epoch": 0.7205748805622458,
      "grad_norm": 0.6754403114318848,
      "learning_rate": 6.397323014964268e-05,
      "loss": 0.7674,
      "step": 18250
    },
    {
      "epoch": 0.7209697161132388,
      "grad_norm": 0.3859657049179077,
      "learning_rate": 6.395348837209303e-05,
      "loss": 0.7789,
      "step": 18260
    },
    {
      "epoch": 0.7213645516642319,
      "grad_norm": 0.7976656556129456,
      "learning_rate": 6.393374659454338e-05,
      "loss": 0.7949,
      "step": 18270
    },
    {
      "epoch": 0.7217593872152248,
      "grad_norm": 0.9016116261482239,
      "learning_rate": 6.391400481699373e-05,
      "loss": 0.8047,
      "step": 18280
    },
    {
      "epoch": 0.7221542227662179,
      "grad_norm": 0.5744324922561646,
      "learning_rate": 6.389426303944408e-05,
      "loss": 0.7602,
      "step": 18290
    },
    {
      "epoch": 0.7225490583172108,
      "grad_norm": 0.6575122475624084,
      "learning_rate": 6.387452126189443e-05,
      "loss": 0.743,
      "step": 18300
    },
    {
      "epoch": 0.7229438938682039,
      "grad_norm": 0.7184702157974243,
      "learning_rate": 6.385477948434478e-05,
      "loss": 0.7509,
      "step": 18310
    },
    {
      "epoch": 0.7233387294191969,
      "grad_norm": 0.6900597214698792,
      "learning_rate": 6.383503770679513e-05,
      "loss": 0.7571,
      "step": 18320
    },
    {
      "epoch": 0.7237335649701899,
      "grad_norm": 0.6855784058570862,
      "learning_rate": 6.381529592924548e-05,
      "loss": 0.7928,
      "step": 18330
    },
    {
      "epoch": 0.7241284005211829,
      "grad_norm": 0.7796950936317444,
      "learning_rate": 6.379555415169583e-05,
      "loss": 0.7474,
      "step": 18340
    },
    {
      "epoch": 0.724523236072176,
      "grad_norm": 0.684313178062439,
      "learning_rate": 6.377581237414618e-05,
      "loss": 0.7318,
      "step": 18350
    },
    {
      "epoch": 0.7249180716231689,
      "grad_norm": 0.5241828560829163,
      "learning_rate": 6.375607059659651e-05,
      "loss": 0.7589,
      "step": 18360
    },
    {
      "epoch": 0.725312907174162,
      "grad_norm": 0.6920871734619141,
      "learning_rate": 6.373632881904688e-05,
      "loss": 0.7866,
      "step": 18370
    },
    {
      "epoch": 0.7257077427251549,
      "grad_norm": 0.5664311051368713,
      "learning_rate": 6.371658704149723e-05,
      "loss": 0.7626,
      "step": 18380
    },
    {
      "epoch": 0.726102578276148,
      "grad_norm": 0.6589643359184265,
      "learning_rate": 6.369684526394758e-05,
      "loss": 0.7551,
      "step": 18390
    },
    {
      "epoch": 0.726497413827141,
      "grad_norm": 0.6881661415100098,
      "learning_rate": 6.367710348639791e-05,
      "loss": 0.763,
      "step": 18400
    },
    {
      "epoch": 0.726892249378134,
      "grad_norm": 0.6480695605278015,
      "learning_rate": 6.365736170884827e-05,
      "loss": 0.7946,
      "step": 18410
    },
    {
      "epoch": 0.727287084929127,
      "grad_norm": 0.6484044194221497,
      "learning_rate": 6.363761993129862e-05,
      "loss": 0.7756,
      "step": 18420
    },
    {
      "epoch": 0.72768192048012,
      "grad_norm": 0.9483808875083923,
      "learning_rate": 6.361787815374896e-05,
      "loss": 0.7677,
      "step": 18430
    },
    {
      "epoch": 0.728076756031113,
      "grad_norm": 0.7001683115959167,
      "learning_rate": 6.359813637619931e-05,
      "loss": 0.7579,
      "step": 18440
    },
    {
      "epoch": 0.7284715915821061,
      "grad_norm": 0.5634722709655762,
      "learning_rate": 6.357839459864967e-05,
      "loss": 0.7605,
      "step": 18450
    },
    {
      "epoch": 0.728866427133099,
      "grad_norm": 0.5831778645515442,
      "learning_rate": 6.355865282110001e-05,
      "loss": 0.7234,
      "step": 18460
    },
    {
      "epoch": 0.7292612626840921,
      "grad_norm": 0.6008062958717346,
      "learning_rate": 6.353891104355036e-05,
      "loss": 0.7178,
      "step": 18470
    },
    {
      "epoch": 0.729656098235085,
      "grad_norm": 0.7102906107902527,
      "learning_rate": 6.351916926600071e-05,
      "loss": 0.7762,
      "step": 18480
    },
    {
      "epoch": 0.7300509337860781,
      "grad_norm": 0.6493794918060303,
      "learning_rate": 6.349942748845107e-05,
      "loss": 0.7892,
      "step": 18490
    },
    {
      "epoch": 0.7304457693370711,
      "grad_norm": 0.7076886892318726,
      "learning_rate": 6.347968571090141e-05,
      "loss": 0.7704,
      "step": 18500
    },
    {
      "epoch": 0.7308406048880641,
      "grad_norm": 1.2430285215377808,
      "learning_rate": 6.345994393335176e-05,
      "loss": 0.7876,
      "step": 18510
    },
    {
      "epoch": 0.7312354404390571,
      "grad_norm": 0.6408206224441528,
      "learning_rate": 6.344020215580211e-05,
      "loss": 0.7502,
      "step": 18520
    },
    {
      "epoch": 0.7316302759900501,
      "grad_norm": 0.6193190217018127,
      "learning_rate": 6.342046037825246e-05,
      "loss": 0.7489,
      "step": 18530
    },
    {
      "epoch": 0.7320251115410431,
      "grad_norm": 0.724234402179718,
      "learning_rate": 6.340071860070281e-05,
      "loss": 0.7416,
      "step": 18540
    },
    {
      "epoch": 0.7324199470920362,
      "grad_norm": 0.6897169947624207,
      "learning_rate": 6.338097682315316e-05,
      "loss": 0.695,
      "step": 18550
    },
    {
      "epoch": 0.7328147826430291,
      "grad_norm": 0.7056193351745605,
      "learning_rate": 6.336123504560351e-05,
      "loss": 0.7499,
      "step": 18560
    },
    {
      "epoch": 0.7332096181940222,
      "grad_norm": 0.7064064741134644,
      "learning_rate": 6.334149326805385e-05,
      "loss": 0.7448,
      "step": 18570
    },
    {
      "epoch": 0.7336044537450152,
      "grad_norm": 0.723278284072876,
      "learning_rate": 6.332175149050421e-05,
      "loss": 0.7661,
      "step": 18580
    },
    {
      "epoch": 0.7339992892960082,
      "grad_norm": 0.5164334774017334,
      "learning_rate": 6.330200971295456e-05,
      "loss": 0.7811,
      "step": 18590
    },
    {
      "epoch": 0.7343941248470012,
      "grad_norm": 0.7370377779006958,
      "learning_rate": 6.328226793540491e-05,
      "loss": 0.7382,
      "step": 18600
    },
    {
      "epoch": 0.7347889603979942,
      "grad_norm": 0.7782073616981506,
      "learning_rate": 6.326252615785525e-05,
      "loss": 0.7666,
      "step": 18610
    },
    {
      "epoch": 0.7351837959489872,
      "grad_norm": 0.6590379476547241,
      "learning_rate": 6.324278438030561e-05,
      "loss": 0.7291,
      "step": 18620
    },
    {
      "epoch": 0.7355786314999803,
      "grad_norm": 1.0856823921203613,
      "learning_rate": 6.322304260275596e-05,
      "loss": 0.7485,
      "step": 18630
    },
    {
      "epoch": 0.7359734670509732,
      "grad_norm": 0.6078216433525085,
      "learning_rate": 6.32033008252063e-05,
      "loss": 0.7466,
      "step": 18640
    },
    {
      "epoch": 0.7363683026019663,
      "grad_norm": 0.6494882702827454,
      "learning_rate": 6.318355904765665e-05,
      "loss": 0.8141,
      "step": 18650
    },
    {
      "epoch": 0.7367631381529592,
      "grad_norm": 0.6244936585426331,
      "learning_rate": 6.316381727010701e-05,
      "loss": 0.7494,
      "step": 18660
    },
    {
      "epoch": 0.7371579737039523,
      "grad_norm": 0.6882637143135071,
      "learning_rate": 6.314407549255736e-05,
      "loss": 0.7458,
      "step": 18670
    },
    {
      "epoch": 0.7375528092549453,
      "grad_norm": 0.6099334359169006,
      "learning_rate": 6.31243337150077e-05,
      "loss": 0.7844,
      "step": 18680
    },
    {
      "epoch": 0.7379476448059383,
      "grad_norm": 0.69880211353302,
      "learning_rate": 6.310459193745805e-05,
      "loss": 0.7572,
      "step": 18690
    },
    {
      "epoch": 0.7383424803569313,
      "grad_norm": 0.7939314842224121,
      "learning_rate": 6.308485015990841e-05,
      "loss": 0.7668,
      "step": 18700
    },
    {
      "epoch": 0.7387373159079244,
      "grad_norm": 0.6245890259742737,
      "learning_rate": 6.306510838235875e-05,
      "loss": 0.7863,
      "step": 18710
    },
    {
      "epoch": 0.7391321514589173,
      "grad_norm": 0.7767401933670044,
      "learning_rate": 6.30453666048091e-05,
      "loss": 0.76,
      "step": 18720
    },
    {
      "epoch": 0.7395269870099104,
      "grad_norm": 0.9237650036811829,
      "learning_rate": 6.302562482725945e-05,
      "loss": 0.7852,
      "step": 18730
    },
    {
      "epoch": 0.7399218225609033,
      "grad_norm": 0.5962645411491394,
      "learning_rate": 6.300588304970981e-05,
      "loss": 0.746,
      "step": 18740
    },
    {
      "epoch": 0.7403166581118964,
      "grad_norm": 0.6350162625312805,
      "learning_rate": 6.298614127216014e-05,
      "loss": 0.7858,
      "step": 18750
    },
    {
      "epoch": 0.7407114936628894,
      "grad_norm": 0.5364828705787659,
      "learning_rate": 6.29663994946105e-05,
      "loss": 0.7589,
      "step": 18760
    },
    {
      "epoch": 0.7411063292138824,
      "grad_norm": 0.8219362497329712,
      "learning_rate": 6.294665771706084e-05,
      "loss": 0.7464,
      "step": 18770
    },
    {
      "epoch": 0.7415011647648754,
      "grad_norm": 0.8007829785346985,
      "learning_rate": 6.29269159395112e-05,
      "loss": 0.778,
      "step": 18780
    },
    {
      "epoch": 0.7418960003158684,
      "grad_norm": 0.6769288778305054,
      "learning_rate": 6.290717416196154e-05,
      "loss": 0.7475,
      "step": 18790
    },
    {
      "epoch": 0.7422908358668614,
      "grad_norm": 0.682543158531189,
      "learning_rate": 6.28874323844119e-05,
      "loss": 0.7655,
      "step": 18800
    },
    {
      "epoch": 0.7426856714178545,
      "grad_norm": 0.680316686630249,
      "learning_rate": 6.286769060686224e-05,
      "loss": 0.7121,
      "step": 18810
    },
    {
      "epoch": 0.7430805069688474,
      "grad_norm": 0.6567206382751465,
      "learning_rate": 6.28479488293126e-05,
      "loss": 0.7844,
      "step": 18820
    },
    {
      "epoch": 0.7434753425198405,
      "grad_norm": 0.6552153825759888,
      "learning_rate": 6.282820705176294e-05,
      "loss": 0.7884,
      "step": 18830
    },
    {
      "epoch": 0.7438701780708334,
      "grad_norm": 0.5915831327438354,
      "learning_rate": 6.28084652742133e-05,
      "loss": 0.786,
      "step": 18840
    },
    {
      "epoch": 0.7442650136218265,
      "grad_norm": 0.673895537853241,
      "learning_rate": 6.278872349666364e-05,
      "loss": 0.7786,
      "step": 18850
    },
    {
      "epoch": 0.7446598491728195,
      "grad_norm": 0.9115302562713623,
      "learning_rate": 6.2768981719114e-05,
      "loss": 0.7735,
      "step": 18860
    },
    {
      "epoch": 0.7450546847238125,
      "grad_norm": 0.6455349922180176,
      "learning_rate": 6.274923994156434e-05,
      "loss": 0.7522,
      "step": 18870
    },
    {
      "epoch": 0.7454495202748055,
      "grad_norm": 0.6219595670700073,
      "learning_rate": 6.27294981640147e-05,
      "loss": 0.7643,
      "step": 18880
    },
    {
      "epoch": 0.7458443558257986,
      "grad_norm": 0.617177426815033,
      "learning_rate": 6.270975638646503e-05,
      "loss": 0.7425,
      "step": 18890
    },
    {
      "epoch": 0.7462391913767915,
      "grad_norm": 0.5584836602210999,
      "learning_rate": 6.26900146089154e-05,
      "loss": 0.7453,
      "step": 18900
    },
    {
      "epoch": 0.7466340269277846,
      "grad_norm": 0.6233077645301819,
      "learning_rate": 6.267027283136574e-05,
      "loss": 0.7551,
      "step": 18910
    },
    {
      "epoch": 0.7470288624787776,
      "grad_norm": 0.5937451124191284,
      "learning_rate": 6.265053105381608e-05,
      "loss": 0.7985,
      "step": 18920
    },
    {
      "epoch": 0.7474236980297706,
      "grad_norm": 0.7775344252586365,
      "learning_rate": 6.263078927626643e-05,
      "loss": 0.7151,
      "step": 18930
    },
    {
      "epoch": 0.7478185335807637,
      "grad_norm": 0.6273540258407593,
      "learning_rate": 6.261104749871679e-05,
      "loss": 0.7335,
      "step": 18940
    },
    {
      "epoch": 0.7482133691317566,
      "grad_norm": 0.5692470669746399,
      "learning_rate": 6.259130572116714e-05,
      "loss": 0.8113,
      "step": 18950
    },
    {
      "epoch": 0.7486082046827497,
      "grad_norm": 0.7156858444213867,
      "learning_rate": 6.257156394361748e-05,
      "loss": 0.7446,
      "step": 18960
    },
    {
      "epoch": 0.7490030402337426,
      "grad_norm": 0.74570631980896,
      "learning_rate": 6.255182216606783e-05,
      "loss": 0.7535,
      "step": 18970
    },
    {
      "epoch": 0.7493978757847357,
      "grad_norm": 0.6567177176475525,
      "learning_rate": 6.253208038851819e-05,
      "loss": 0.7364,
      "step": 18980
    },
    {
      "epoch": 0.7497927113357287,
      "grad_norm": 0.6204395890235901,
      "learning_rate": 6.251233861096853e-05,
      "loss": 0.7708,
      "step": 18990
    },
    {
      "epoch": 0.7501875468867217,
      "grad_norm": 0.723355770111084,
      "learning_rate": 6.249259683341888e-05,
      "loss": 0.7791,
      "step": 19000
    },
    {
      "epoch": 0.7501875468867217,
      "eval_loss": 0.7847444415092468,
      "eval_runtime": 1217.9788,
      "eval_samples_per_second": 9.242,
      "eval_steps_per_second": 9.242,
      "step": 19000
    },
    {
      "epoch": 0.7505823824377147,
      "grad_norm": 0.6278022527694702,
      "learning_rate": 6.247285505586923e-05,
      "loss": 0.756,
      "step": 19010
    },
    {
      "epoch": 0.7509772179887078,
      "grad_norm": 0.8105144500732422,
      "learning_rate": 6.245311327831959e-05,
      "loss": 0.7444,
      "step": 19020
    },
    {
      "epoch": 0.7513720535397007,
      "grad_norm": 0.6481295228004456,
      "learning_rate": 6.243337150076993e-05,
      "loss": 0.7476,
      "step": 19030
    },
    {
      "epoch": 0.7517668890906938,
      "grad_norm": 0.6100083589553833,
      "learning_rate": 6.241362972322028e-05,
      "loss": 0.736,
      "step": 19040
    },
    {
      "epoch": 0.7521617246416867,
      "grad_norm": 0.569666862487793,
      "learning_rate": 6.239388794567063e-05,
      "loss": 0.7558,
      "step": 19050
    },
    {
      "epoch": 0.7525565601926798,
      "grad_norm": 0.6737748980522156,
      "learning_rate": 6.237414616812098e-05,
      "loss": 0.7373,
      "step": 19060
    },
    {
      "epoch": 0.7529513957436728,
      "grad_norm": 0.6110496520996094,
      "learning_rate": 6.235440439057133e-05,
      "loss": 0.8141,
      "step": 19070
    },
    {
      "epoch": 0.7533462312946658,
      "grad_norm": 0.6720634698867798,
      "learning_rate": 6.233466261302168e-05,
      "loss": 0.7501,
      "step": 19080
    },
    {
      "epoch": 0.7537410668456588,
      "grad_norm": 0.5708483457565308,
      "learning_rate": 6.231492083547203e-05,
      "loss": 0.7605,
      "step": 19090
    },
    {
      "epoch": 0.7541359023966518,
      "grad_norm": 0.6211496591567993,
      "learning_rate": 6.229517905792238e-05,
      "loss": 0.8052,
      "step": 19100
    },
    {
      "epoch": 0.7545307379476448,
      "grad_norm": 0.7307137846946716,
      "learning_rate": 6.227543728037273e-05,
      "loss": 0.7888,
      "step": 19110
    },
    {
      "epoch": 0.7549255734986379,
      "grad_norm": 0.6613839268684387,
      "learning_rate": 6.225569550282308e-05,
      "loss": 0.7608,
      "step": 19120
    },
    {
      "epoch": 0.7553204090496308,
      "grad_norm": 0.6751605868339539,
      "learning_rate": 6.223595372527343e-05,
      "loss": 0.7458,
      "step": 19130
    },
    {
      "epoch": 0.7557152446006239,
      "grad_norm": 0.7446117997169495,
      "learning_rate": 6.221621194772378e-05,
      "loss": 0.7382,
      "step": 19140
    },
    {
      "epoch": 0.7561100801516168,
      "grad_norm": 0.6871846318244934,
      "learning_rate": 6.219647017017413e-05,
      "loss": 0.7848,
      "step": 19150
    },
    {
      "epoch": 0.7565049157026099,
      "grad_norm": 0.8401902318000793,
      "learning_rate": 6.217672839262448e-05,
      "loss": 0.7961,
      "step": 19160
    },
    {
      "epoch": 0.7568997512536029,
      "grad_norm": 0.6197373270988464,
      "learning_rate": 6.215698661507481e-05,
      "loss": 0.7589,
      "step": 19170
    },
    {
      "epoch": 0.7572945868045959,
      "grad_norm": 0.5927439332008362,
      "learning_rate": 6.213724483752518e-05,
      "loss": 0.754,
      "step": 19180
    },
    {
      "epoch": 0.7576894223555889,
      "grad_norm": 0.7949334979057312,
      "learning_rate": 6.211750305997553e-05,
      "loss": 0.7743,
      "step": 19190
    },
    {
      "epoch": 0.758084257906582,
      "grad_norm": 0.6682984232902527,
      "learning_rate": 6.209776128242586e-05,
      "loss": 0.7566,
      "step": 19200
    },
    {
      "epoch": 0.7584790934575749,
      "grad_norm": 0.7019145488739014,
      "learning_rate": 6.207801950487621e-05,
      "loss": 0.7484,
      "step": 19210
    },
    {
      "epoch": 0.758873929008568,
      "grad_norm": 0.6040531992912292,
      "learning_rate": 6.205827772732658e-05,
      "loss": 0.7456,
      "step": 19220
    },
    {
      "epoch": 0.7592687645595609,
      "grad_norm": 1.1745989322662354,
      "learning_rate": 6.203853594977693e-05,
      "loss": 0.7978,
      "step": 19230
    },
    {
      "epoch": 0.759663600110554,
      "grad_norm": 0.6935317516326904,
      "learning_rate": 6.201879417222726e-05,
      "loss": 0.7483,
      "step": 19240
    },
    {
      "epoch": 0.760058435661547,
      "grad_norm": 0.7078613638877869,
      "learning_rate": 6.199905239467761e-05,
      "loss": 0.7713,
      "step": 19250
    },
    {
      "epoch": 0.76045327121254,
      "grad_norm": 0.6338851451873779,
      "learning_rate": 6.197931061712798e-05,
      "loss": 0.7659,
      "step": 19260
    },
    {
      "epoch": 0.760848106763533,
      "grad_norm": 0.6895703077316284,
      "learning_rate": 6.195956883957831e-05,
      "loss": 0.7234,
      "step": 19270
    },
    {
      "epoch": 0.761242942314526,
      "grad_norm": 0.6556699275970459,
      "learning_rate": 6.193982706202866e-05,
      "loss": 0.7558,
      "step": 19280
    },
    {
      "epoch": 0.761637777865519,
      "grad_norm": 0.5607295036315918,
      "learning_rate": 6.192008528447901e-05,
      "loss": 0.747,
      "step": 19290
    },
    {
      "epoch": 0.7620326134165121,
      "grad_norm": 0.7927216291427612,
      "learning_rate": 6.190034350692938e-05,
      "loss": 0.7822,
      "step": 19300
    },
    {
      "epoch": 0.762427448967505,
      "grad_norm": 0.6981233954429626,
      "learning_rate": 6.188060172937971e-05,
      "loss": 0.751,
      "step": 19310
    },
    {
      "epoch": 0.7628222845184981,
      "grad_norm": 0.8553351163864136,
      "learning_rate": 6.186085995183006e-05,
      "loss": 0.791,
      "step": 19320
    },
    {
      "epoch": 0.763217120069491,
      "grad_norm": 0.7939368486404419,
      "learning_rate": 6.184111817428041e-05,
      "loss": 0.7477,
      "step": 19330
    },
    {
      "epoch": 0.7636119556204841,
      "grad_norm": 0.5649716258049011,
      "learning_rate": 6.182137639673076e-05,
      "loss": 0.7493,
      "step": 19340
    },
    {
      "epoch": 0.7640067911714771,
      "grad_norm": 0.9252603650093079,
      "learning_rate": 6.180163461918111e-05,
      "loss": 0.8151,
      "step": 19350
    },
    {
      "epoch": 0.7644016267224701,
      "grad_norm": 0.5876927971839905,
      "learning_rate": 6.178189284163146e-05,
      "loss": 0.7261,
      "step": 19360
    },
    {
      "epoch": 0.7647964622734631,
      "grad_norm": 0.713848888874054,
      "learning_rate": 6.176215106408181e-05,
      "loss": 0.776,
      "step": 19370
    },
    {
      "epoch": 0.7651912978244562,
      "grad_norm": 0.6257109642028809,
      "learning_rate": 6.174240928653216e-05,
      "loss": 0.7617,
      "step": 19380
    },
    {
      "epoch": 0.7655861333754491,
      "grad_norm": 0.6629334688186646,
      "learning_rate": 6.172266750898251e-05,
      "loss": 0.7448,
      "step": 19390
    },
    {
      "epoch": 0.7659809689264422,
      "grad_norm": 0.6310097575187683,
      "learning_rate": 6.170292573143286e-05,
      "loss": 0.7637,
      "step": 19400
    },
    {
      "epoch": 0.7663758044774351,
      "grad_norm": 0.6626302003860474,
      "learning_rate": 6.168318395388321e-05,
      "loss": 0.7955,
      "step": 19410
    },
    {
      "epoch": 0.7667706400284282,
      "grad_norm": 0.698698878288269,
      "learning_rate": 6.166344217633356e-05,
      "loss": 0.7623,
      "step": 19420
    },
    {
      "epoch": 0.7671654755794212,
      "grad_norm": 0.5064301490783691,
      "learning_rate": 6.164370039878391e-05,
      "loss": 0.757,
      "step": 19430
    },
    {
      "epoch": 0.7675603111304142,
      "grad_norm": 0.7587046027183533,
      "learning_rate": 6.162395862123426e-05,
      "loss": 0.7326,
      "step": 19440
    },
    {
      "epoch": 0.7679551466814072,
      "grad_norm": 0.5835659503936768,
      "learning_rate": 6.16042168436846e-05,
      "loss": 0.7717,
      "step": 19450
    },
    {
      "epoch": 0.7683499822324003,
      "grad_norm": 0.6613929867744446,
      "learning_rate": 6.158447506613496e-05,
      "loss": 0.7388,
      "step": 19460
    },
    {
      "epoch": 0.7687448177833932,
      "grad_norm": 0.8434509634971619,
      "learning_rate": 6.156473328858531e-05,
      "loss": 0.7797,
      "step": 19470
    },
    {
      "epoch": 0.7691396533343863,
      "grad_norm": 0.5739575624465942,
      "learning_rate": 6.154499151103565e-05,
      "loss": 0.7634,
      "step": 19480
    },
    {
      "epoch": 0.7695344888853792,
      "grad_norm": 0.713836669921875,
      "learning_rate": 6.1525249733486e-05,
      "loss": 0.7553,
      "step": 19490
    },
    {
      "epoch": 0.7699293244363723,
      "grad_norm": 0.6558735370635986,
      "learning_rate": 6.150550795593636e-05,
      "loss": 0.7307,
      "step": 19500
    },
    {
      "epoch": 0.7703241599873653,
      "grad_norm": 0.5613111853599548,
      "learning_rate": 6.148576617838671e-05,
      "loss": 0.7589,
      "step": 19510
    },
    {
      "epoch": 0.7707189955383583,
      "grad_norm": 0.710249662399292,
      "learning_rate": 6.146602440083705e-05,
      "loss": 0.7371,
      "step": 19520
    },
    {
      "epoch": 0.7711138310893513,
      "grad_norm": 0.770794153213501,
      "learning_rate": 6.14462826232874e-05,
      "loss": 0.7772,
      "step": 19530
    },
    {
      "epoch": 0.7715086666403443,
      "grad_norm": 0.7134220004081726,
      "learning_rate": 6.142654084573776e-05,
      "loss": 0.798,
      "step": 19540
    },
    {
      "epoch": 0.7719035021913373,
      "grad_norm": 0.6515070199966431,
      "learning_rate": 6.14067990681881e-05,
      "loss": 0.7813,
      "step": 19550
    },
    {
      "epoch": 0.7722983377423304,
      "grad_norm": 0.6024087071418762,
      "learning_rate": 6.138705729063845e-05,
      "loss": 0.757,
      "step": 19560
    },
    {
      "epoch": 0.7726931732933233,
      "grad_norm": 0.722319483757019,
      "learning_rate": 6.13673155130888e-05,
      "loss": 0.7908,
      "step": 19570
    },
    {
      "epoch": 0.7730880088443164,
      "grad_norm": 0.5254853367805481,
      "learning_rate": 6.134757373553916e-05,
      "loss": 0.7287,
      "step": 19580
    },
    {
      "epoch": 0.7734828443953093,
      "grad_norm": 0.7155462503433228,
      "learning_rate": 6.13278319579895e-05,
      "loss": 0.7501,
      "step": 19590
    },
    {
      "epoch": 0.7738776799463024,
      "grad_norm": 0.5672827363014221,
      "learning_rate": 6.130809018043985e-05,
      "loss": 0.7622,
      "step": 19600
    },
    {
      "epoch": 0.7742725154972954,
      "grad_norm": 0.6594387292861938,
      "learning_rate": 6.12883484028902e-05,
      "loss": 0.7941,
      "step": 19610
    },
    {
      "epoch": 0.7746673510482884,
      "grad_norm": 0.6447163224220276,
      "learning_rate": 6.126860662534055e-05,
      "loss": 0.7302,
      "step": 19620
    },
    {
      "epoch": 0.7750621865992814,
      "grad_norm": 0.6081201434135437,
      "learning_rate": 6.12488648477909e-05,
      "loss": 0.8068,
      "step": 19630
    },
    {
      "epoch": 0.7754570221502745,
      "grad_norm": 0.6974090337753296,
      "learning_rate": 6.122912307024125e-05,
      "loss": 0.7614,
      "step": 19640
    },
    {
      "epoch": 0.7758518577012674,
      "grad_norm": 0.7168981432914734,
      "learning_rate": 6.12093812926916e-05,
      "loss": 0.7496,
      "step": 19650
    },
    {
      "epoch": 0.7762466932522605,
      "grad_norm": 0.6526238322257996,
      "learning_rate": 6.118963951514195e-05,
      "loss": 0.7226,
      "step": 19660
    },
    {
      "epoch": 0.7766415288032534,
      "grad_norm": 0.6626632213592529,
      "learning_rate": 6.11698977375923e-05,
      "loss": 0.7638,
      "step": 19670
    },
    {
      "epoch": 0.7770363643542465,
      "grad_norm": 0.6690067648887634,
      "learning_rate": 6.115015596004265e-05,
      "loss": 0.7512,
      "step": 19680
    },
    {
      "epoch": 0.7774311999052395,
      "grad_norm": 0.600575864315033,
      "learning_rate": 6.1130414182493e-05,
      "loss": 0.7838,
      "step": 19690
    },
    {
      "epoch": 0.7778260354562325,
      "grad_norm": 0.5928352475166321,
      "learning_rate": 6.111067240494335e-05,
      "loss": 0.7693,
      "step": 19700
    },
    {
      "epoch": 0.7782208710072255,
      "grad_norm": 0.735683798789978,
      "learning_rate": 6.10909306273937e-05,
      "loss": 0.7809,
      "step": 19710
    },
    {
      "epoch": 0.7786157065582185,
      "grad_norm": 1.0375276803970337,
      "learning_rate": 6.107118884984405e-05,
      "loss": 0.7332,
      "step": 19720
    },
    {
      "epoch": 0.7790105421092115,
      "grad_norm": 0.7121360898017883,
      "learning_rate": 6.105144707229438e-05,
      "loss": 0.7436,
      "step": 19730
    },
    {
      "epoch": 0.7794053776602046,
      "grad_norm": 0.514378547668457,
      "learning_rate": 6.1031705294744746e-05,
      "loss": 0.7667,
      "step": 19740
    },
    {
      "epoch": 0.7798002132111975,
      "grad_norm": 0.66032874584198,
      "learning_rate": 6.101196351719509e-05,
      "loss": 0.7581,
      "step": 19750
    },
    {
      "epoch": 0.7801950487621906,
      "grad_norm": 0.5684134364128113,
      "learning_rate": 6.099222173964544e-05,
      "loss": 0.7865,
      "step": 19760
    },
    {
      "epoch": 0.7805898843131835,
      "grad_norm": 0.6462849974632263,
      "learning_rate": 6.097247996209579e-05,
      "loss": 0.7837,
      "step": 19770
    },
    {
      "epoch": 0.7809847198641766,
      "grad_norm": 0.7562693953514099,
      "learning_rate": 6.0952738184546145e-05,
      "loss": 0.7852,
      "step": 19780
    },
    {
      "epoch": 0.7813795554151696,
      "grad_norm": 0.705197811126709,
      "learning_rate": 6.093299640699649e-05,
      "loss": 0.7588,
      "step": 19790
    },
    {
      "epoch": 0.7817743909661626,
      "grad_norm": 0.7588688731193542,
      "learning_rate": 6.091325462944684e-05,
      "loss": 0.7532,
      "step": 19800
    },
    {
      "epoch": 0.7821692265171556,
      "grad_norm": 0.6425011157989502,
      "learning_rate": 6.089351285189718e-05,
      "loss": 0.7463,
      "step": 19810
    },
    {
      "epoch": 0.7825640620681487,
      "grad_norm": 0.7064322233200073,
      "learning_rate": 6.087377107434754e-05,
      "loss": 0.7455,
      "step": 19820
    },
    {
      "epoch": 0.7829588976191416,
      "grad_norm": 0.5821722745895386,
      "learning_rate": 6.085402929679789e-05,
      "loss": 0.701,
      "step": 19830
    },
    {
      "epoch": 0.7833537331701347,
      "grad_norm": 0.7332910895347595,
      "learning_rate": 6.083428751924823e-05,
      "loss": 0.7514,
      "step": 19840
    },
    {
      "epoch": 0.7837485687211276,
      "grad_norm": 0.638238787651062,
      "learning_rate": 6.081454574169858e-05,
      "loss": 0.7474,
      "step": 19850
    },
    {
      "epoch": 0.7841434042721207,
      "grad_norm": 0.6393411755561829,
      "learning_rate": 6.079480396414894e-05,
      "loss": 0.7631,
      "step": 19860
    },
    {
      "epoch": 0.7845382398231137,
      "grad_norm": 0.7689927220344543,
      "learning_rate": 6.077506218659929e-05,
      "loss": 0.7741,
      "step": 19870
    },
    {
      "epoch": 0.7849330753741067,
      "grad_norm": 0.6264607310295105,
      "learning_rate": 6.075532040904963e-05,
      "loss": 0.7961,
      "step": 19880
    },
    {
      "epoch": 0.7853279109250997,
      "grad_norm": 0.675240159034729,
      "learning_rate": 6.073557863149998e-05,
      "loss": 0.7358,
      "step": 19890
    },
    {
      "epoch": 0.7857227464760927,
      "grad_norm": 0.5748541355133057,
      "learning_rate": 6.071583685395034e-05,
      "loss": 0.7439,
      "step": 19900
    },
    {
      "epoch": 0.7861175820270857,
      "grad_norm": 0.8197774887084961,
      "learning_rate": 6.069609507640068e-05,
      "loss": 0.796,
      "step": 19910
    },
    {
      "epoch": 0.7865124175780788,
      "grad_norm": 0.7094137668609619,
      "learning_rate": 6.067635329885103e-05,
      "loss": 0.7739,
      "step": 19920
    },
    {
      "epoch": 0.7869072531290717,
      "grad_norm": 0.6543534398078918,
      "learning_rate": 6.0656611521301374e-05,
      "loss": 0.749,
      "step": 19930
    },
    {
      "epoch": 0.7873020886800648,
      "grad_norm": 1.090155005455017,
      "learning_rate": 6.063686974375174e-05,
      "loss": 0.7546,
      "step": 19940
    },
    {
      "epoch": 0.7876969242310577,
      "grad_norm": 0.6571090817451477,
      "learning_rate": 6.061712796620208e-05,
      "loss": 0.7733,
      "step": 19950
    },
    {
      "epoch": 0.7880917597820508,
      "grad_norm": 0.6080721020698547,
      "learning_rate": 6.059738618865243e-05,
      "loss": 0.7615,
      "step": 19960
    },
    {
      "epoch": 0.7884865953330438,
      "grad_norm": 0.6435588002204895,
      "learning_rate": 6.057764441110277e-05,
      "loss": 0.7859,
      "step": 19970
    },
    {
      "epoch": 0.7888814308840368,
      "grad_norm": 0.6332112550735474,
      "learning_rate": 6.055790263355313e-05,
      "loss": 0.7438,
      "step": 19980
    },
    {
      "epoch": 0.7892762664350298,
      "grad_norm": 0.6428854465484619,
      "learning_rate": 6.053816085600348e-05,
      "loss": 0.7359,
      "step": 19990
    },
    {
      "epoch": 0.7896711019860229,
      "grad_norm": 0.6681484580039978,
      "learning_rate": 6.051841907845382e-05,
      "loss": 0.7919,
      "step": 20000
    },
    {
      "epoch": 0.7896711019860229,
      "eval_loss": 0.7821593880653381,
      "eval_runtime": 1217.9204,
      "eval_samples_per_second": 9.243,
      "eval_steps_per_second": 9.243,
      "step": 20000
    },
    {
      "epoch": 0.7900659375370158,
      "grad_norm": 0.6710469722747803,
      "learning_rate": 6.049867730090417e-05,
      "loss": 0.7999,
      "step": 20010
    },
    {
      "epoch": 0.7904607730880089,
      "grad_norm": 0.5900397896766663,
      "learning_rate": 6.047893552335453e-05,
      "loss": 0.79,
      "step": 20020
    },
    {
      "epoch": 0.7908556086390018,
      "grad_norm": 0.5785884857177734,
      "learning_rate": 6.045919374580488e-05,
      "loss": 0.7604,
      "step": 20030
    },
    {
      "epoch": 0.7912504441899949,
      "grad_norm": 0.6469764709472656,
      "learning_rate": 6.043945196825522e-05,
      "loss": 0.7714,
      "step": 20040
    },
    {
      "epoch": 0.7916452797409879,
      "grad_norm": 0.6849796175956726,
      "learning_rate": 6.041971019070557e-05,
      "loss": 0.7271,
      "step": 20050
    },
    {
      "epoch": 0.7920401152919809,
      "grad_norm": 0.6107760667800903,
      "learning_rate": 6.039996841315593e-05,
      "loss": 0.7754,
      "step": 20060
    },
    {
      "epoch": 0.7924349508429739,
      "grad_norm": 0.6203515529632568,
      "learning_rate": 6.038022663560627e-05,
      "loss": 0.7289,
      "step": 20070
    },
    {
      "epoch": 0.792829786393967,
      "grad_norm": 0.62950599193573,
      "learning_rate": 6.036048485805662e-05,
      "loss": 0.7919,
      "step": 20080
    },
    {
      "epoch": 0.7932246219449599,
      "grad_norm": 0.6968435049057007,
      "learning_rate": 6.0340743080506966e-05,
      "loss": 0.7614,
      "step": 20090
    },
    {
      "epoch": 0.793619457495953,
      "grad_norm": 0.5959716439247131,
      "learning_rate": 6.032100130295732e-05,
      "loss": 0.749,
      "step": 20100
    },
    {
      "epoch": 0.7940142930469459,
      "grad_norm": 0.7982717752456665,
      "learning_rate": 6.030125952540767e-05,
      "loss": 0.8036,
      "step": 20110
    },
    {
      "epoch": 0.794409128597939,
      "grad_norm": 0.6725864410400391,
      "learning_rate": 6.0281517747858015e-05,
      "loss": 0.7582,
      "step": 20120
    },
    {
      "epoch": 0.794803964148932,
      "grad_norm": 0.6614556908607483,
      "learning_rate": 6.0261775970308365e-05,
      "loss": 0.7666,
      "step": 20130
    },
    {
      "epoch": 0.795198799699925,
      "grad_norm": 0.6360943913459778,
      "learning_rate": 6.024203419275872e-05,
      "loss": 0.7594,
      "step": 20140
    },
    {
      "epoch": 0.795593635250918,
      "grad_norm": 0.7128324508666992,
      "learning_rate": 6.022229241520907e-05,
      "loss": 0.8067,
      "step": 20150
    },
    {
      "epoch": 0.795988470801911,
      "grad_norm": 0.6594889760017395,
      "learning_rate": 6.0202550637659415e-05,
      "loss": 0.7983,
      "step": 20160
    },
    {
      "epoch": 0.796383306352904,
      "grad_norm": 0.9090096354484558,
      "learning_rate": 6.0182808860109765e-05,
      "loss": 0.7802,
      "step": 20170
    },
    {
      "epoch": 0.7967781419038971,
      "grad_norm": 0.7087458372116089,
      "learning_rate": 6.016306708256012e-05,
      "loss": 0.7158,
      "step": 20180
    },
    {
      "epoch": 0.79717297745489,
      "grad_norm": 0.7600945234298706,
      "learning_rate": 6.0143325305010465e-05,
      "loss": 0.7363,
      "step": 20190
    },
    {
      "epoch": 0.7975678130058831,
      "grad_norm": 0.6624404191970825,
      "learning_rate": 6.0123583527460815e-05,
      "loss": 0.7245,
      "step": 20200
    },
    {
      "epoch": 0.797962648556876,
      "grad_norm": 0.6831139922142029,
      "learning_rate": 6.010384174991116e-05,
      "loss": 0.7489,
      "step": 20210
    },
    {
      "epoch": 0.7983574841078691,
      "grad_norm": 0.8111611008644104,
      "learning_rate": 6.008409997236152e-05,
      "loss": 0.7705,
      "step": 20220
    },
    {
      "epoch": 0.7987523196588621,
      "grad_norm": 0.6357056498527527,
      "learning_rate": 6.0064358194811864e-05,
      "loss": 0.7388,
      "step": 20230
    },
    {
      "epoch": 0.7991471552098551,
      "grad_norm": 0.769523024559021,
      "learning_rate": 6.0044616417262214e-05,
      "loss": 0.7563,
      "step": 20240
    },
    {
      "epoch": 0.7995419907608481,
      "grad_norm": 0.7264702320098877,
      "learning_rate": 6.002487463971256e-05,
      "loss": 0.7693,
      "step": 20250
    },
    {
      "epoch": 0.7999368263118412,
      "grad_norm": 0.9428048133850098,
      "learning_rate": 6.0005132862162914e-05,
      "loss": 0.7383,
      "step": 20260
    },
    {
      "epoch": 0.8003316618628341,
      "grad_norm": 0.6314225792884827,
      "learning_rate": 5.9985391084613264e-05,
      "loss": 0.7323,
      "step": 20270
    },
    {
      "epoch": 0.8007264974138272,
      "grad_norm": 0.8453285098075867,
      "learning_rate": 5.996564930706361e-05,
      "loss": 0.7649,
      "step": 20280
    },
    {
      "epoch": 0.8011213329648201,
      "grad_norm": 0.7789329290390015,
      "learning_rate": 5.994590752951396e-05,
      "loss": 0.7469,
      "step": 20290
    },
    {
      "epoch": 0.8015161685158132,
      "grad_norm": 0.6780755519866943,
      "learning_rate": 5.9926165751964314e-05,
      "loss": 0.7615,
      "step": 20300
    },
    {
      "epoch": 0.8019110040668062,
      "grad_norm": 0.6032227873802185,
      "learning_rate": 5.9906423974414664e-05,
      "loss": 0.7224,
      "step": 20310
    },
    {
      "epoch": 0.8023058396177992,
      "grad_norm": 0.7345583438873291,
      "learning_rate": 5.988668219686501e-05,
      "loss": 0.7616,
      "step": 20320
    },
    {
      "epoch": 0.8027006751687922,
      "grad_norm": 0.6635563969612122,
      "learning_rate": 5.986694041931536e-05,
      "loss": 0.7711,
      "step": 20330
    },
    {
      "epoch": 0.8030955107197852,
      "grad_norm": 0.6779434084892273,
      "learning_rate": 5.984719864176571e-05,
      "loss": 0.7692,
      "step": 20340
    },
    {
      "epoch": 0.8034903462707782,
      "grad_norm": 0.6019047498703003,
      "learning_rate": 5.9827456864216057e-05,
      "loss": 0.725,
      "step": 20350
    },
    {
      "epoch": 0.8038851818217713,
      "grad_norm": 0.6733769774436951,
      "learning_rate": 5.9807715086666406e-05,
      "loss": 0.8121,
      "step": 20360
    },
    {
      "epoch": 0.8042800173727642,
      "grad_norm": 0.5993396639823914,
      "learning_rate": 5.978797330911675e-05,
      "loss": 0.7374,
      "step": 20370
    },
    {
      "epoch": 0.8046748529237573,
      "grad_norm": 0.7163065075874329,
      "learning_rate": 5.9768231531567106e-05,
      "loss": 0.82,
      "step": 20380
    },
    {
      "epoch": 0.8050696884747502,
      "grad_norm": 0.7066831588745117,
      "learning_rate": 5.9748489754017456e-05,
      "loss": 0.784,
      "step": 20390
    },
    {
      "epoch": 0.8054645240257433,
      "grad_norm": 0.8007363677024841,
      "learning_rate": 5.97287479764678e-05,
      "loss": 0.76,
      "step": 20400
    },
    {
      "epoch": 0.8058593595767363,
      "grad_norm": 0.7370585799217224,
      "learning_rate": 5.970900619891815e-05,
      "loss": 0.7671,
      "step": 20410
    },
    {
      "epoch": 0.8062541951277293,
      "grad_norm": 0.696688175201416,
      "learning_rate": 5.9689264421368506e-05,
      "loss": 0.7549,
      "step": 20420
    },
    {
      "epoch": 0.8066490306787223,
      "grad_norm": 0.4709770381450653,
      "learning_rate": 5.9669522643818856e-05,
      "loss": 0.7469,
      "step": 20430
    },
    {
      "epoch": 0.8070438662297154,
      "grad_norm": 0.7138009071350098,
      "learning_rate": 5.96497808662692e-05,
      "loss": 0.7476,
      "step": 20440
    },
    {
      "epoch": 0.8074387017807083,
      "grad_norm": 0.47647812962532043,
      "learning_rate": 5.963003908871955e-05,
      "loss": 0.7459,
      "step": 20450
    },
    {
      "epoch": 0.8078335373317014,
      "grad_norm": 0.5490167737007141,
      "learning_rate": 5.9610297311169906e-05,
      "loss": 0.7523,
      "step": 20460
    },
    {
      "epoch": 0.8082283728826943,
      "grad_norm": 0.6764341592788696,
      "learning_rate": 5.959055553362025e-05,
      "loss": 0.7932,
      "step": 20470
    },
    {
      "epoch": 0.8086232084336874,
      "grad_norm": 0.8769111037254333,
      "learning_rate": 5.95708137560706e-05,
      "loss": 0.7513,
      "step": 20480
    },
    {
      "epoch": 0.8090180439846804,
      "grad_norm": 0.6966015696525574,
      "learning_rate": 5.955107197852094e-05,
      "loss": 0.7463,
      "step": 20490
    },
    {
      "epoch": 0.8094128795356734,
      "grad_norm": 0.5675612688064575,
      "learning_rate": 5.9531330200971305e-05,
      "loss": 0.7595,
      "step": 20500
    },
    {
      "epoch": 0.8098077150866664,
      "grad_norm": 0.8051703572273254,
      "learning_rate": 5.951158842342165e-05,
      "loss": 0.7765,
      "step": 20510
    },
    {
      "epoch": 0.8102025506376594,
      "grad_norm": 1.029219627380371,
      "learning_rate": 5.9491846645872e-05,
      "loss": 0.7433,
      "step": 20520
    },
    {
      "epoch": 0.8105973861886524,
      "grad_norm": 0.6833358407020569,
      "learning_rate": 5.947210486832234e-05,
      "loss": 0.7387,
      "step": 20530
    },
    {
      "epoch": 0.8109922217396455,
      "grad_norm": 0.6476337909698486,
      "learning_rate": 5.94523630907727e-05,
      "loss": 0.7471,
      "step": 20540
    },
    {
      "epoch": 0.8113870572906384,
      "grad_norm": 0.5820796489715576,
      "learning_rate": 5.943262131322305e-05,
      "loss": 0.7498,
      "step": 20550
    },
    {
      "epoch": 0.8117818928416315,
      "grad_norm": 0.6507365703582764,
      "learning_rate": 5.941287953567339e-05,
      "loss": 0.7255,
      "step": 20560
    },
    {
      "epoch": 0.8121767283926244,
      "grad_norm": 0.5952295064926147,
      "learning_rate": 5.939313775812374e-05,
      "loss": 0.7537,
      "step": 20570
    },
    {
      "epoch": 0.8125715639436175,
      "grad_norm": 0.773481011390686,
      "learning_rate": 5.93733959805741e-05,
      "loss": 0.7444,
      "step": 20580
    },
    {
      "epoch": 0.8129663994946105,
      "grad_norm": 0.8470826148986816,
      "learning_rate": 5.935365420302445e-05,
      "loss": 0.7837,
      "step": 20590
    },
    {
      "epoch": 0.8133612350456035,
      "grad_norm": 0.7065639495849609,
      "learning_rate": 5.933391242547479e-05,
      "loss": 0.7711,
      "step": 20600
    },
    {
      "epoch": 0.8137560705965965,
      "grad_norm": 0.75709068775177,
      "learning_rate": 5.931417064792514e-05,
      "loss": 0.8076,
      "step": 20610
    },
    {
      "epoch": 0.8141509061475896,
      "grad_norm": 0.65621417760849,
      "learning_rate": 5.92944288703755e-05,
      "loss": 0.7691,
      "step": 20620
    },
    {
      "epoch": 0.8145457416985825,
      "grad_norm": 0.7173454165458679,
      "learning_rate": 5.927468709282584e-05,
      "loss": 0.7768,
      "step": 20630
    },
    {
      "epoch": 0.8149405772495756,
      "grad_norm": 1.004711627960205,
      "learning_rate": 5.925494531527619e-05,
      "loss": 0.7891,
      "step": 20640
    },
    {
      "epoch": 0.8153354128005685,
      "grad_norm": 0.8896418213844299,
      "learning_rate": 5.9235203537726534e-05,
      "loss": 0.7087,
      "step": 20650
    },
    {
      "epoch": 0.8157302483515616,
      "grad_norm": 0.6547505855560303,
      "learning_rate": 5.921546176017689e-05,
      "loss": 0.774,
      "step": 20660
    },
    {
      "epoch": 0.8161250839025546,
      "grad_norm": 0.7413849234580994,
      "learning_rate": 5.919571998262724e-05,
      "loss": 0.7561,
      "step": 20670
    },
    {
      "epoch": 0.8165199194535476,
      "grad_norm": 0.5325650572776794,
      "learning_rate": 5.917597820507759e-05,
      "loss": 0.7472,
      "step": 20680
    },
    {
      "epoch": 0.8169147550045406,
      "grad_norm": 0.6728851199150085,
      "learning_rate": 5.915623642752793e-05,
      "loss": 0.701,
      "step": 20690
    },
    {
      "epoch": 0.8173095905555336,
      "grad_norm": 0.6610905528068542,
      "learning_rate": 5.913649464997829e-05,
      "loss": 0.7293,
      "step": 20700
    },
    {
      "epoch": 0.8177044261065266,
      "grad_norm": 0.5791271924972534,
      "learning_rate": 5.911675287242864e-05,
      "loss": 0.7334,
      "step": 20710
    },
    {
      "epoch": 0.8180992616575197,
      "grad_norm": 0.7239906787872314,
      "learning_rate": 5.909701109487898e-05,
      "loss": 0.7361,
      "step": 20720
    },
    {
      "epoch": 0.8184940972085126,
      "grad_norm": 0.971089780330658,
      "learning_rate": 5.907726931732933e-05,
      "loss": 0.7725,
      "step": 20730
    },
    {
      "epoch": 0.8188889327595057,
      "grad_norm": 0.7258158922195435,
      "learning_rate": 5.905752753977969e-05,
      "loss": 0.7224,
      "step": 20740
    },
    {
      "epoch": 0.8192837683104987,
      "grad_norm": 0.6422065496444702,
      "learning_rate": 5.903778576223003e-05,
      "loss": 0.7747,
      "step": 20750
    },
    {
      "epoch": 0.8196786038614917,
      "grad_norm": 0.6176814436912537,
      "learning_rate": 5.901804398468038e-05,
      "loss": 0.7974,
      "step": 20760
    },
    {
      "epoch": 0.8200734394124847,
      "grad_norm": 0.7141971588134766,
      "learning_rate": 5.8998302207130726e-05,
      "loss": 0.7711,
      "step": 20770
    },
    {
      "epoch": 0.8204682749634777,
      "grad_norm": 0.6602132320404053,
      "learning_rate": 5.897856042958109e-05,
      "loss": 0.7471,
      "step": 20780
    },
    {
      "epoch": 0.8208631105144707,
      "grad_norm": 0.7733362317085266,
      "learning_rate": 5.895881865203143e-05,
      "loss": 0.7685,
      "step": 20790
    },
    {
      "epoch": 0.8212579460654638,
      "grad_norm": 0.6653853058815002,
      "learning_rate": 5.893907687448178e-05,
      "loss": 0.7331,
      "step": 20800
    },
    {
      "epoch": 0.8216527816164567,
      "grad_norm": 0.6341962814331055,
      "learning_rate": 5.8919335096932125e-05,
      "loss": 0.7553,
      "step": 20810
    },
    {
      "epoch": 0.8220476171674498,
      "grad_norm": 0.689655065536499,
      "learning_rate": 5.889959331938248e-05,
      "loss": 0.7193,
      "step": 20820
    },
    {
      "epoch": 0.8224424527184427,
      "grad_norm": 0.9072830677032471,
      "learning_rate": 5.887985154183283e-05,
      "loss": 0.7878,
      "step": 20830
    },
    {
      "epoch": 0.8228372882694358,
      "grad_norm": 0.6337432861328125,
      "learning_rate": 5.8860109764283175e-05,
      "loss": 0.78,
      "step": 20840
    },
    {
      "epoch": 0.8232321238204288,
      "grad_norm": 0.6469025611877441,
      "learning_rate": 5.8840367986733525e-05,
      "loss": 0.7377,
      "step": 20850
    },
    {
      "epoch": 0.8236269593714218,
      "grad_norm": 0.6643280386924744,
      "learning_rate": 5.882062620918388e-05,
      "loss": 0.7564,
      "step": 20860
    },
    {
      "epoch": 0.8240217949224148,
      "grad_norm": 0.6065333485603333,
      "learning_rate": 5.880088443163423e-05,
      "loss": 0.7897,
      "step": 20870
    },
    {
      "epoch": 0.8244166304734079,
      "grad_norm": 0.6076835989952087,
      "learning_rate": 5.8781142654084575e-05,
      "loss": 0.7302,
      "step": 20880
    },
    {
      "epoch": 0.8248114660244008,
      "grad_norm": 0.6201714277267456,
      "learning_rate": 5.8761400876534925e-05,
      "loss": 0.7117,
      "step": 20890
    },
    {
      "epoch": 0.8252063015753939,
      "grad_norm": 0.48567232489585876,
      "learning_rate": 5.874165909898528e-05,
      "loss": 0.7921,
      "step": 20900
    },
    {
      "epoch": 0.8256011371263868,
      "grad_norm": 0.7390448451042175,
      "learning_rate": 5.8721917321435625e-05,
      "loss": 0.7674,
      "step": 20910
    },
    {
      "epoch": 0.8259959726773799,
      "grad_norm": 0.6391912698745728,
      "learning_rate": 5.8702175543885974e-05,
      "loss": 0.7529,
      "step": 20920
    },
    {
      "epoch": 0.8263908082283729,
      "grad_norm": 0.8272713422775269,
      "learning_rate": 5.868243376633632e-05,
      "loss": 0.7906,
      "step": 20930
    },
    {
      "epoch": 0.8267856437793659,
      "grad_norm": 0.712860643863678,
      "learning_rate": 5.8662691988786674e-05,
      "loss": 0.7232,
      "step": 20940
    },
    {
      "epoch": 0.8271804793303589,
      "grad_norm": 0.8180277943611145,
      "learning_rate": 5.8642950211237024e-05,
      "loss": 0.7309,
      "step": 20950
    },
    {
      "epoch": 0.8275753148813519,
      "grad_norm": 0.6656115055084229,
      "learning_rate": 5.8623208433687374e-05,
      "loss": 0.7435,
      "step": 20960
    },
    {
      "epoch": 0.8279701504323449,
      "grad_norm": 0.5983191728591919,
      "learning_rate": 5.860346665613772e-05,
      "loss": 0.7422,
      "step": 20970
    },
    {
      "epoch": 0.828364985983338,
      "grad_norm": 0.7351613640785217,
      "learning_rate": 5.8583724878588074e-05,
      "loss": 0.7271,
      "step": 20980
    },
    {
      "epoch": 0.8287598215343309,
      "grad_norm": 0.5190735459327698,
      "learning_rate": 5.8563983101038424e-05,
      "loss": 0.8094,
      "step": 20990
    },
    {
      "epoch": 0.829154657085324,
      "grad_norm": 0.7351370453834534,
      "learning_rate": 5.854424132348877e-05,
      "loss": 0.7567,
      "step": 21000
    },
    {
      "epoch": 0.829154657085324,
      "eval_loss": 0.7816848158836365,
      "eval_runtime": 1217.9166,
      "eval_samples_per_second": 9.243,
      "eval_steps_per_second": 9.243,
      "step": 21000
    },
    {
      "epoch": 0.8295494926363169,
      "grad_norm": 0.7564843893051147,
      "learning_rate": 5.852449954593912e-05,
      "loss": 0.776,
      "step": 21010
    },
    {
      "epoch": 0.82994432818731,
      "grad_norm": 0.574838399887085,
      "learning_rate": 5.8504757768389474e-05,
      "loss": 0.7473,
      "step": 21020
    },
    {
      "epoch": 0.830339163738303,
      "grad_norm": 0.7206167578697205,
      "learning_rate": 5.848501599083982e-05,
      "loss": 0.7722,
      "step": 21030
    },
    {
      "epoch": 0.830733999289296,
      "grad_norm": 0.8193633556365967,
      "learning_rate": 5.846527421329017e-05,
      "loss": 0.7181,
      "step": 21040
    },
    {
      "epoch": 0.831128834840289,
      "grad_norm": 0.5402549505233765,
      "learning_rate": 5.844553243574051e-05,
      "loss": 0.7672,
      "step": 21050
    },
    {
      "epoch": 0.831523670391282,
      "grad_norm": 0.7308644652366638,
      "learning_rate": 5.842579065819087e-05,
      "loss": 0.7638,
      "step": 21060
    },
    {
      "epoch": 0.831918505942275,
      "grad_norm": 0.5958729982376099,
      "learning_rate": 5.8406048880641216e-05,
      "loss": 0.7628,
      "step": 21070
    },
    {
      "epoch": 0.8323133414932681,
      "grad_norm": 0.680152952671051,
      "learning_rate": 5.8386307103091566e-05,
      "loss": 0.7505,
      "step": 21080
    },
    {
      "epoch": 0.832708177044261,
      "grad_norm": 0.8886580467224121,
      "learning_rate": 5.836656532554191e-05,
      "loss": 0.7731,
      "step": 21090
    },
    {
      "epoch": 0.8331030125952541,
      "grad_norm": 0.6659764051437378,
      "learning_rate": 5.8346823547992266e-05,
      "loss": 0.7527,
      "step": 21100
    },
    {
      "epoch": 0.8334978481462471,
      "grad_norm": 0.5491812229156494,
      "learning_rate": 5.8327081770442616e-05,
      "loss": 0.7538,
      "step": 21110
    },
    {
      "epoch": 0.8338926836972401,
      "grad_norm": 0.7321270704269409,
      "learning_rate": 5.830733999289296e-05,
      "loss": 0.7537,
      "step": 21120
    },
    {
      "epoch": 0.8342875192482331,
      "grad_norm": 0.508726179599762,
      "learning_rate": 5.828759821534331e-05,
      "loss": 0.7589,
      "step": 21130
    },
    {
      "epoch": 0.8346823547992261,
      "grad_norm": 0.6526979804039001,
      "learning_rate": 5.8267856437793666e-05,
      "loss": 0.7396,
      "step": 21140
    },
    {
      "epoch": 0.8350771903502191,
      "grad_norm": 0.6741453409194946,
      "learning_rate": 5.8248114660244016e-05,
      "loss": 0.8035,
      "step": 21150
    },
    {
      "epoch": 0.8354720259012122,
      "grad_norm": 0.6441367864608765,
      "learning_rate": 5.822837288269436e-05,
      "loss": 0.7626,
      "step": 21160
    },
    {
      "epoch": 0.8358668614522051,
      "grad_norm": 0.7205752730369568,
      "learning_rate": 5.820863110514471e-05,
      "loss": 0.7712,
      "step": 21170
    },
    {
      "epoch": 0.8362616970031982,
      "grad_norm": 0.6216292381286621,
      "learning_rate": 5.8188889327595065e-05,
      "loss": 0.7052,
      "step": 21180
    },
    {
      "epoch": 0.8366565325541911,
      "grad_norm": 0.8860183358192444,
      "learning_rate": 5.816914755004541e-05,
      "loss": 0.7489,
      "step": 21190
    },
    {
      "epoch": 0.8370513681051842,
      "grad_norm": 0.67876136302948,
      "learning_rate": 5.814940577249576e-05,
      "loss": 0.7647,
      "step": 21200
    },
    {
      "epoch": 0.8374462036561772,
      "grad_norm": 0.6293696761131287,
      "learning_rate": 5.81296639949461e-05,
      "loss": 0.7471,
      "step": 21210
    },
    {
      "epoch": 0.8378410392071702,
      "grad_norm": 0.8035379648208618,
      "learning_rate": 5.8109922217396465e-05,
      "loss": 0.7566,
      "step": 21220
    },
    {
      "epoch": 0.8382358747581632,
      "grad_norm": 0.7197655439376831,
      "learning_rate": 5.809018043984681e-05,
      "loss": 0.7476,
      "step": 21230
    },
    {
      "epoch": 0.8386307103091563,
      "grad_norm": 0.8482726216316223,
      "learning_rate": 5.807043866229716e-05,
      "loss": 0.7653,
      "step": 21240
    },
    {
      "epoch": 0.8390255458601492,
      "grad_norm": 0.8314816355705261,
      "learning_rate": 5.80506968847475e-05,
      "loss": 0.7618,
      "step": 21250
    },
    {
      "epoch": 0.8394203814111423,
      "grad_norm": 0.6483626961708069,
      "learning_rate": 5.803095510719786e-05,
      "loss": 0.7851,
      "step": 21260
    },
    {
      "epoch": 0.8398152169621352,
      "grad_norm": 0.7335089445114136,
      "learning_rate": 5.801121332964821e-05,
      "loss": 0.758,
      "step": 21270
    },
    {
      "epoch": 0.8402100525131283,
      "grad_norm": 0.6836029291152954,
      "learning_rate": 5.799147155209855e-05,
      "loss": 0.7549,
      "step": 21280
    },
    {
      "epoch": 0.8406048880641213,
      "grad_norm": 0.6834287047386169,
      "learning_rate": 5.79717297745489e-05,
      "loss": 0.7739,
      "step": 21290
    },
    {
      "epoch": 0.8409997236151143,
      "grad_norm": 0.7262325286865234,
      "learning_rate": 5.795198799699926e-05,
      "loss": 0.7743,
      "step": 21300
    },
    {
      "epoch": 0.8413945591661073,
      "grad_norm": 0.6233310103416443,
      "learning_rate": 5.79322462194496e-05,
      "loss": 0.7296,
      "step": 21310
    },
    {
      "epoch": 0.8417893947171003,
      "grad_norm": 0.64301997423172,
      "learning_rate": 5.791250444189995e-05,
      "loss": 0.7649,
      "step": 21320
    },
    {
      "epoch": 0.8421842302680933,
      "grad_norm": 0.6667980551719666,
      "learning_rate": 5.78927626643503e-05,
      "loss": 0.7372,
      "step": 21330
    },
    {
      "epoch": 0.8425790658190864,
      "grad_norm": 0.5721102356910706,
      "learning_rate": 5.787302088680066e-05,
      "loss": 0.776,
      "step": 21340
    },
    {
      "epoch": 0.8429739013700793,
      "grad_norm": 1.0698473453521729,
      "learning_rate": 5.7853279109251e-05,
      "loss": 0.7686,
      "step": 21350
    },
    {
      "epoch": 0.8433687369210724,
      "grad_norm": 0.8261855244636536,
      "learning_rate": 5.783353733170135e-05,
      "loss": 0.7518,
      "step": 21360
    },
    {
      "epoch": 0.8437635724720653,
      "grad_norm": 0.7729058265686035,
      "learning_rate": 5.7813795554151694e-05,
      "loss": 0.7217,
      "step": 21370
    },
    {
      "epoch": 0.8441584080230584,
      "grad_norm": 0.6108086705207825,
      "learning_rate": 5.779405377660205e-05,
      "loss": 0.7163,
      "step": 21380
    },
    {
      "epoch": 0.8445532435740514,
      "grad_norm": 0.6499069333076477,
      "learning_rate": 5.77743119990524e-05,
      "loss": 0.7618,
      "step": 21390
    },
    {
      "epoch": 0.8449480791250444,
      "grad_norm": 0.7056443095207214,
      "learning_rate": 5.775457022150274e-05,
      "loss": 0.8058,
      "step": 21400
    },
    {
      "epoch": 0.8453429146760374,
      "grad_norm": 0.7484158873558044,
      "learning_rate": 5.773482844395309e-05,
      "loss": 0.7619,
      "step": 21410
    },
    {
      "epoch": 0.8457377502270305,
      "grad_norm": 0.7097505927085876,
      "learning_rate": 5.771508666640345e-05,
      "loss": 0.7248,
      "step": 21420
    },
    {
      "epoch": 0.8461325857780234,
      "grad_norm": 0.6788109540939331,
      "learning_rate": 5.76953448888538e-05,
      "loss": 0.722,
      "step": 21430
    },
    {
      "epoch": 0.8465274213290165,
      "grad_norm": 0.6711010336875916,
      "learning_rate": 5.767560311130414e-05,
      "loss": 0.7266,
      "step": 21440
    },
    {
      "epoch": 0.8469222568800094,
      "grad_norm": 1.076675295829773,
      "learning_rate": 5.765586133375449e-05,
      "loss": 0.7561,
      "step": 21450
    },
    {
      "epoch": 0.8473170924310025,
      "grad_norm": 0.7256506681442261,
      "learning_rate": 5.763611955620485e-05,
      "loss": 0.7316,
      "step": 21460
    },
    {
      "epoch": 0.8477119279819955,
      "grad_norm": 0.5956962704658508,
      "learning_rate": 5.761637777865519e-05,
      "loss": 0.7296,
      "step": 21470
    },
    {
      "epoch": 0.8481067635329885,
      "grad_norm": 0.5019909143447876,
      "learning_rate": 5.759663600110554e-05,
      "loss": 0.7435,
      "step": 21480
    },
    {
      "epoch": 0.8485015990839815,
      "grad_norm": 0.7167496681213379,
      "learning_rate": 5.7576894223555886e-05,
      "loss": 0.737,
      "step": 21490
    },
    {
      "epoch": 0.8488964346349746,
      "grad_norm": 0.7765791416168213,
      "learning_rate": 5.755715244600625e-05,
      "loss": 0.7606,
      "step": 21500
    },
    {
      "epoch": 0.8492912701859675,
      "grad_norm": 0.8560741543769836,
      "learning_rate": 5.753741066845659e-05,
      "loss": 0.7798,
      "step": 21510
    },
    {
      "epoch": 0.8496861057369606,
      "grad_norm": 0.79220050573349,
      "learning_rate": 5.751766889090694e-05,
      "loss": 0.7478,
      "step": 21520
    },
    {
      "epoch": 0.8500809412879535,
      "grad_norm": 0.7788212895393372,
      "learning_rate": 5.7497927113357285e-05,
      "loss": 0.7235,
      "step": 21530
    },
    {
      "epoch": 0.8504757768389466,
      "grad_norm": 0.6420115828514099,
      "learning_rate": 5.747818533580764e-05,
      "loss": 0.7546,
      "step": 21540
    },
    {
      "epoch": 0.8508706123899396,
      "grad_norm": 0.6250614523887634,
      "learning_rate": 5.745844355825799e-05,
      "loss": 0.7626,
      "step": 21550
    },
    {
      "epoch": 0.8512654479409326,
      "grad_norm": 0.6211594343185425,
      "learning_rate": 5.7438701780708335e-05,
      "loss": 0.7332,
      "step": 21560
    },
    {
      "epoch": 0.8516602834919256,
      "grad_norm": 0.6858277916908264,
      "learning_rate": 5.7418960003158685e-05,
      "loss": 0.7499,
      "step": 21570
    },
    {
      "epoch": 0.8520551190429186,
      "grad_norm": 0.638497531414032,
      "learning_rate": 5.739921822560904e-05,
      "loss": 0.7713,
      "step": 21580
    },
    {
      "epoch": 0.8524499545939116,
      "grad_norm": 0.6084383130073547,
      "learning_rate": 5.737947644805939e-05,
      "loss": 0.7816,
      "step": 21590
    },
    {
      "epoch": 0.8528447901449047,
      "grad_norm": 0.8342476487159729,
      "learning_rate": 5.7359734670509735e-05,
      "loss": 0.7586,
      "step": 21600
    },
    {
      "epoch": 0.8532396256958976,
      "grad_norm": 0.8196957111358643,
      "learning_rate": 5.7339992892960085e-05,
      "loss": 0.7673,
      "step": 21610
    },
    {
      "epoch": 0.8536344612468907,
      "grad_norm": 0.5529582500457764,
      "learning_rate": 5.732025111541044e-05,
      "loss": 0.7525,
      "step": 21620
    },
    {
      "epoch": 0.8540292967978836,
      "grad_norm": 0.6538633108139038,
      "learning_rate": 5.7300509337860785e-05,
      "loss": 0.7343,
      "step": 21630
    },
    {
      "epoch": 0.8544241323488767,
      "grad_norm": 0.7493451237678528,
      "learning_rate": 5.7280767560311134e-05,
      "loss": 0.782,
      "step": 21640
    },
    {
      "epoch": 0.8548189678998697,
      "grad_norm": 0.7499096989631653,
      "learning_rate": 5.726102578276148e-05,
      "loss": 0.8077,
      "step": 21650
    },
    {
      "epoch": 0.8552138034508627,
      "grad_norm": 0.6096306443214417,
      "learning_rate": 5.7241284005211834e-05,
      "loss": 0.7621,
      "step": 21660
    },
    {
      "epoch": 0.8556086390018557,
      "grad_norm": 0.7578902244567871,
      "learning_rate": 5.7221542227662184e-05,
      "loss": 0.7402,
      "step": 21670
    },
    {
      "epoch": 0.8560034745528488,
      "grad_norm": 0.6855514049530029,
      "learning_rate": 5.720180045011253e-05,
      "loss": 0.7545,
      "step": 21680
    },
    {
      "epoch": 0.8563983101038417,
      "grad_norm": 0.7734330892562866,
      "learning_rate": 5.718205867256288e-05,
      "loss": 0.771,
      "step": 21690
    },
    {
      "epoch": 0.8567931456548348,
      "grad_norm": 0.5447717308998108,
      "learning_rate": 5.7162316895013234e-05,
      "loss": 0.7203,
      "step": 21700
    },
    {
      "epoch": 0.8571879812058277,
      "grad_norm": 0.6098698377609253,
      "learning_rate": 5.7142575117463584e-05,
      "loss": 0.7494,
      "step": 21710
    },
    {
      "epoch": 0.8575828167568208,
      "grad_norm": 0.45321986079216003,
      "learning_rate": 5.712283333991393e-05,
      "loss": 0.7471,
      "step": 21720
    },
    {
      "epoch": 0.8579776523078138,
      "grad_norm": 0.5802358984947205,
      "learning_rate": 5.710309156236428e-05,
      "loss": 0.7745,
      "step": 21730
    },
    {
      "epoch": 0.8583724878588068,
      "grad_norm": 0.6974774599075317,
      "learning_rate": 5.7083349784814634e-05,
      "loss": 0.7512,
      "step": 21740
    },
    {
      "epoch": 0.8587673234097998,
      "grad_norm": 1.0106534957885742,
      "learning_rate": 5.706360800726498e-05,
      "loss": 0.7789,
      "step": 21750
    },
    {
      "epoch": 0.8591621589607928,
      "grad_norm": 0.7432875633239746,
      "learning_rate": 5.7043866229715327e-05,
      "loss": 0.7487,
      "step": 21760
    },
    {
      "epoch": 0.8595569945117858,
      "grad_norm": 0.9646039009094238,
      "learning_rate": 5.702412445216567e-05,
      "loss": 0.7517,
      "step": 21770
    },
    {
      "epoch": 0.8599518300627789,
      "grad_norm": 0.7303464412689209,
      "learning_rate": 5.700438267461603e-05,
      "loss": 0.7833,
      "step": 21780
    },
    {
      "epoch": 0.8603466656137718,
      "grad_norm": 1.009006142616272,
      "learning_rate": 5.6984640897066376e-05,
      "loss": 0.7516,
      "step": 21790
    },
    {
      "epoch": 0.8607415011647649,
      "grad_norm": 0.8406314849853516,
      "learning_rate": 5.6964899119516726e-05,
      "loss": 0.8096,
      "step": 21800
    },
    {
      "epoch": 0.8611363367157578,
      "grad_norm": 0.8546476364135742,
      "learning_rate": 5.694515734196707e-05,
      "loss": 0.7884,
      "step": 21810
    },
    {
      "epoch": 0.8615311722667509,
      "grad_norm": 0.5718725919723511,
      "learning_rate": 5.6925415564417426e-05,
      "loss": 0.781,
      "step": 21820
    },
    {
      "epoch": 0.8619260078177439,
      "grad_norm": 0.7399299144744873,
      "learning_rate": 5.6905673786867776e-05,
      "loss": 0.7379,
      "step": 21830
    },
    {
      "epoch": 0.8623208433687369,
      "grad_norm": 0.6619206666946411,
      "learning_rate": 5.688593200931812e-05,
      "loss": 0.7917,
      "step": 21840
    },
    {
      "epoch": 0.8627156789197299,
      "grad_norm": 0.7256755828857422,
      "learning_rate": 5.686619023176847e-05,
      "loss": 0.7609,
      "step": 21850
    },
    {
      "epoch": 0.863110514470723,
      "grad_norm": 0.8848468065261841,
      "learning_rate": 5.6846448454218826e-05,
      "loss": 0.713,
      "step": 21860
    },
    {
      "epoch": 0.8635053500217159,
      "grad_norm": 0.6518433690071106,
      "learning_rate": 5.6826706676669176e-05,
      "loss": 0.784,
      "step": 21870
    },
    {
      "epoch": 0.863900185572709,
      "grad_norm": 0.6858741044998169,
      "learning_rate": 5.680696489911952e-05,
      "loss": 0.7689,
      "step": 21880
    },
    {
      "epoch": 0.8642950211237019,
      "grad_norm": 0.7209097743034363,
      "learning_rate": 5.678722312156987e-05,
      "loss": 0.7605,
      "step": 21890
    },
    {
      "epoch": 0.864689856674695,
      "grad_norm": 0.574601411819458,
      "learning_rate": 5.6767481344020225e-05,
      "loss": 0.7704,
      "step": 21900
    },
    {
      "epoch": 0.865084692225688,
      "grad_norm": 0.7394668459892273,
      "learning_rate": 5.674773956647057e-05,
      "loss": 0.792,
      "step": 21910
    },
    {
      "epoch": 0.865479527776681,
      "grad_norm": 0.8398510813713074,
      "learning_rate": 5.672799778892092e-05,
      "loss": 0.8086,
      "step": 21920
    },
    {
      "epoch": 0.865874363327674,
      "grad_norm": 0.6431428790092468,
      "learning_rate": 5.670825601137126e-05,
      "loss": 0.7444,
      "step": 21930
    },
    {
      "epoch": 0.866269198878667,
      "grad_norm": 0.7799075841903687,
      "learning_rate": 5.668851423382162e-05,
      "loss": 0.763,
      "step": 21940
    },
    {
      "epoch": 0.86666403442966,
      "grad_norm": 0.6093316078186035,
      "learning_rate": 5.666877245627197e-05,
      "loss": 0.7322,
      "step": 21950
    },
    {
      "epoch": 0.8670588699806531,
      "grad_norm": 0.7564591765403748,
      "learning_rate": 5.664903067872231e-05,
      "loss": 0.7413,
      "step": 21960
    },
    {
      "epoch": 0.867453705531646,
      "grad_norm": 0.5986406803131104,
      "learning_rate": 5.662928890117266e-05,
      "loss": 0.711,
      "step": 21970
    },
    {
      "epoch": 0.8678485410826391,
      "grad_norm": 0.5933488011360168,
      "learning_rate": 5.660954712362302e-05,
      "loss": 0.7633,
      "step": 21980
    },
    {
      "epoch": 0.868243376633632,
      "grad_norm": 0.7492916584014893,
      "learning_rate": 5.658980534607337e-05,
      "loss": 0.7875,
      "step": 21990
    },
    {
      "epoch": 0.8686382121846251,
      "grad_norm": 0.650576651096344,
      "learning_rate": 5.657006356852371e-05,
      "loss": 0.7403,
      "step": 22000
    },
    {
      "epoch": 0.8686382121846251,
      "eval_loss": 0.778810441493988,
      "eval_runtime": 1217.799,
      "eval_samples_per_second": 9.244,
      "eval_steps_per_second": 9.244,
      "step": 22000
    },
    {
      "epoch": 0.8690330477356181,
      "grad_norm": 0.6785261034965515,
      "learning_rate": 5.655032179097406e-05,
      "loss": 0.7505,
      "step": 22010
    },
    {
      "epoch": 0.8694278832866111,
      "grad_norm": 0.8311244249343872,
      "learning_rate": 5.653058001342442e-05,
      "loss": 0.7919,
      "step": 22020
    },
    {
      "epoch": 0.8698227188376041,
      "grad_norm": 0.8512912392616272,
      "learning_rate": 5.651083823587476e-05,
      "loss": 0.7519,
      "step": 22030
    },
    {
      "epoch": 0.8702175543885972,
      "grad_norm": 0.7936884164810181,
      "learning_rate": 5.649109645832511e-05,
      "loss": 0.7694,
      "step": 22040
    },
    {
      "epoch": 0.8706123899395901,
      "grad_norm": 0.6946341395378113,
      "learning_rate": 5.6471354680775454e-05,
      "loss": 0.7448,
      "step": 22050
    },
    {
      "epoch": 0.8710072254905832,
      "grad_norm": 0.7009299397468567,
      "learning_rate": 5.645161290322582e-05,
      "loss": 0.7345,
      "step": 22060
    },
    {
      "epoch": 0.8714020610415761,
      "grad_norm": 0.6081793904304504,
      "learning_rate": 5.643187112567616e-05,
      "loss": 0.7305,
      "step": 22070
    },
    {
      "epoch": 0.8717968965925692,
      "grad_norm": 0.8675084710121155,
      "learning_rate": 5.641212934812651e-05,
      "loss": 0.7361,
      "step": 22080
    },
    {
      "epoch": 0.8721917321435622,
      "grad_norm": 0.6838505864143372,
      "learning_rate": 5.6392387570576853e-05,
      "loss": 0.7617,
      "step": 22090
    },
    {
      "epoch": 0.8725865676945552,
      "grad_norm": 0.5866378545761108,
      "learning_rate": 5.637264579302721e-05,
      "loss": 0.7519,
      "step": 22100
    },
    {
      "epoch": 0.8729814032455482,
      "grad_norm": 0.6034181118011475,
      "learning_rate": 5.635290401547756e-05,
      "loss": 0.7258,
      "step": 22110
    },
    {
      "epoch": 0.8733762387965412,
      "grad_norm": 0.565950870513916,
      "learning_rate": 5.63331622379279e-05,
      "loss": 0.7498,
      "step": 22120
    },
    {
      "epoch": 0.8737710743475342,
      "grad_norm": 0.6014813184738159,
      "learning_rate": 5.631342046037825e-05,
      "loss": 0.7271,
      "step": 22130
    },
    {
      "epoch": 0.8741659098985273,
      "grad_norm": 0.8573052287101746,
      "learning_rate": 5.629367868282861e-05,
      "loss": 0.7877,
      "step": 22140
    },
    {
      "epoch": 0.8745607454495202,
      "grad_norm": 0.7103994488716125,
      "learning_rate": 5.627393690527896e-05,
      "loss": 0.7394,
      "step": 22150
    },
    {
      "epoch": 0.8749555810005133,
      "grad_norm": 0.5258495807647705,
      "learning_rate": 5.62541951277293e-05,
      "loss": 0.7296,
      "step": 22160
    },
    {
      "epoch": 0.8753504165515062,
      "grad_norm": 0.7299365997314453,
      "learning_rate": 5.623445335017965e-05,
      "loss": 0.7804,
      "step": 22170
    },
    {
      "epoch": 0.8757452521024993,
      "grad_norm": 0.502619206905365,
      "learning_rate": 5.621471157263001e-05,
      "loss": 0.7475,
      "step": 22180
    },
    {
      "epoch": 0.8761400876534923,
      "grad_norm": 0.6176130175590515,
      "learning_rate": 5.619496979508035e-05,
      "loss": 0.6964,
      "step": 22190
    },
    {
      "epoch": 0.8765349232044853,
      "grad_norm": 0.657793402671814,
      "learning_rate": 5.61752280175307e-05,
      "loss": 0.7383,
      "step": 22200
    },
    {
      "epoch": 0.8769297587554783,
      "grad_norm": 0.6384347081184387,
      "learning_rate": 5.6155486239981046e-05,
      "loss": 0.731,
      "step": 22210
    },
    {
      "epoch": 0.8773245943064714,
      "grad_norm": 0.6780728697776794,
      "learning_rate": 5.61357444624314e-05,
      "loss": 0.7549,
      "step": 22220
    },
    {
      "epoch": 0.8777194298574643,
      "grad_norm": 0.654335081577301,
      "learning_rate": 5.611600268488175e-05,
      "loss": 0.7727,
      "step": 22230
    },
    {
      "epoch": 0.8781142654084574,
      "grad_norm": 0.6636806726455688,
      "learning_rate": 5.60962609073321e-05,
      "loss": 0.7421,
      "step": 22240
    },
    {
      "epoch": 0.8785091009594503,
      "grad_norm": 0.6669430732727051,
      "learning_rate": 5.6076519129782445e-05,
      "loss": 0.7348,
      "step": 22250
    },
    {
      "epoch": 0.8789039365104434,
      "grad_norm": 0.9295649528503418,
      "learning_rate": 5.60567773522328e-05,
      "loss": 0.7733,
      "step": 22260
    },
    {
      "epoch": 0.8792987720614364,
      "grad_norm": 0.707947850227356,
      "learning_rate": 5.603703557468315e-05,
      "loss": 0.7642,
      "step": 22270
    },
    {
      "epoch": 0.8796936076124294,
      "grad_norm": 0.62317955493927,
      "learning_rate": 5.6017293797133495e-05,
      "loss": 0.7861,
      "step": 22280
    },
    {
      "epoch": 0.8800884431634224,
      "grad_norm": 0.7785137891769409,
      "learning_rate": 5.5997552019583845e-05,
      "loss": 0.783,
      "step": 22290
    },
    {
      "epoch": 0.8804832787144155,
      "grad_norm": 0.7435354590415955,
      "learning_rate": 5.59778102420342e-05,
      "loss": 0.7518,
      "step": 22300
    },
    {
      "epoch": 0.8808781142654084,
      "grad_norm": 0.5656827092170715,
      "learning_rate": 5.5958068464484545e-05,
      "loss": 0.8207,
      "step": 22310
    },
    {
      "epoch": 0.8812729498164015,
      "grad_norm": 0.5971795320510864,
      "learning_rate": 5.5938326686934895e-05,
      "loss": 0.7838,
      "step": 22320
    },
    {
      "epoch": 0.8816677853673944,
      "grad_norm": 0.8100100159645081,
      "learning_rate": 5.591858490938524e-05,
      "loss": 0.8253,
      "step": 22330
    },
    {
      "epoch": 0.8820626209183875,
      "grad_norm": 0.6312414407730103,
      "learning_rate": 5.589884313183559e-05,
      "loss": 0.7511,
      "step": 22340
    },
    {
      "epoch": 0.8824574564693805,
      "grad_norm": 0.5983083248138428,
      "learning_rate": 5.5879101354285944e-05,
      "loss": 0.7882,
      "step": 22350
    },
    {
      "epoch": 0.8828522920203735,
      "grad_norm": 0.8065258860588074,
      "learning_rate": 5.5859359576736294e-05,
      "loss": 0.7814,
      "step": 22360
    },
    {
      "epoch": 0.8832471275713665,
      "grad_norm": 0.5780666470527649,
      "learning_rate": 5.583961779918664e-05,
      "loss": 0.7912,
      "step": 22370
    },
    {
      "epoch": 0.8836419631223595,
      "grad_norm": 0.8875576257705688,
      "learning_rate": 5.581987602163699e-05,
      "loss": 0.7401,
      "step": 22380
    },
    {
      "epoch": 0.8840367986733525,
      "grad_norm": 0.6573742628097534,
      "learning_rate": 5.5800134244087344e-05,
      "loss": 0.763,
      "step": 22390
    },
    {
      "epoch": 0.8844316342243456,
      "grad_norm": 0.7925001978874207,
      "learning_rate": 5.578039246653769e-05,
      "loss": 0.8033,
      "step": 22400
    },
    {
      "epoch": 0.8848264697753385,
      "grad_norm": 0.6396066546440125,
      "learning_rate": 5.576065068898804e-05,
      "loss": 0.7481,
      "step": 22410
    },
    {
      "epoch": 0.8852213053263316,
      "grad_norm": 0.7103902101516724,
      "learning_rate": 5.574090891143838e-05,
      "loss": 0.7796,
      "step": 22420
    },
    {
      "epoch": 0.8856161408773245,
      "grad_norm": 0.666616678237915,
      "learning_rate": 5.5721167133888744e-05,
      "loss": 0.7797,
      "step": 22430
    },
    {
      "epoch": 0.8860109764283176,
      "grad_norm": 0.7519524097442627,
      "learning_rate": 5.570142535633909e-05,
      "loss": 0.7572,
      "step": 22440
    },
    {
      "epoch": 0.8864058119793106,
      "grad_norm": 0.8473224639892578,
      "learning_rate": 5.568168357878944e-05,
      "loss": 0.7909,
      "step": 22450
    },
    {
      "epoch": 0.8868006475303036,
      "grad_norm": 0.704234778881073,
      "learning_rate": 5.566194180123978e-05,
      "loss": 0.7694,
      "step": 22460
    },
    {
      "epoch": 0.8871954830812966,
      "grad_norm": 0.6108385920524597,
      "learning_rate": 5.5642200023690137e-05,
      "loss": 0.7532,
      "step": 22470
    },
    {
      "epoch": 0.8875903186322897,
      "grad_norm": 0.7943953275680542,
      "learning_rate": 5.5622458246140487e-05,
      "loss": 0.792,
      "step": 22480
    },
    {
      "epoch": 0.8879851541832826,
      "grad_norm": 0.9054985642433167,
      "learning_rate": 5.560271646859083e-05,
      "loss": 0.7393,
      "step": 22490
    },
    {
      "epoch": 0.8883799897342757,
      "grad_norm": 0.726731538772583,
      "learning_rate": 5.558297469104118e-05,
      "loss": 0.7456,
      "step": 22500
    },
    {
      "epoch": 0.8887748252852686,
      "grad_norm": 0.7891333699226379,
      "learning_rate": 5.5563232913491536e-05,
      "loss": 0.7757,
      "step": 22510
    },
    {
      "epoch": 0.8891696608362617,
      "grad_norm": 0.6803507804870605,
      "learning_rate": 5.5543491135941886e-05,
      "loss": 0.7444,
      "step": 22520
    },
    {
      "epoch": 0.8895644963872547,
      "grad_norm": 0.6531479954719543,
      "learning_rate": 5.552374935839223e-05,
      "loss": 0.7417,
      "step": 22530
    },
    {
      "epoch": 0.8899593319382477,
      "grad_norm": 0.6894510984420776,
      "learning_rate": 5.550400758084258e-05,
      "loss": 0.7573,
      "step": 22540
    },
    {
      "epoch": 0.8903541674892407,
      "grad_norm": 0.5470763444900513,
      "learning_rate": 5.5484265803292936e-05,
      "loss": 0.7576,
      "step": 22550
    },
    {
      "epoch": 0.8907490030402337,
      "grad_norm": 0.7765732407569885,
      "learning_rate": 5.546452402574328e-05,
      "loss": 0.7386,
      "step": 22560
    },
    {
      "epoch": 0.8911438385912267,
      "grad_norm": 0.8924948573112488,
      "learning_rate": 5.544478224819363e-05,
      "loss": 0.7657,
      "step": 22570
    },
    {
      "epoch": 0.8915386741422198,
      "grad_norm": 0.6631487607955933,
      "learning_rate": 5.542504047064397e-05,
      "loss": 0.7328,
      "step": 22580
    },
    {
      "epoch": 0.8919335096932127,
      "grad_norm": 0.5181528329849243,
      "learning_rate": 5.540529869309433e-05,
      "loss": 0.7788,
      "step": 22590
    },
    {
      "epoch": 0.8923283452442058,
      "grad_norm": 0.6720436811447144,
      "learning_rate": 5.538555691554468e-05,
      "loss": 0.7379,
      "step": 22600
    },
    {
      "epoch": 0.8927231807951987,
      "grad_norm": 0.7358434200286865,
      "learning_rate": 5.536581513799502e-05,
      "loss": 0.7631,
      "step": 22610
    },
    {
      "epoch": 0.8931180163461918,
      "grad_norm": 0.7154426574707031,
      "learning_rate": 5.534607336044537e-05,
      "loss": 0.7595,
      "step": 22620
    },
    {
      "epoch": 0.8935128518971848,
      "grad_norm": 0.689461886882782,
      "learning_rate": 5.532633158289573e-05,
      "loss": 0.7904,
      "step": 22630
    },
    {
      "epoch": 0.8939076874481778,
      "grad_norm": 0.6518831253051758,
      "learning_rate": 5.530658980534608e-05,
      "loss": 0.7489,
      "step": 22640
    },
    {
      "epoch": 0.8943025229991708,
      "grad_norm": 0.8151546716690063,
      "learning_rate": 5.528684802779642e-05,
      "loss": 0.7876,
      "step": 22650
    },
    {
      "epoch": 0.8946973585501639,
      "grad_norm": 0.7727034091949463,
      "learning_rate": 5.526710625024677e-05,
      "loss": 0.7625,
      "step": 22660
    },
    {
      "epoch": 0.8950921941011568,
      "grad_norm": 0.5371704697608948,
      "learning_rate": 5.524736447269713e-05,
      "loss": 0.7388,
      "step": 22670
    },
    {
      "epoch": 0.8954870296521499,
      "grad_norm": 0.6472512483596802,
      "learning_rate": 5.522762269514747e-05,
      "loss": 0.7047,
      "step": 22680
    },
    {
      "epoch": 0.895881865203143,
      "grad_norm": 0.6075650453567505,
      "learning_rate": 5.520788091759782e-05,
      "loss": 0.7742,
      "step": 22690
    },
    {
      "epoch": 0.8962767007541359,
      "grad_norm": 0.5131752490997314,
      "learning_rate": 5.5188139140048164e-05,
      "loss": 0.7768,
      "step": 22700
    },
    {
      "epoch": 0.896671536305129,
      "grad_norm": 0.6297288537025452,
      "learning_rate": 5.516839736249853e-05,
      "loss": 0.7834,
      "step": 22710
    },
    {
      "epoch": 0.8970663718561219,
      "grad_norm": 0.7352483868598938,
      "learning_rate": 5.514865558494887e-05,
      "loss": 0.7281,
      "step": 22720
    },
    {
      "epoch": 0.897461207407115,
      "grad_norm": 0.5367588400840759,
      "learning_rate": 5.512891380739922e-05,
      "loss": 0.7611,
      "step": 22730
    },
    {
      "epoch": 0.897856042958108,
      "grad_norm": 0.6588345170021057,
      "learning_rate": 5.5109172029849564e-05,
      "loss": 0.7405,
      "step": 22740
    },
    {
      "epoch": 0.898250878509101,
      "grad_norm": 0.75510174036026,
      "learning_rate": 5.508943025229992e-05,
      "loss": 0.806,
      "step": 22750
    },
    {
      "epoch": 0.898645714060094,
      "grad_norm": 0.5140815377235413,
      "learning_rate": 5.506968847475027e-05,
      "loss": 0.7627,
      "step": 22760
    },
    {
      "epoch": 0.899040549611087,
      "grad_norm": 0.571681559085846,
      "learning_rate": 5.5049946697200614e-05,
      "loss": 0.7498,
      "step": 22770
    },
    {
      "epoch": 0.89943538516208,
      "grad_norm": 0.6373323798179626,
      "learning_rate": 5.5030204919650964e-05,
      "loss": 0.7171,
      "step": 22780
    },
    {
      "epoch": 0.8998302207130731,
      "grad_norm": 0.6457083821296692,
      "learning_rate": 5.501046314210132e-05,
      "loss": 0.7525,
      "step": 22790
    },
    {
      "epoch": 0.900225056264066,
      "grad_norm": 0.6969906687736511,
      "learning_rate": 5.499072136455167e-05,
      "loss": 0.7825,
      "step": 22800
    },
    {
      "epoch": 0.9006198918150591,
      "grad_norm": 0.5580013394355774,
      "learning_rate": 5.497097958700201e-05,
      "loss": 0.7484,
      "step": 22810
    },
    {
      "epoch": 0.901014727366052,
      "grad_norm": 0.7774592041969299,
      "learning_rate": 5.495123780945236e-05,
      "loss": 0.7965,
      "step": 22820
    },
    {
      "epoch": 0.9014095629170451,
      "grad_norm": 0.8081968426704407,
      "learning_rate": 5.493149603190272e-05,
      "loss": 0.7914,
      "step": 22830
    },
    {
      "epoch": 0.9018043984680381,
      "grad_norm": 0.8315101861953735,
      "learning_rate": 5.491175425435306e-05,
      "loss": 0.732,
      "step": 22840
    },
    {
      "epoch": 0.9021992340190311,
      "grad_norm": 0.6290115118026733,
      "learning_rate": 5.489201247680341e-05,
      "loss": 0.6985,
      "step": 22850
    },
    {
      "epoch": 0.9025940695700241,
      "grad_norm": 0.6908605694770813,
      "learning_rate": 5.4872270699253756e-05,
      "loss": 0.7484,
      "step": 22860
    },
    {
      "epoch": 0.9029889051210171,
      "grad_norm": 0.5915210247039795,
      "learning_rate": 5.485252892170411e-05,
      "loss": 0.7792,
      "step": 22870
    },
    {
      "epoch": 0.9033837406720101,
      "grad_norm": 0.8564170598983765,
      "learning_rate": 5.483278714415446e-05,
      "loss": 0.7564,
      "step": 22880
    },
    {
      "epoch": 0.9037785762230032,
      "grad_norm": 0.6616445183753967,
      "learning_rate": 5.481304536660481e-05,
      "loss": 0.7157,
      "step": 22890
    },
    {
      "epoch": 0.9041734117739961,
      "grad_norm": 0.6678001880645752,
      "learning_rate": 5.4793303589055156e-05,
      "loss": 0.7602,
      "step": 22900
    },
    {
      "epoch": 0.9045682473249892,
      "grad_norm": 0.8453550934791565,
      "learning_rate": 5.477356181150551e-05,
      "loss": 0.7743,
      "step": 22910
    },
    {
      "epoch": 0.9049630828759821,
      "grad_norm": 0.7361973524093628,
      "learning_rate": 5.475382003395586e-05,
      "loss": 0.7601,
      "step": 22920
    },
    {
      "epoch": 0.9053579184269752,
      "grad_norm": 0.6242012977600098,
      "learning_rate": 5.4734078256406206e-05,
      "loss": 0.797,
      "step": 22930
    },
    {
      "epoch": 0.9057527539779682,
      "grad_norm": 0.6538001298904419,
      "learning_rate": 5.4714336478856555e-05,
      "loss": 0.7722,
      "step": 22940
    },
    {
      "epoch": 0.9061475895289612,
      "grad_norm": 0.574722409248352,
      "learning_rate": 5.469459470130691e-05,
      "loss": 0.7527,
      "step": 22950
    },
    {
      "epoch": 0.9065424250799542,
      "grad_norm": 0.6484817266464233,
      "learning_rate": 5.4674852923757255e-05,
      "loss": 0.7124,
      "step": 22960
    },
    {
      "epoch": 0.9069372606309473,
      "grad_norm": 0.7276567816734314,
      "learning_rate": 5.4655111146207605e-05,
      "loss": 0.7772,
      "step": 22970
    },
    {
      "epoch": 0.9073320961819402,
      "grad_norm": 0.6803218722343445,
      "learning_rate": 5.463536936865795e-05,
      "loss": 0.7204,
      "step": 22980
    },
    {
      "epoch": 0.9077269317329333,
      "grad_norm": 0.6360435485839844,
      "learning_rate": 5.461562759110831e-05,
      "loss": 0.7741,
      "step": 22990
    },
    {
      "epoch": 0.9081217672839262,
      "grad_norm": 0.6926742196083069,
      "learning_rate": 5.4595885813558655e-05,
      "loss": 0.7417,
      "step": 23000
    },
    {
      "epoch": 0.9081217672839262,
      "eval_loss": 0.7771672010421753,
      "eval_runtime": 1218.7292,
      "eval_samples_per_second": 9.237,
      "eval_steps_per_second": 9.237,
      "step": 23000
    },
    {
      "epoch": 0.9085166028349193,
      "grad_norm": 0.6784130930900574,
      "learning_rate": 5.4576144036009005e-05,
      "loss": 0.7868,
      "step": 23010
    },
    {
      "epoch": 0.9089114383859123,
      "grad_norm": 0.4777798354625702,
      "learning_rate": 5.455640225845935e-05,
      "loss": 0.7836,
      "step": 23020
    },
    {
      "epoch": 0.9093062739369053,
      "grad_norm": 0.8316764831542969,
      "learning_rate": 5.4536660480909705e-05,
      "loss": 0.7714,
      "step": 23030
    },
    {
      "epoch": 0.9097011094878983,
      "grad_norm": 0.6751574277877808,
      "learning_rate": 5.4516918703360055e-05,
      "loss": 0.7992,
      "step": 23040
    },
    {
      "epoch": 0.9100959450388914,
      "grad_norm": 0.8078408241271973,
      "learning_rate": 5.44971769258104e-05,
      "loss": 0.7795,
      "step": 23050
    },
    {
      "epoch": 0.9104907805898843,
      "grad_norm": 0.830814778804779,
      "learning_rate": 5.447743514826075e-05,
      "loss": 0.7199,
      "step": 23060
    },
    {
      "epoch": 0.9108856161408774,
      "grad_norm": 0.6376978158950806,
      "learning_rate": 5.4457693370711104e-05,
      "loss": 0.7971,
      "step": 23070
    },
    {
      "epoch": 0.9112804516918703,
      "grad_norm": 0.6737875938415527,
      "learning_rate": 5.4437951593161454e-05,
      "loss": 0.7309,
      "step": 23080
    },
    {
      "epoch": 0.9116752872428634,
      "grad_norm": 0.7748898863792419,
      "learning_rate": 5.44182098156118e-05,
      "loss": 0.8006,
      "step": 23090
    },
    {
      "epoch": 0.9120701227938564,
      "grad_norm": 0.661419689655304,
      "learning_rate": 5.439846803806215e-05,
      "loss": 0.7167,
      "step": 23100
    },
    {
      "epoch": 0.9124649583448494,
      "grad_norm": 0.7619231939315796,
      "learning_rate": 5.4378726260512504e-05,
      "loss": 0.7748,
      "step": 23110
    },
    {
      "epoch": 0.9128597938958424,
      "grad_norm": 0.6927257180213928,
      "learning_rate": 5.435898448296285e-05,
      "loss": 0.7665,
      "step": 23120
    },
    {
      "epoch": 0.9132546294468354,
      "grad_norm": 0.6417753100395203,
      "learning_rate": 5.43392427054132e-05,
      "loss": 0.7535,
      "step": 23130
    },
    {
      "epoch": 0.9136494649978284,
      "grad_norm": 0.6352993845939636,
      "learning_rate": 5.431950092786354e-05,
      "loss": 0.7671,
      "step": 23140
    },
    {
      "epoch": 0.9140443005488215,
      "grad_norm": 0.6922234892845154,
      "learning_rate": 5.4299759150313904e-05,
      "loss": 0.7381,
      "step": 23150
    },
    {
      "epoch": 0.9144391360998144,
      "grad_norm": 0.593791663646698,
      "learning_rate": 5.428001737276425e-05,
      "loss": 0.7736,
      "step": 23160
    },
    {
      "epoch": 0.9148339716508075,
      "grad_norm": 0.7084019184112549,
      "learning_rate": 5.42602755952146e-05,
      "loss": 0.7598,
      "step": 23170
    },
    {
      "epoch": 0.9152288072018004,
      "grad_norm": 0.8569918870925903,
      "learning_rate": 5.424053381766494e-05,
      "loss": 0.7709,
      "step": 23180
    },
    {
      "epoch": 0.9156236427527935,
      "grad_norm": 0.6214587688446045,
      "learning_rate": 5.4220792040115297e-05,
      "loss": 0.7948,
      "step": 23190
    },
    {
      "epoch": 0.9160184783037865,
      "grad_norm": 0.6545723080635071,
      "learning_rate": 5.4201050262565646e-05,
      "loss": 0.75,
      "step": 23200
    },
    {
      "epoch": 0.9164133138547795,
      "grad_norm": 0.8650539517402649,
      "learning_rate": 5.418130848501599e-05,
      "loss": 0.7758,
      "step": 23210
    },
    {
      "epoch": 0.9168081494057725,
      "grad_norm": 0.6027035713195801,
      "learning_rate": 5.416156670746634e-05,
      "loss": 0.7321,
      "step": 23220
    },
    {
      "epoch": 0.9172029849567656,
      "grad_norm": 0.5947343707084656,
      "learning_rate": 5.4141824929916696e-05,
      "loss": 0.7732,
      "step": 23230
    },
    {
      "epoch": 0.9175978205077585,
      "grad_norm": 0.8695787191390991,
      "learning_rate": 5.412208315236704e-05,
      "loss": 0.7356,
      "step": 23240
    },
    {
      "epoch": 0.9179926560587516,
      "grad_norm": 0.6122599840164185,
      "learning_rate": 5.410234137481739e-05,
      "loss": 0.7511,
      "step": 23250
    },
    {
      "epoch": 0.9183874916097445,
      "grad_norm": 0.6532960534095764,
      "learning_rate": 5.408259959726773e-05,
      "loss": 0.7824,
      "step": 23260
    },
    {
      "epoch": 0.9187823271607376,
      "grad_norm": 0.6840642690658569,
      "learning_rate": 5.4062857819718096e-05,
      "loss": 0.7617,
      "step": 23270
    },
    {
      "epoch": 0.9191771627117306,
      "grad_norm": 0.6582516431808472,
      "learning_rate": 5.404311604216844e-05,
      "loss": 0.7501,
      "step": 23280
    },
    {
      "epoch": 0.9195719982627236,
      "grad_norm": 0.6815613508224487,
      "learning_rate": 5.402337426461879e-05,
      "loss": 0.7849,
      "step": 23290
    },
    {
      "epoch": 0.9199668338137166,
      "grad_norm": 0.7194442749023438,
      "learning_rate": 5.400363248706913e-05,
      "loss": 0.763,
      "step": 23300
    },
    {
      "epoch": 0.9203616693647096,
      "grad_norm": 0.572320282459259,
      "learning_rate": 5.398389070951949e-05,
      "loss": 0.7586,
      "step": 23310
    },
    {
      "epoch": 0.9207565049157026,
      "grad_norm": 0.6700228452682495,
      "learning_rate": 5.396414893196984e-05,
      "loss": 0.7536,
      "step": 23320
    },
    {
      "epoch": 0.9211513404666957,
      "grad_norm": 0.5581182837486267,
      "learning_rate": 5.394440715442018e-05,
      "loss": 0.7345,
      "step": 23330
    },
    {
      "epoch": 0.9215461760176886,
      "grad_norm": 0.5629006028175354,
      "learning_rate": 5.392466537687053e-05,
      "loss": 0.7516,
      "step": 23340
    },
    {
      "epoch": 0.9219410115686817,
      "grad_norm": 0.6194537281990051,
      "learning_rate": 5.390492359932089e-05,
      "loss": 0.7345,
      "step": 23350
    },
    {
      "epoch": 0.9223358471196746,
      "grad_norm": 0.673001229763031,
      "learning_rate": 5.388518182177124e-05,
      "loss": 0.7167,
      "step": 23360
    },
    {
      "epoch": 0.9227306826706677,
      "grad_norm": 0.6859041452407837,
      "learning_rate": 5.386544004422158e-05,
      "loss": 0.7533,
      "step": 23370
    },
    {
      "epoch": 0.9231255182216607,
      "grad_norm": 0.8188633918762207,
      "learning_rate": 5.384569826667193e-05,
      "loss": 0.7405,
      "step": 23380
    },
    {
      "epoch": 0.9235203537726537,
      "grad_norm": 0.7392277121543884,
      "learning_rate": 5.382595648912229e-05,
      "loss": 0.7541,
      "step": 23390
    },
    {
      "epoch": 0.9239151893236467,
      "grad_norm": 0.641899824142456,
      "learning_rate": 5.380621471157263e-05,
      "loss": 0.7499,
      "step": 23400
    },
    {
      "epoch": 0.9243100248746398,
      "grad_norm": 0.591330885887146,
      "learning_rate": 5.378647293402298e-05,
      "loss": 0.7573,
      "step": 23410
    },
    {
      "epoch": 0.9247048604256327,
      "grad_norm": 0.8512395620346069,
      "learning_rate": 5.3766731156473324e-05,
      "loss": 0.7608,
      "step": 23420
    },
    {
      "epoch": 0.9250996959766258,
      "grad_norm": 0.5789991021156311,
      "learning_rate": 5.374698937892369e-05,
      "loss": 0.7545,
      "step": 23430
    },
    {
      "epoch": 0.9254945315276187,
      "grad_norm": 0.83906090259552,
      "learning_rate": 5.372724760137403e-05,
      "loss": 0.7619,
      "step": 23440
    },
    {
      "epoch": 0.9258893670786118,
      "grad_norm": 0.7496359944343567,
      "learning_rate": 5.370750582382438e-05,
      "loss": 0.7723,
      "step": 23450
    },
    {
      "epoch": 0.9262842026296048,
      "grad_norm": 0.7134745121002197,
      "learning_rate": 5.3687764046274724e-05,
      "loss": 0.7582,
      "step": 23460
    },
    {
      "epoch": 0.9266790381805978,
      "grad_norm": 0.7464697957038879,
      "learning_rate": 5.366802226872508e-05,
      "loss": 0.7477,
      "step": 23470
    },
    {
      "epoch": 0.9270738737315908,
      "grad_norm": 0.8300132155418396,
      "learning_rate": 5.364828049117543e-05,
      "loss": 0.7758,
      "step": 23480
    },
    {
      "epoch": 0.9274687092825838,
      "grad_norm": 0.6321331858634949,
      "learning_rate": 5.3628538713625774e-05,
      "loss": 0.7227,
      "step": 23490
    },
    {
      "epoch": 0.9278635448335768,
      "grad_norm": 0.6581608653068542,
      "learning_rate": 5.3608796936076124e-05,
      "loss": 0.7424,
      "step": 23500
    },
    {
      "epoch": 0.9282583803845699,
      "grad_norm": 0.7005730867385864,
      "learning_rate": 5.358905515852648e-05,
      "loss": 0.7426,
      "step": 23510
    },
    {
      "epoch": 0.9286532159355628,
      "grad_norm": 0.6862449645996094,
      "learning_rate": 5.356931338097682e-05,
      "loss": 0.7904,
      "step": 23520
    },
    {
      "epoch": 0.9290480514865559,
      "grad_norm": 0.6444319486618042,
      "learning_rate": 5.354957160342717e-05,
      "loss": 0.7038,
      "step": 23530
    },
    {
      "epoch": 0.9294428870375488,
      "grad_norm": 0.689479410648346,
      "learning_rate": 5.352982982587752e-05,
      "loss": 0.7757,
      "step": 23540
    },
    {
      "epoch": 0.9298377225885419,
      "grad_norm": 0.7464650869369507,
      "learning_rate": 5.351008804832788e-05,
      "loss": 0.7743,
      "step": 23550
    },
    {
      "epoch": 0.9302325581395349,
      "grad_norm": 0.7080198526382446,
      "learning_rate": 5.349034627077822e-05,
      "loss": 0.7393,
      "step": 23560
    },
    {
      "epoch": 0.9306273936905279,
      "grad_norm": 0.6590821146965027,
      "learning_rate": 5.347060449322857e-05,
      "loss": 0.7364,
      "step": 23570
    },
    {
      "epoch": 0.9310222292415209,
      "grad_norm": 0.6389283537864685,
      "learning_rate": 5.3450862715678916e-05,
      "loss": 0.7502,
      "step": 23580
    },
    {
      "epoch": 0.931417064792514,
      "grad_norm": 0.5650458931922913,
      "learning_rate": 5.343112093812927e-05,
      "loss": 0.7228,
      "step": 23590
    },
    {
      "epoch": 0.9318119003435069,
      "grad_norm": 0.8106938600540161,
      "learning_rate": 5.341137916057962e-05,
      "loss": 0.7922,
      "step": 23600
    },
    {
      "epoch": 0.9322067358945,
      "grad_norm": 0.7999834418296814,
      "learning_rate": 5.3391637383029966e-05,
      "loss": 0.7419,
      "step": 23610
    },
    {
      "epoch": 0.9326015714454929,
      "grad_norm": 0.5884515643119812,
      "learning_rate": 5.3371895605480316e-05,
      "loss": 0.742,
      "step": 23620
    },
    {
      "epoch": 0.932996406996486,
      "grad_norm": 0.7845566272735596,
      "learning_rate": 5.335215382793067e-05,
      "loss": 0.7898,
      "step": 23630
    },
    {
      "epoch": 0.933391242547479,
      "grad_norm": 0.7215331792831421,
      "learning_rate": 5.333241205038102e-05,
      "loss": 0.7909,
      "step": 23640
    },
    {
      "epoch": 0.933786078098472,
      "grad_norm": 0.861382007598877,
      "learning_rate": 5.3312670272831365e-05,
      "loss": 0.7504,
      "step": 23650
    },
    {
      "epoch": 0.934180913649465,
      "grad_norm": 0.5602884292602539,
      "learning_rate": 5.3292928495281715e-05,
      "loss": 0.7479,
      "step": 23660
    },
    {
      "epoch": 0.934575749200458,
      "grad_norm": 0.5583564043045044,
      "learning_rate": 5.327318671773207e-05,
      "loss": 0.7479,
      "step": 23670
    },
    {
      "epoch": 0.934970584751451,
      "grad_norm": 0.9193887710571289,
      "learning_rate": 5.3253444940182415e-05,
      "loss": 0.7997,
      "step": 23680
    },
    {
      "epoch": 0.9353654203024441,
      "grad_norm": 0.617523193359375,
      "learning_rate": 5.3233703162632765e-05,
      "loss": 0.7205,
      "step": 23690
    },
    {
      "epoch": 0.935760255853437,
      "grad_norm": 0.8105116486549377,
      "learning_rate": 5.321396138508311e-05,
      "loss": 0.7606,
      "step": 23700
    },
    {
      "epoch": 0.9361550914044301,
      "grad_norm": 0.6256777048110962,
      "learning_rate": 5.319421960753347e-05,
      "loss": 0.7367,
      "step": 23710
    },
    {
      "epoch": 0.936549926955423,
      "grad_norm": 0.8382683992385864,
      "learning_rate": 5.3174477829983815e-05,
      "loss": 0.768,
      "step": 23720
    },
    {
      "epoch": 0.9369447625064161,
      "grad_norm": 0.8762311935424805,
      "learning_rate": 5.3154736052434165e-05,
      "loss": 0.775,
      "step": 23730
    },
    {
      "epoch": 0.9373395980574091,
      "grad_norm": 0.5944895148277283,
      "learning_rate": 5.313499427488451e-05,
      "loss": 0.784,
      "step": 23740
    },
    {
      "epoch": 0.9377344336084021,
      "grad_norm": 0.8089868426322937,
      "learning_rate": 5.3115252497334865e-05,
      "loss": 0.7483,
      "step": 23750
    },
    {
      "epoch": 0.9381292691593951,
      "grad_norm": 0.6886867880821228,
      "learning_rate": 5.3095510719785215e-05,
      "loss": 0.7982,
      "step": 23760
    },
    {
      "epoch": 0.9385241047103882,
      "grad_norm": 0.6158190965652466,
      "learning_rate": 5.307576894223556e-05,
      "loss": 0.7495,
      "step": 23770
    },
    {
      "epoch": 0.9389189402613811,
      "grad_norm": 0.5258999466896057,
      "learning_rate": 5.305602716468591e-05,
      "loss": 0.751,
      "step": 23780
    },
    {
      "epoch": 0.9393137758123742,
      "grad_norm": 0.6004019379615784,
      "learning_rate": 5.3036285387136264e-05,
      "loss": 0.7369,
      "step": 23790
    },
    {
      "epoch": 0.9397086113633671,
      "grad_norm": 0.743547797203064,
      "learning_rate": 5.3016543609586614e-05,
      "loss": 0.7355,
      "step": 23800
    },
    {
      "epoch": 0.9401034469143602,
      "grad_norm": 0.6555246114730835,
      "learning_rate": 5.299680183203696e-05,
      "loss": 0.7963,
      "step": 23810
    },
    {
      "epoch": 0.9404982824653532,
      "grad_norm": 0.7012253403663635,
      "learning_rate": 5.297706005448731e-05,
      "loss": 0.7998,
      "step": 23820
    },
    {
      "epoch": 0.9408931180163462,
      "grad_norm": 0.7363013029098511,
      "learning_rate": 5.2957318276937664e-05,
      "loss": 0.7988,
      "step": 23830
    },
    {
      "epoch": 0.9412879535673392,
      "grad_norm": 0.5670210719108582,
      "learning_rate": 5.293757649938801e-05,
      "loss": 0.7324,
      "step": 23840
    },
    {
      "epoch": 0.9416827891183323,
      "grad_norm": 0.8943610787391663,
      "learning_rate": 5.291783472183836e-05,
      "loss": 0.7589,
      "step": 23850
    },
    {
      "epoch": 0.9420776246693252,
      "grad_norm": 0.5567736625671387,
      "learning_rate": 5.28980929442887e-05,
      "loss": 0.7478,
      "step": 23860
    },
    {
      "epoch": 0.9424724602203183,
      "grad_norm": 0.6737360954284668,
      "learning_rate": 5.287835116673906e-05,
      "loss": 0.7722,
      "step": 23870
    },
    {
      "epoch": 0.9428672957713112,
      "grad_norm": 0.7233296036720276,
      "learning_rate": 5.285860938918941e-05,
      "loss": 0.742,
      "step": 23880
    },
    {
      "epoch": 0.9432621313223043,
      "grad_norm": 0.7514002919197083,
      "learning_rate": 5.283886761163975e-05,
      "loss": 0.7545,
      "step": 23890
    },
    {
      "epoch": 0.9436569668732973,
      "grad_norm": 0.6479358673095703,
      "learning_rate": 5.28191258340901e-05,
      "loss": 0.7722,
      "step": 23900
    },
    {
      "epoch": 0.9440518024242903,
      "grad_norm": 0.5840457081794739,
      "learning_rate": 5.2799384056540456e-05,
      "loss": 0.7375,
      "step": 23910
    },
    {
      "epoch": 0.9444466379752833,
      "grad_norm": 0.6919030547142029,
      "learning_rate": 5.2779642278990806e-05,
      "loss": 0.7786,
      "step": 23920
    },
    {
      "epoch": 0.9448414735262763,
      "grad_norm": 0.6808739900588989,
      "learning_rate": 5.275990050144115e-05,
      "loss": 0.7833,
      "step": 23930
    },
    {
      "epoch": 0.9452363090772693,
      "grad_norm": 0.7258140444755554,
      "learning_rate": 5.27401587238915e-05,
      "loss": 0.7736,
      "step": 23940
    },
    {
      "epoch": 0.9456311446282624,
      "grad_norm": 0.5623875260353088,
      "learning_rate": 5.2720416946341856e-05,
      "loss": 0.7188,
      "step": 23950
    },
    {
      "epoch": 0.9460259801792553,
      "grad_norm": 0.5970304608345032,
      "learning_rate": 5.27006751687922e-05,
      "loss": 0.7517,
      "step": 23960
    },
    {
      "epoch": 0.9464208157302484,
      "grad_norm": 0.6858279705047607,
      "learning_rate": 5.268093339124255e-05,
      "loss": 0.761,
      "step": 23970
    },
    {
      "epoch": 0.9468156512812413,
      "grad_norm": 0.6776041984558105,
      "learning_rate": 5.266119161369289e-05,
      "loss": 0.7215,
      "step": 23980
    },
    {
      "epoch": 0.9472104868322344,
      "grad_norm": 0.8701377511024475,
      "learning_rate": 5.2641449836143256e-05,
      "loss": 0.7899,
      "step": 23990
    },
    {
      "epoch": 0.9476053223832274,
      "grad_norm": 0.5907626748085022,
      "learning_rate": 5.26217080585936e-05,
      "loss": 0.7253,
      "step": 24000
    },
    {
      "epoch": 0.9476053223832274,
      "eval_loss": 0.7756940126419067,
      "eval_runtime": 1220.4078,
      "eval_samples_per_second": 9.224,
      "eval_steps_per_second": 9.224,
      "step": 24000
    },
    {
      "epoch": 0.9480001579342204,
      "grad_norm": 0.507584273815155,
      "learning_rate": 5.260196628104395e-05,
      "loss": 0.7343,
      "step": 24010
    },
    {
      "epoch": 0.9483949934852134,
      "grad_norm": 0.727943480014801,
      "learning_rate": 5.258222450349429e-05,
      "loss": 0.7473,
      "step": 24020
    },
    {
      "epoch": 0.9487898290362065,
      "grad_norm": 0.5688599944114685,
      "learning_rate": 5.256248272594465e-05,
      "loss": 0.7696,
      "step": 24030
    },
    {
      "epoch": 0.9491846645871994,
      "grad_norm": 0.8207051157951355,
      "learning_rate": 5.2542740948395e-05,
      "loss": 0.775,
      "step": 24040
    },
    {
      "epoch": 0.9495795001381925,
      "grad_norm": 1.0194498300552368,
      "learning_rate": 5.252299917084534e-05,
      "loss": 0.7653,
      "step": 24050
    },
    {
      "epoch": 0.9499743356891854,
      "grad_norm": 0.7164926528930664,
      "learning_rate": 5.250325739329569e-05,
      "loss": 0.7737,
      "step": 24060
    },
    {
      "epoch": 0.9503691712401785,
      "grad_norm": 0.6595232486724854,
      "learning_rate": 5.248351561574605e-05,
      "loss": 0.7257,
      "step": 24070
    },
    {
      "epoch": 0.9507640067911715,
      "grad_norm": 0.6840726733207703,
      "learning_rate": 5.24637738381964e-05,
      "loss": 0.7557,
      "step": 24080
    },
    {
      "epoch": 0.9511588423421645,
      "grad_norm": 0.6056532859802246,
      "learning_rate": 5.244403206064674e-05,
      "loss": 0.739,
      "step": 24090
    },
    {
      "epoch": 0.9515536778931575,
      "grad_norm": 0.5530741214752197,
      "learning_rate": 5.242429028309709e-05,
      "loss": 0.7331,
      "step": 24100
    },
    {
      "epoch": 0.9519485134441505,
      "grad_norm": 0.5029979348182678,
      "learning_rate": 5.240454850554745e-05,
      "loss": 0.7535,
      "step": 24110
    },
    {
      "epoch": 0.9523433489951435,
      "grad_norm": 0.6970428824424744,
      "learning_rate": 5.238480672799779e-05,
      "loss": 0.7448,
      "step": 24120
    },
    {
      "epoch": 0.9527381845461366,
      "grad_norm": 0.8355937004089355,
      "learning_rate": 5.236506495044814e-05,
      "loss": 0.7383,
      "step": 24130
    },
    {
      "epoch": 0.9531330200971295,
      "grad_norm": 0.7585216760635376,
      "learning_rate": 5.2345323172898484e-05,
      "loss": 0.7591,
      "step": 24140
    },
    {
      "epoch": 0.9535278556481226,
      "grad_norm": 0.5761707425117493,
      "learning_rate": 5.232558139534884e-05,
      "loss": 0.7454,
      "step": 24150
    },
    {
      "epoch": 0.9539226911991155,
      "grad_norm": 0.7319878339767456,
      "learning_rate": 5.230583961779919e-05,
      "loss": 0.7722,
      "step": 24160
    },
    {
      "epoch": 0.9543175267501086,
      "grad_norm": 0.6879215836524963,
      "learning_rate": 5.2286097840249534e-05,
      "loss": 0.702,
      "step": 24170
    },
    {
      "epoch": 0.9547123623011016,
      "grad_norm": 0.7809497117996216,
      "learning_rate": 5.2266356062699884e-05,
      "loss": 0.7268,
      "step": 24180
    },
    {
      "epoch": 0.9551071978520946,
      "grad_norm": 0.6075472831726074,
      "learning_rate": 5.224661428515024e-05,
      "loss": 0.7547,
      "step": 24190
    },
    {
      "epoch": 0.9555020334030876,
      "grad_norm": 0.6522313952445984,
      "learning_rate": 5.222687250760059e-05,
      "loss": 0.7654,
      "step": 24200
    },
    {
      "epoch": 0.9558968689540807,
      "grad_norm": 0.6663751006126404,
      "learning_rate": 5.2207130730050934e-05,
      "loss": 0.7507,
      "step": 24210
    },
    {
      "epoch": 0.9562917045050736,
      "grad_norm": 0.8468092679977417,
      "learning_rate": 5.2187388952501283e-05,
      "loss": 0.8093,
      "step": 24220
    },
    {
      "epoch": 0.9566865400560667,
      "grad_norm": 0.7050454020500183,
      "learning_rate": 5.216764717495164e-05,
      "loss": 0.7828,
      "step": 24230
    },
    {
      "epoch": 0.9570813756070596,
      "grad_norm": 0.87821364402771,
      "learning_rate": 5.214790539740198e-05,
      "loss": 0.7935,
      "step": 24240
    },
    {
      "epoch": 0.9574762111580527,
      "grad_norm": 0.589400589466095,
      "learning_rate": 5.212816361985233e-05,
      "loss": 0.7884,
      "step": 24250
    },
    {
      "epoch": 0.9578710467090457,
      "grad_norm": 0.5602564811706543,
      "learning_rate": 5.2108421842302676e-05,
      "loss": 0.6974,
      "step": 24260
    },
    {
      "epoch": 0.9582658822600387,
      "grad_norm": 0.7239298820495605,
      "learning_rate": 5.208868006475304e-05,
      "loss": 0.7599,
      "step": 24270
    },
    {
      "epoch": 0.9586607178110317,
      "grad_norm": 0.6882236003875732,
      "learning_rate": 5.206893828720338e-05,
      "loss": 0.7153,
      "step": 24280
    },
    {
      "epoch": 0.9590555533620247,
      "grad_norm": 0.6796707510948181,
      "learning_rate": 5.204919650965373e-05,
      "loss": 0.7447,
      "step": 24290
    },
    {
      "epoch": 0.9594503889130177,
      "grad_norm": 0.6979352235794067,
      "learning_rate": 5.2029454732104076e-05,
      "loss": 0.7932,
      "step": 24300
    },
    {
      "epoch": 0.9598452244640108,
      "grad_norm": 0.7290498614311218,
      "learning_rate": 5.200971295455443e-05,
      "loss": 0.748,
      "step": 24310
    },
    {
      "epoch": 0.9602400600150037,
      "grad_norm": 0.6839702129364014,
      "learning_rate": 5.198997117700478e-05,
      "loss": 0.7426,
      "step": 24320
    },
    {
      "epoch": 0.9606348955659968,
      "grad_norm": 0.6245478391647339,
      "learning_rate": 5.1970229399455126e-05,
      "loss": 0.7653,
      "step": 24330
    },
    {
      "epoch": 0.9610297311169897,
      "grad_norm": 0.7324169278144836,
      "learning_rate": 5.1950487621905476e-05,
      "loss": 0.7493,
      "step": 24340
    },
    {
      "epoch": 0.9614245666679828,
      "grad_norm": 0.5845973491668701,
      "learning_rate": 5.193074584435583e-05,
      "loss": 0.7727,
      "step": 24350
    },
    {
      "epoch": 0.9618194022189758,
      "grad_norm": 0.6797917485237122,
      "learning_rate": 5.191100406680618e-05,
      "loss": 0.7612,
      "step": 24360
    },
    {
      "epoch": 0.9622142377699688,
      "grad_norm": 0.6854456663131714,
      "learning_rate": 5.1891262289256525e-05,
      "loss": 0.7453,
      "step": 24370
    },
    {
      "epoch": 0.9626090733209618,
      "grad_norm": 0.5761273503303528,
      "learning_rate": 5.1871520511706875e-05,
      "loss": 0.7831,
      "step": 24380
    },
    {
      "epoch": 0.9630039088719549,
      "grad_norm": 0.6833099722862244,
      "learning_rate": 5.185177873415723e-05,
      "loss": 0.741,
      "step": 24390
    },
    {
      "epoch": 0.9633987444229478,
      "grad_norm": 0.8588469624519348,
      "learning_rate": 5.1832036956607575e-05,
      "loss": 0.7588,
      "step": 24400
    },
    {
      "epoch": 0.9637935799739409,
      "grad_norm": 0.6478479504585266,
      "learning_rate": 5.1812295179057925e-05,
      "loss": 0.7166,
      "step": 24410
    },
    {
      "epoch": 0.9641884155249338,
      "grad_norm": 0.9183929562568665,
      "learning_rate": 5.179255340150827e-05,
      "loss": 0.7489,
      "step": 24420
    },
    {
      "epoch": 0.9645832510759269,
      "grad_norm": 0.6975483298301697,
      "learning_rate": 5.1772811623958625e-05,
      "loss": 0.7479,
      "step": 24430
    },
    {
      "epoch": 0.9649780866269199,
      "grad_norm": 0.6302173733711243,
      "learning_rate": 5.1753069846408975e-05,
      "loss": 0.7648,
      "step": 24440
    },
    {
      "epoch": 0.9653729221779129,
      "grad_norm": 0.7452186346054077,
      "learning_rate": 5.1733328068859325e-05,
      "loss": 0.7693,
      "step": 24450
    },
    {
      "epoch": 0.9657677577289059,
      "grad_norm": 0.8475795984268188,
      "learning_rate": 5.171358629130967e-05,
      "loss": 0.7953,
      "step": 24460
    },
    {
      "epoch": 0.966162593279899,
      "grad_norm": 0.6468015313148499,
      "learning_rate": 5.1693844513760025e-05,
      "loss": 0.7697,
      "step": 24470
    },
    {
      "epoch": 0.9665574288308919,
      "grad_norm": 0.8016439080238342,
      "learning_rate": 5.1674102736210374e-05,
      "loss": 0.7563,
      "step": 24480
    },
    {
      "epoch": 0.966952264381885,
      "grad_norm": 0.7676268815994263,
      "learning_rate": 5.165436095866072e-05,
      "loss": 0.7722,
      "step": 24490
    },
    {
      "epoch": 0.9673470999328779,
      "grad_norm": 0.5418907999992371,
      "learning_rate": 5.163461918111107e-05,
      "loss": 0.797,
      "step": 24500
    },
    {
      "epoch": 0.967741935483871,
      "grad_norm": 0.6864181756973267,
      "learning_rate": 5.1614877403561424e-05,
      "loss": 0.7664,
      "step": 24510
    },
    {
      "epoch": 0.968136771034864,
      "grad_norm": 0.867748498916626,
      "learning_rate": 5.159513562601177e-05,
      "loss": 0.7245,
      "step": 24520
    },
    {
      "epoch": 0.968531606585857,
      "grad_norm": 0.7512654662132263,
      "learning_rate": 5.157539384846212e-05,
      "loss": 0.7681,
      "step": 24530
    },
    {
      "epoch": 0.96892644213685,
      "grad_norm": 0.7165803909301758,
      "learning_rate": 5.155565207091246e-05,
      "loss": 0.7486,
      "step": 24540
    },
    {
      "epoch": 0.969321277687843,
      "grad_norm": 0.6567918062210083,
      "learning_rate": 5.1535910293362824e-05,
      "loss": 0.7635,
      "step": 24550
    },
    {
      "epoch": 0.969716113238836,
      "grad_norm": 0.7379655838012695,
      "learning_rate": 5.151616851581317e-05,
      "loss": 0.7275,
      "step": 24560
    },
    {
      "epoch": 0.9701109487898291,
      "grad_norm": 0.7537217140197754,
      "learning_rate": 5.149642673826352e-05,
      "loss": 0.7439,
      "step": 24570
    },
    {
      "epoch": 0.970505784340822,
      "grad_norm": 0.6213301420211792,
      "learning_rate": 5.147668496071386e-05,
      "loss": 0.7515,
      "step": 24580
    },
    {
      "epoch": 0.9709006198918151,
      "grad_norm": 0.7285314798355103,
      "learning_rate": 5.145694318316422e-05,
      "loss": 0.7893,
      "step": 24590
    },
    {
      "epoch": 0.971295455442808,
      "grad_norm": 0.5836297273635864,
      "learning_rate": 5.143720140561457e-05,
      "loss": 0.7799,
      "step": 24600
    },
    {
      "epoch": 0.9716902909938011,
      "grad_norm": 0.5522946715354919,
      "learning_rate": 5.141745962806491e-05,
      "loss": 0.7369,
      "step": 24610
    },
    {
      "epoch": 0.9720851265447941,
      "grad_norm": 0.8459252715110779,
      "learning_rate": 5.139771785051526e-05,
      "loss": 0.7533,
      "step": 24620
    },
    {
      "epoch": 0.9724799620957871,
      "grad_norm": 0.5978450775146484,
      "learning_rate": 5.1377976072965616e-05,
      "loss": 0.697,
      "step": 24630
    },
    {
      "epoch": 0.9728747976467801,
      "grad_norm": 0.8702607154846191,
      "learning_rate": 5.1358234295415966e-05,
      "loss": 0.7789,
      "step": 24640
    },
    {
      "epoch": 0.9732696331977732,
      "grad_norm": 0.6413499116897583,
      "learning_rate": 5.133849251786631e-05,
      "loss": 0.7604,
      "step": 24650
    },
    {
      "epoch": 0.9736644687487661,
      "grad_norm": 0.7703062891960144,
      "learning_rate": 5.131875074031666e-05,
      "loss": 0.7801,
      "step": 24660
    },
    {
      "epoch": 0.9740593042997592,
      "grad_norm": 0.6298730373382568,
      "learning_rate": 5.1299008962767016e-05,
      "loss": 0.7805,
      "step": 24670
    },
    {
      "epoch": 0.9744541398507521,
      "grad_norm": 0.8341426849365234,
      "learning_rate": 5.127926718521736e-05,
      "loss": 0.7876,
      "step": 24680
    },
    {
      "epoch": 0.9748489754017452,
      "grad_norm": 0.6476655006408691,
      "learning_rate": 5.125952540766771e-05,
      "loss": 0.7494,
      "step": 24690
    },
    {
      "epoch": 0.9752438109527382,
      "grad_norm": 0.6533557176589966,
      "learning_rate": 5.123978363011805e-05,
      "loss": 0.7513,
      "step": 24700
    },
    {
      "epoch": 0.9756386465037312,
      "grad_norm": 0.5875779986381531,
      "learning_rate": 5.122004185256841e-05,
      "loss": 0.7668,
      "step": 24710
    },
    {
      "epoch": 0.9760334820547242,
      "grad_norm": 0.7631396055221558,
      "learning_rate": 5.120030007501876e-05,
      "loss": 0.7918,
      "step": 24720
    },
    {
      "epoch": 0.9764283176057172,
      "grad_norm": 0.9912247657775879,
      "learning_rate": 5.118055829746911e-05,
      "loss": 0.7526,
      "step": 24730
    },
    {
      "epoch": 0.9768231531567102,
      "grad_norm": 0.7103157043457031,
      "learning_rate": 5.116081651991945e-05,
      "loss": 0.7953,
      "step": 24740
    },
    {
      "epoch": 0.9772179887077033,
      "grad_norm": 0.5874265432357788,
      "learning_rate": 5.114107474236981e-05,
      "loss": 0.7876,
      "step": 24750
    },
    {
      "epoch": 0.9776128242586962,
      "grad_norm": 0.6617926955223083,
      "learning_rate": 5.112133296482016e-05,
      "loss": 0.7642,
      "step": 24760
    },
    {
      "epoch": 0.9780076598096893,
      "grad_norm": 0.865167498588562,
      "learning_rate": 5.11015911872705e-05,
      "loss": 0.7802,
      "step": 24770
    },
    {
      "epoch": 0.9784024953606822,
      "grad_norm": 0.5506856441497803,
      "learning_rate": 5.108184940972085e-05,
      "loss": 0.751,
      "step": 24780
    },
    {
      "epoch": 0.9787973309116753,
      "grad_norm": 0.6219415664672852,
      "learning_rate": 5.106210763217121e-05,
      "loss": 0.7398,
      "step": 24790
    },
    {
      "epoch": 0.9791921664626683,
      "grad_norm": 0.810820460319519,
      "learning_rate": 5.104236585462155e-05,
      "loss": 0.7986,
      "step": 24800
    },
    {
      "epoch": 0.9795870020136613,
      "grad_norm": 0.7649339437484741,
      "learning_rate": 5.10226240770719e-05,
      "loss": 0.793,
      "step": 24810
    },
    {
      "epoch": 0.9799818375646543,
      "grad_norm": 0.6550641059875488,
      "learning_rate": 5.1002882299522244e-05,
      "loss": 0.7374,
      "step": 24820
    },
    {
      "epoch": 0.9803766731156474,
      "grad_norm": 0.6568483710289001,
      "learning_rate": 5.098314052197261e-05,
      "loss": 0.733,
      "step": 24830
    },
    {
      "epoch": 0.9807715086666403,
      "grad_norm": 0.687482476234436,
      "learning_rate": 5.096339874442295e-05,
      "loss": 0.7507,
      "step": 24840
    },
    {
      "epoch": 0.9811663442176334,
      "grad_norm": 0.7058338522911072,
      "learning_rate": 5.09436569668733e-05,
      "loss": 0.7257,
      "step": 24850
    },
    {
      "epoch": 0.9815611797686263,
      "grad_norm": 0.6356987953186035,
      "learning_rate": 5.0923915189323644e-05,
      "loss": 0.8179,
      "step": 24860
    },
    {
      "epoch": 0.9819560153196194,
      "grad_norm": 0.7436060309410095,
      "learning_rate": 5.0904173411774e-05,
      "loss": 0.7177,
      "step": 24870
    },
    {
      "epoch": 0.9823508508706124,
      "grad_norm": 0.620699942111969,
      "learning_rate": 5.088443163422435e-05,
      "loss": 0.7558,
      "step": 24880
    },
    {
      "epoch": 0.9827456864216054,
      "grad_norm": 0.8980542421340942,
      "learning_rate": 5.0864689856674694e-05,
      "loss": 0.7775,
      "step": 24890
    },
    {
      "epoch": 0.9831405219725984,
      "grad_norm": 0.7797737121582031,
      "learning_rate": 5.0844948079125044e-05,
      "loss": 0.7449,
      "step": 24900
    },
    {
      "epoch": 0.9835353575235914,
      "grad_norm": 0.6138296723365784,
      "learning_rate": 5.08252063015754e-05,
      "loss": 0.7544,
      "step": 24910
    },
    {
      "epoch": 0.9839301930745844,
      "grad_norm": 0.6850663423538208,
      "learning_rate": 5.080546452402575e-05,
      "loss": 0.7255,
      "step": 24920
    },
    {
      "epoch": 0.9843250286255775,
      "grad_norm": 0.788729727268219,
      "learning_rate": 5.0785722746476093e-05,
      "loss": 0.7856,
      "step": 24930
    },
    {
      "epoch": 0.9847198641765704,
      "grad_norm": 0.587067723274231,
      "learning_rate": 5.076598096892644e-05,
      "loss": 0.7392,
      "step": 24940
    },
    {
      "epoch": 0.9851146997275635,
      "grad_norm": 0.8060004711151123,
      "learning_rate": 5.07462391913768e-05,
      "loss": 0.771,
      "step": 24950
    },
    {
      "epoch": 0.9855095352785564,
      "grad_norm": 1.0146093368530273,
      "learning_rate": 5.072649741382714e-05,
      "loss": 0.7645,
      "step": 24960
    },
    {
      "epoch": 0.9859043708295495,
      "grad_norm": 0.683570384979248,
      "learning_rate": 5.070675563627749e-05,
      "loss": 0.7518,
      "step": 24970
    },
    {
      "epoch": 0.9862992063805425,
      "grad_norm": 0.772046685218811,
      "learning_rate": 5.0687013858727836e-05,
      "loss": 0.7835,
      "step": 24980
    },
    {
      "epoch": 0.9866940419315355,
      "grad_norm": 0.5909391045570374,
      "learning_rate": 5.06672720811782e-05,
      "loss": 0.7651,
      "step": 24990
    },
    {
      "epoch": 0.9870888774825285,
      "grad_norm": 0.6889273524284363,
      "learning_rate": 5.064753030362854e-05,
      "loss": 0.766,
      "step": 25000
    },
    {
      "epoch": 0.9870888774825285,
      "eval_loss": 0.77464759349823,
      "eval_runtime": 1217.676,
      "eval_samples_per_second": 9.245,
      "eval_steps_per_second": 9.245,
      "step": 25000
    },
    {
      "epoch": 0.9874837130335216,
      "grad_norm": 0.6630967855453491,
      "learning_rate": 5.062778852607889e-05,
      "loss": 0.7912,
      "step": 25010
    },
    {
      "epoch": 0.9878785485845145,
      "grad_norm": 0.6203853487968445,
      "learning_rate": 5.0608046748529236e-05,
      "loss": 0.7243,
      "step": 25020
    },
    {
      "epoch": 0.9882733841355076,
      "grad_norm": 0.6398270726203918,
      "learning_rate": 5.058830497097959e-05,
      "loss": 0.7731,
      "step": 25030
    },
    {
      "epoch": 0.9886682196865005,
      "grad_norm": 0.7979245185852051,
      "learning_rate": 5.056856319342994e-05,
      "loss": 0.7821,
      "step": 25040
    },
    {
      "epoch": 0.9890630552374936,
      "grad_norm": 0.6518794894218445,
      "learning_rate": 5.0548821415880286e-05,
      "loss": 0.7213,
      "step": 25050
    },
    {
      "epoch": 0.9894578907884866,
      "grad_norm": 0.6775597333908081,
      "learning_rate": 5.0529079638330636e-05,
      "loss": 0.7541,
      "step": 25060
    },
    {
      "epoch": 0.9898527263394796,
      "grad_norm": 0.6898284554481506,
      "learning_rate": 5.050933786078099e-05,
      "loss": 0.7302,
      "step": 25070
    },
    {
      "epoch": 0.9902475618904726,
      "grad_norm": 0.5668608546257019,
      "learning_rate": 5.0489596083231335e-05,
      "loss": 0.7455,
      "step": 25080
    },
    {
      "epoch": 0.9906423974414656,
      "grad_norm": 0.5563997626304626,
      "learning_rate": 5.0469854305681685e-05,
      "loss": 0.7429,
      "step": 25090
    },
    {
      "epoch": 0.9910372329924586,
      "grad_norm": 0.7514516115188599,
      "learning_rate": 5.0450112528132035e-05,
      "loss": 0.7538,
      "step": 25100
    },
    {
      "epoch": 0.9914320685434517,
      "grad_norm": 0.7036972045898438,
      "learning_rate": 5.043037075058239e-05,
      "loss": 0.8018,
      "step": 25110
    },
    {
      "epoch": 0.9918269040944446,
      "grad_norm": 0.7489221096038818,
      "learning_rate": 5.0410628973032735e-05,
      "loss": 0.7995,
      "step": 25120
    },
    {
      "epoch": 0.9922217396454377,
      "grad_norm": 0.641272246837616,
      "learning_rate": 5.0390887195483085e-05,
      "loss": 0.7731,
      "step": 25130
    },
    {
      "epoch": 0.9926165751964307,
      "grad_norm": 0.591301679611206,
      "learning_rate": 5.037114541793343e-05,
      "loss": 0.7801,
      "step": 25140
    },
    {
      "epoch": 0.9930114107474237,
      "grad_norm": 0.6092597842216492,
      "learning_rate": 5.0351403640383785e-05,
      "loss": 0.7361,
      "step": 25150
    },
    {
      "epoch": 0.9934062462984167,
      "grad_norm": 0.7007703185081482,
      "learning_rate": 5.0331661862834135e-05,
      "loss": 0.7894,
      "step": 25160
    },
    {
      "epoch": 0.9938010818494097,
      "grad_norm": 0.5556220412254333,
      "learning_rate": 5.031192008528448e-05,
      "loss": 0.7118,
      "step": 25170
    },
    {
      "epoch": 0.9941959174004027,
      "grad_norm": 0.7173001766204834,
      "learning_rate": 5.029217830773483e-05,
      "loss": 0.739,
      "step": 25180
    },
    {
      "epoch": 0.9945907529513958,
      "grad_norm": 0.5998225808143616,
      "learning_rate": 5.0272436530185184e-05,
      "loss": 0.7876,
      "step": 25190
    },
    {
      "epoch": 0.9949855885023887,
      "grad_norm": 0.6498355269432068,
      "learning_rate": 5.0252694752635534e-05,
      "loss": 0.7481,
      "step": 25200
    },
    {
      "epoch": 0.9953804240533818,
      "grad_norm": 0.6638177633285522,
      "learning_rate": 5.023295297508588e-05,
      "loss": 0.7319,
      "step": 25210
    },
    {
      "epoch": 0.9957752596043747,
      "grad_norm": 0.6361849904060364,
      "learning_rate": 5.021321119753623e-05,
      "loss": 0.7624,
      "step": 25220
    },
    {
      "epoch": 0.9961700951553678,
      "grad_norm": 0.6734476685523987,
      "learning_rate": 5.0193469419986584e-05,
      "loss": 0.7517,
      "step": 25230
    },
    {
      "epoch": 0.9965649307063608,
      "grad_norm": 0.5747575163841248,
      "learning_rate": 5.017372764243693e-05,
      "loss": 0.7056,
      "step": 25240
    },
    {
      "epoch": 0.9969597662573538,
      "grad_norm": 0.748163640499115,
      "learning_rate": 5.015398586488728e-05,
      "loss": 0.7739,
      "step": 25250
    },
    {
      "epoch": 0.9973546018083468,
      "grad_norm": 0.6505012512207031,
      "learning_rate": 5.013424408733762e-05,
      "loss": 0.7767,
      "step": 25260
    },
    {
      "epoch": 0.9977494373593399,
      "grad_norm": 0.6706058979034424,
      "learning_rate": 5.0114502309787984e-05,
      "loss": 0.7278,
      "step": 25270
    },
    {
      "epoch": 0.9981442729103328,
      "grad_norm": 0.658446729183197,
      "learning_rate": 5.009476053223833e-05,
      "loss": 0.8013,
      "step": 25280
    },
    {
      "epoch": 0.9985391084613259,
      "grad_norm": 0.6232558488845825,
      "learning_rate": 5.007501875468868e-05,
      "loss": 0.7618,
      "step": 25290
    },
    {
      "epoch": 0.9989339440123188,
      "grad_norm": 0.5887000560760498,
      "learning_rate": 5.005527697713902e-05,
      "loss": 0.7739,
      "step": 25300
    },
    {
      "epoch": 0.9993287795633119,
      "grad_norm": 0.5986097455024719,
      "learning_rate": 5.003553519958938e-05,
      "loss": 0.7271,
      "step": 25310
    },
    {
      "epoch": 0.9997236151143049,
      "grad_norm": 0.6850456595420837,
      "learning_rate": 5.0015793422039727e-05,
      "loss": 0.7675,
      "step": 25320
    },
    {
      "epoch": 1.0001184506652978,
      "grad_norm": 0.6077843904495239,
      "learning_rate": 4.999605164449007e-05,
      "loss": 0.752,
      "step": 25330
    },
    {
      "epoch": 1.0005132862162909,
      "grad_norm": 0.6277823448181152,
      "learning_rate": 4.9976309866940426e-05,
      "loss": 0.7333,
      "step": 25340
    },
    {
      "epoch": 1.000908121767284,
      "grad_norm": 0.7015221118927002,
      "learning_rate": 4.995656808939077e-05,
      "loss": 0.7017,
      "step": 25350
    },
    {
      "epoch": 1.001302957318277,
      "grad_norm": 0.6246042847633362,
      "learning_rate": 4.9936826311841126e-05,
      "loss": 0.7174,
      "step": 25360
    },
    {
      "epoch": 1.0016977928692699,
      "grad_norm": 0.6506706476211548,
      "learning_rate": 4.991708453429147e-05,
      "loss": 0.706,
      "step": 25370
    },
    {
      "epoch": 1.002092628420263,
      "grad_norm": 0.8577408194541931,
      "learning_rate": 4.989734275674182e-05,
      "loss": 0.7319,
      "step": 25380
    },
    {
      "epoch": 1.002487463971256,
      "grad_norm": 0.8254954814910889,
      "learning_rate": 4.987760097919217e-05,
      "loss": 0.7015,
      "step": 25390
    },
    {
      "epoch": 1.002882299522249,
      "grad_norm": 0.7131408452987671,
      "learning_rate": 4.985785920164252e-05,
      "loss": 0.6918,
      "step": 25400
    },
    {
      "epoch": 1.003277135073242,
      "grad_norm": 0.7373110055923462,
      "learning_rate": 4.983811742409287e-05,
      "loss": 0.7296,
      "step": 25410
    },
    {
      "epoch": 1.003671970624235,
      "grad_norm": 0.6201266646385193,
      "learning_rate": 4.981837564654322e-05,
      "loss": 0.6851,
      "step": 25420
    },
    {
      "epoch": 1.004066806175228,
      "grad_norm": 0.576078474521637,
      "learning_rate": 4.979863386899357e-05,
      "loss": 0.7556,
      "step": 25430
    },
    {
      "epoch": 1.004461641726221,
      "grad_norm": 0.648994505405426,
      "learning_rate": 4.977889209144392e-05,
      "loss": 0.7028,
      "step": 25440
    },
    {
      "epoch": 1.004856477277214,
      "grad_norm": 0.623231828212738,
      "learning_rate": 4.975915031389426e-05,
      "loss": 0.7151,
      "step": 25450
    },
    {
      "epoch": 1.005251312828207,
      "grad_norm": 0.7365100979804993,
      "learning_rate": 4.973940853634462e-05,
      "loss": 0.738,
      "step": 25460
    },
    {
      "epoch": 1.0056461483792,
      "grad_norm": 0.6309475898742676,
      "learning_rate": 4.971966675879496e-05,
      "loss": 0.7175,
      "step": 25470
    },
    {
      "epoch": 1.0060409839301931,
      "grad_norm": 0.7287161350250244,
      "learning_rate": 4.969992498124532e-05,
      "loss": 0.7423,
      "step": 25480
    },
    {
      "epoch": 1.006435819481186,
      "grad_norm": 0.6459001302719116,
      "learning_rate": 4.968018320369566e-05,
      "loss": 0.6856,
      "step": 25490
    },
    {
      "epoch": 1.006830655032179,
      "grad_norm": 0.7624677419662476,
      "learning_rate": 4.966044142614602e-05,
      "loss": 0.7602,
      "step": 25500
    },
    {
      "epoch": 1.0072254905831721,
      "grad_norm": 0.7721124291419983,
      "learning_rate": 4.964069964859636e-05,
      "loss": 0.7344,
      "step": 25510
    },
    {
      "epoch": 1.0076203261341652,
      "grad_norm": 0.7362136840820312,
      "learning_rate": 4.962095787104671e-05,
      "loss": 0.7862,
      "step": 25520
    },
    {
      "epoch": 1.008015161685158,
      "grad_norm": 0.5886848568916321,
      "learning_rate": 4.960121609349706e-05,
      "loss": 0.7282,
      "step": 25530
    },
    {
      "epoch": 1.008409997236151,
      "grad_norm": 0.6351431608200073,
      "learning_rate": 4.958147431594741e-05,
      "loss": 0.7144,
      "step": 25540
    },
    {
      "epoch": 1.0088048327871442,
      "grad_norm": 0.6776130199432373,
      "learning_rate": 4.956173253839776e-05,
      "loss": 0.6892,
      "step": 25550
    },
    {
      "epoch": 1.0091996683381372,
      "grad_norm": 1.0250097513198853,
      "learning_rate": 4.954199076084811e-05,
      "loss": 0.741,
      "step": 25560
    },
    {
      "epoch": 1.00959450388913,
      "grad_norm": 0.6545882821083069,
      "learning_rate": 4.952224898329846e-05,
      "loss": 0.6945,
      "step": 25570
    },
    {
      "epoch": 1.0099893394401231,
      "grad_norm": 0.7548978924751282,
      "learning_rate": 4.950250720574881e-05,
      "loss": 0.7052,
      "step": 25580
    },
    {
      "epoch": 1.0103841749911162,
      "grad_norm": 0.7567318081855774,
      "learning_rate": 4.9482765428199154e-05,
      "loss": 0.7091,
      "step": 25590
    },
    {
      "epoch": 1.0107790105421093,
      "grad_norm": 0.7872278690338135,
      "learning_rate": 4.946302365064951e-05,
      "loss": 0.6848,
      "step": 25600
    },
    {
      "epoch": 1.0111738460931021,
      "grad_norm": 0.6681800484657288,
      "learning_rate": 4.9443281873099854e-05,
      "loss": 0.7272,
      "step": 25610
    },
    {
      "epoch": 1.0115686816440952,
      "grad_norm": 0.6063538789749146,
      "learning_rate": 4.942354009555021e-05,
      "loss": 0.7257,
      "step": 25620
    },
    {
      "epoch": 1.0119635171950883,
      "grad_norm": 0.7049441337585449,
      "learning_rate": 4.9403798318000554e-05,
      "loss": 0.7367,
      "step": 25630
    },
    {
      "epoch": 1.0123583527460813,
      "grad_norm": 0.5642848014831543,
      "learning_rate": 4.938405654045091e-05,
      "loss": 0.7599,
      "step": 25640
    },
    {
      "epoch": 1.0127531882970742,
      "grad_norm": 0.838900625705719,
      "learning_rate": 4.936431476290125e-05,
      "loss": 0.6954,
      "step": 25650
    },
    {
      "epoch": 1.0131480238480672,
      "grad_norm": 0.7031674981117249,
      "learning_rate": 4.93445729853516e-05,
      "loss": 0.6944,
      "step": 25660
    },
    {
      "epoch": 1.0135428593990603,
      "grad_norm": 0.7922678589820862,
      "learning_rate": 4.932483120780195e-05,
      "loss": 0.7151,
      "step": 25670
    },
    {
      "epoch": 1.0139376949500534,
      "grad_norm": 0.6277074813842773,
      "learning_rate": 4.93050894302523e-05,
      "loss": 0.7279,
      "step": 25680
    },
    {
      "epoch": 1.0143325305010462,
      "grad_norm": 0.8423132300376892,
      "learning_rate": 4.928534765270265e-05,
      "loss": 0.783,
      "step": 25690
    },
    {
      "epoch": 1.0147273660520393,
      "grad_norm": 0.690549910068512,
      "learning_rate": 4.9265605875153e-05,
      "loss": 0.7167,
      "step": 25700
    },
    {
      "epoch": 1.0151222016030323,
      "grad_norm": 0.5328707098960876,
      "learning_rate": 4.924586409760335e-05,
      "loss": 0.7245,
      "step": 25710
    },
    {
      "epoch": 1.0155170371540254,
      "grad_norm": 0.8793671727180481,
      "learning_rate": 4.92261223200537e-05,
      "loss": 0.6815,
      "step": 25720
    },
    {
      "epoch": 1.0159118727050183,
      "grad_norm": 0.5833209156990051,
      "learning_rate": 4.9206380542504046e-05,
      "loss": 0.7432,
      "step": 25730
    },
    {
      "epoch": 1.0163067082560113,
      "grad_norm": 0.6331603527069092,
      "learning_rate": 4.91866387649544e-05,
      "loss": 0.7099,
      "step": 25740
    },
    {
      "epoch": 1.0167015438070044,
      "grad_norm": 0.727934718132019,
      "learning_rate": 4.9166896987404746e-05,
      "loss": 0.7263,
      "step": 25750
    },
    {
      "epoch": 1.0170963793579975,
      "grad_norm": 0.5738426446914673,
      "learning_rate": 4.91471552098551e-05,
      "loss": 0.7157,
      "step": 25760
    },
    {
      "epoch": 1.0174912149089903,
      "grad_norm": 0.7175693511962891,
      "learning_rate": 4.9127413432305446e-05,
      "loss": 0.7064,
      "step": 25770
    },
    {
      "epoch": 1.0178860504599834,
      "grad_norm": 0.5953588485717773,
      "learning_rate": 4.91076716547558e-05,
      "loss": 0.7304,
      "step": 25780
    },
    {
      "epoch": 1.0182808860109764,
      "grad_norm": 0.8007057905197144,
      "learning_rate": 4.9087929877206145e-05,
      "loss": 0.7549,
      "step": 25790
    },
    {
      "epoch": 1.0186757215619695,
      "grad_norm": 0.8607452511787415,
      "learning_rate": 4.9068188099656495e-05,
      "loss": 0.7059,
      "step": 25800
    },
    {
      "epoch": 1.0190705571129623,
      "grad_norm": 0.5895666480064392,
      "learning_rate": 4.9048446322106845e-05,
      "loss": 0.7052,
      "step": 25810
    },
    {
      "epoch": 1.0194653926639554,
      "grad_norm": 0.5132613778114319,
      "learning_rate": 4.9028704544557195e-05,
      "loss": 0.6958,
      "step": 25820
    },
    {
      "epoch": 1.0198602282149485,
      "grad_norm": 0.6269766688346863,
      "learning_rate": 4.9008962767007545e-05,
      "loss": 0.7755,
      "step": 25830
    },
    {
      "epoch": 1.0202550637659416,
      "grad_norm": 0.6620681285858154,
      "learning_rate": 4.8989220989457895e-05,
      "loss": 0.7156,
      "step": 25840
    },
    {
      "epoch": 1.0206498993169344,
      "grad_norm": 0.7424439787864685,
      "learning_rate": 4.8969479211908245e-05,
      "loss": 0.6781,
      "step": 25850
    },
    {
      "epoch": 1.0210447348679275,
      "grad_norm": 0.911382794380188,
      "learning_rate": 4.8949737434358595e-05,
      "loss": 0.702,
      "step": 25860
    },
    {
      "epoch": 1.0214395704189205,
      "grad_norm": 0.8464640378952026,
      "learning_rate": 4.892999565680894e-05,
      "loss": 0.7165,
      "step": 25870
    },
    {
      "epoch": 1.0218344059699136,
      "grad_norm": 0.7239750623703003,
      "learning_rate": 4.8910253879259295e-05,
      "loss": 0.7212,
      "step": 25880
    },
    {
      "epoch": 1.0222292415209064,
      "grad_norm": 0.8571419715881348,
      "learning_rate": 4.889051210170964e-05,
      "loss": 0.7276,
      "step": 25890
    },
    {
      "epoch": 1.0226240770718995,
      "grad_norm": 0.6302492618560791,
      "learning_rate": 4.8870770324159994e-05,
      "loss": 0.7192,
      "step": 25900
    },
    {
      "epoch": 1.0230189126228926,
      "grad_norm": 0.7036004662513733,
      "learning_rate": 4.885102854661034e-05,
      "loss": 0.7376,
      "step": 25910
    },
    {
      "epoch": 1.0234137481738856,
      "grad_norm": 0.7555290460586548,
      "learning_rate": 4.8831286769060694e-05,
      "loss": 0.7042,
      "step": 25920
    },
    {
      "epoch": 1.0238085837248785,
      "grad_norm": 0.6208779811859131,
      "learning_rate": 4.881154499151104e-05,
      "loss": 0.7239,
      "step": 25930
    },
    {
      "epoch": 1.0242034192758716,
      "grad_norm": 0.7819297313690186,
      "learning_rate": 4.879180321396139e-05,
      "loss": 0.7039,
      "step": 25940
    },
    {
      "epoch": 1.0245982548268646,
      "grad_norm": 0.7643092274665833,
      "learning_rate": 4.877206143641174e-05,
      "loss": 0.7036,
      "step": 25950
    },
    {
      "epoch": 1.0249930903778577,
      "grad_norm": 0.624068558216095,
      "learning_rate": 4.875231965886209e-05,
      "loss": 0.7505,
      "step": 25960
    },
    {
      "epoch": 1.0253879259288505,
      "grad_norm": 0.7649586796760559,
      "learning_rate": 4.873257788131244e-05,
      "loss": 0.7176,
      "step": 25970
    },
    {
      "epoch": 1.0257827614798436,
      "grad_norm": 0.8375423550605774,
      "learning_rate": 4.871283610376279e-05,
      "loss": 0.6808,
      "step": 25980
    },
    {
      "epoch": 1.0261775970308367,
      "grad_norm": 0.5645016431808472,
      "learning_rate": 4.869309432621314e-05,
      "loss": 0.6841,
      "step": 25990
    },
    {
      "epoch": 1.0265724325818297,
      "grad_norm": 0.8990481495857239,
      "learning_rate": 4.867335254866349e-05,
      "loss": 0.7149,
      "step": 26000
    },
    {
      "epoch": 1.0265724325818297,
      "eval_loss": 0.7754077315330505,
      "eval_runtime": 1217.9257,
      "eval_samples_per_second": 9.243,
      "eval_steps_per_second": 9.243,
      "step": 26000
    },
    {
      "epoch": 1.0269672681328226,
      "grad_norm": 0.5947741866111755,
      "learning_rate": 4.865361077111384e-05,
      "loss": 0.7187,
      "step": 26010
    },
    {
      "epoch": 1.0273621036838156,
      "grad_norm": 0.622880220413208,
      "learning_rate": 4.863386899356419e-05,
      "loss": 0.7378,
      "step": 26020
    },
    {
      "epoch": 1.0277569392348087,
      "grad_norm": 0.8684437274932861,
      "learning_rate": 4.861412721601453e-05,
      "loss": 0.7226,
      "step": 26030
    },
    {
      "epoch": 1.0281517747858018,
      "grad_norm": 0.5681895613670349,
      "learning_rate": 4.8594385438464886e-05,
      "loss": 0.7024,
      "step": 26040
    },
    {
      "epoch": 1.0285466103367946,
      "grad_norm": 0.6800429224967957,
      "learning_rate": 4.857464366091523e-05,
      "loss": 0.6823,
      "step": 26050
    },
    {
      "epoch": 1.0289414458877877,
      "grad_norm": 0.7133615016937256,
      "learning_rate": 4.8554901883365586e-05,
      "loss": 0.7051,
      "step": 26060
    },
    {
      "epoch": 1.0293362814387808,
      "grad_norm": 0.8791096210479736,
      "learning_rate": 4.853516010581593e-05,
      "loss": 0.7356,
      "step": 26070
    },
    {
      "epoch": 1.0297311169897738,
      "grad_norm": 0.7503722906112671,
      "learning_rate": 4.851541832826628e-05,
      "loss": 0.6812,
      "step": 26080
    },
    {
      "epoch": 1.0301259525407667,
      "grad_norm": 0.735433042049408,
      "learning_rate": 4.849567655071663e-05,
      "loss": 0.7588,
      "step": 26090
    },
    {
      "epoch": 1.0305207880917597,
      "grad_norm": 0.8450652360916138,
      "learning_rate": 4.847593477316697e-05,
      "loss": 0.7579,
      "step": 26100
    },
    {
      "epoch": 1.0309156236427528,
      "grad_norm": 0.656471848487854,
      "learning_rate": 4.845619299561733e-05,
      "loss": 0.7174,
      "step": 26110
    },
    {
      "epoch": 1.0313104591937459,
      "grad_norm": 0.5862520933151245,
      "learning_rate": 4.843645121806767e-05,
      "loss": 0.7351,
      "step": 26120
    },
    {
      "epoch": 1.0317052947447387,
      "grad_norm": 0.6831204891204834,
      "learning_rate": 4.841670944051803e-05,
      "loss": 0.6663,
      "step": 26130
    },
    {
      "epoch": 1.0321001302957318,
      "grad_norm": 0.6202665567398071,
      "learning_rate": 4.839696766296837e-05,
      "loss": 0.7025,
      "step": 26140
    },
    {
      "epoch": 1.0324949658467248,
      "grad_norm": 0.61848384141922,
      "learning_rate": 4.837722588541873e-05,
      "loss": 0.7172,
      "step": 26150
    },
    {
      "epoch": 1.032889801397718,
      "grad_norm": 0.8431857228279114,
      "learning_rate": 4.835748410786907e-05,
      "loss": 0.7091,
      "step": 26160
    },
    {
      "epoch": 1.0332846369487108,
      "grad_norm": 0.6243896484375,
      "learning_rate": 4.833774233031942e-05,
      "loss": 0.708,
      "step": 26170
    },
    {
      "epoch": 1.0336794724997038,
      "grad_norm": 0.8035299777984619,
      "learning_rate": 4.831800055276977e-05,
      "loss": 0.719,
      "step": 26180
    },
    {
      "epoch": 1.0340743080506969,
      "grad_norm": 0.7984701991081238,
      "learning_rate": 4.829825877522012e-05,
      "loss": 0.7176,
      "step": 26190
    },
    {
      "epoch": 1.03446914360169,
      "grad_norm": 0.7231619358062744,
      "learning_rate": 4.827851699767047e-05,
      "loss": 0.6865,
      "step": 26200
    },
    {
      "epoch": 1.0348639791526828,
      "grad_norm": 0.7093676328659058,
      "learning_rate": 4.825877522012082e-05,
      "loss": 0.7542,
      "step": 26210
    },
    {
      "epoch": 1.0352588147036759,
      "grad_norm": 0.723970353603363,
      "learning_rate": 4.823903344257117e-05,
      "loss": 0.6869,
      "step": 26220
    },
    {
      "epoch": 1.035653650254669,
      "grad_norm": 0.7023337483406067,
      "learning_rate": 4.821929166502152e-05,
      "loss": 0.7389,
      "step": 26230
    },
    {
      "epoch": 1.036048485805662,
      "grad_norm": 1.0360320806503296,
      "learning_rate": 4.8199549887471864e-05,
      "loss": 0.7349,
      "step": 26240
    },
    {
      "epoch": 1.0364433213566548,
      "grad_norm": 0.9341004490852356,
      "learning_rate": 4.817980810992222e-05,
      "loss": 0.7545,
      "step": 26250
    },
    {
      "epoch": 1.036838156907648,
      "grad_norm": 0.7548127770423889,
      "learning_rate": 4.8160066332372564e-05,
      "loss": 0.7196,
      "step": 26260
    },
    {
      "epoch": 1.037232992458641,
      "grad_norm": 0.8056025505065918,
      "learning_rate": 4.814032455482292e-05,
      "loss": 0.7305,
      "step": 26270
    },
    {
      "epoch": 1.037627828009634,
      "grad_norm": 0.7747334837913513,
      "learning_rate": 4.8120582777273264e-05,
      "loss": 0.6794,
      "step": 26280
    },
    {
      "epoch": 1.0380226635606271,
      "grad_norm": 0.8431159853935242,
      "learning_rate": 4.810084099972362e-05,
      "loss": 0.6886,
      "step": 26290
    },
    {
      "epoch": 1.03841749911162,
      "grad_norm": 0.7141382694244385,
      "learning_rate": 4.8081099222173964e-05,
      "loss": 0.679,
      "step": 26300
    },
    {
      "epoch": 1.038812334662613,
      "grad_norm": 0.6300081610679626,
      "learning_rate": 4.8061357444624314e-05,
      "loss": 0.7037,
      "step": 26310
    },
    {
      "epoch": 1.039207170213606,
      "grad_norm": 0.8637331128120422,
      "learning_rate": 4.8041615667074664e-05,
      "loss": 0.7191,
      "step": 26320
    },
    {
      "epoch": 1.039602005764599,
      "grad_norm": 0.8785780072212219,
      "learning_rate": 4.8021873889525014e-05,
      "loss": 0.7018,
      "step": 26330
    },
    {
      "epoch": 1.039996841315592,
      "grad_norm": 0.8446933031082153,
      "learning_rate": 4.8002132111975364e-05,
      "loss": 0.7137,
      "step": 26340
    },
    {
      "epoch": 1.040391676866585,
      "grad_norm": 0.8612603545188904,
      "learning_rate": 4.7982390334425713e-05,
      "loss": 0.7454,
      "step": 26350
    },
    {
      "epoch": 1.0407865124175781,
      "grad_norm": 0.6673545241355896,
      "learning_rate": 4.796264855687606e-05,
      "loss": 0.7089,
      "step": 26360
    },
    {
      "epoch": 1.0411813479685712,
      "grad_norm": 0.7646205425262451,
      "learning_rate": 4.794290677932641e-05,
      "loss": 0.752,
      "step": 26370
    },
    {
      "epoch": 1.041576183519564,
      "grad_norm": 0.6242639422416687,
      "learning_rate": 4.7923165001776756e-05,
      "loss": 0.7023,
      "step": 26380
    },
    {
      "epoch": 1.0419710190705571,
      "grad_norm": 0.7159606218338013,
      "learning_rate": 4.790342322422711e-05,
      "loss": 0.7881,
      "step": 26390
    },
    {
      "epoch": 1.0423658546215502,
      "grad_norm": 0.6477059721946716,
      "learning_rate": 4.7883681446677456e-05,
      "loss": 0.7263,
      "step": 26400
    },
    {
      "epoch": 1.042760690172543,
      "grad_norm": 0.9805182814598083,
      "learning_rate": 4.786393966912781e-05,
      "loss": 0.7257,
      "step": 26410
    },
    {
      "epoch": 1.043155525723536,
      "grad_norm": 0.9251106977462769,
      "learning_rate": 4.7844197891578156e-05,
      "loss": 0.7292,
      "step": 26420
    },
    {
      "epoch": 1.0435503612745292,
      "grad_norm": 0.6682171821594238,
      "learning_rate": 4.782445611402851e-05,
      "loss": 0.7071,
      "step": 26430
    },
    {
      "epoch": 1.0439451968255222,
      "grad_norm": 0.7567628622055054,
      "learning_rate": 4.7804714336478856e-05,
      "loss": 0.7287,
      "step": 26440
    },
    {
      "epoch": 1.0443400323765153,
      "grad_norm": 0.955315113067627,
      "learning_rate": 4.7784972558929206e-05,
      "loss": 0.7198,
      "step": 26450
    },
    {
      "epoch": 1.0447348679275081,
      "grad_norm": 0.7812291979789734,
      "learning_rate": 4.7765230781379556e-05,
      "loss": 0.7288,
      "step": 26460
    },
    {
      "epoch": 1.0451297034785012,
      "grad_norm": 0.8407669067382812,
      "learning_rate": 4.7745489003829906e-05,
      "loss": 0.7446,
      "step": 26470
    },
    {
      "epoch": 1.0455245390294943,
      "grad_norm": 0.7551667094230652,
      "learning_rate": 4.7725747226280256e-05,
      "loss": 0.7057,
      "step": 26480
    },
    {
      "epoch": 1.0459193745804871,
      "grad_norm": 0.7588898539543152,
      "learning_rate": 4.7706005448730605e-05,
      "loss": 0.7313,
      "step": 26490
    },
    {
      "epoch": 1.0463142101314802,
      "grad_norm": 0.8113295435905457,
      "learning_rate": 4.7686263671180955e-05,
      "loss": 0.7146,
      "step": 26500
    },
    {
      "epoch": 1.0467090456824732,
      "grad_norm": 1.0044909715652466,
      "learning_rate": 4.7666521893631305e-05,
      "loss": 0.7138,
      "step": 26510
    },
    {
      "epoch": 1.0471038812334663,
      "grad_norm": 0.7176927924156189,
      "learning_rate": 4.7646780116081655e-05,
      "loss": 0.7451,
      "step": 26520
    },
    {
      "epoch": 1.0474987167844594,
      "grad_norm": 0.7421216368675232,
      "learning_rate": 4.7627038338532005e-05,
      "loss": 0.7242,
      "step": 26530
    },
    {
      "epoch": 1.0478935523354522,
      "grad_norm": 0.5740599632263184,
      "learning_rate": 4.760729656098235e-05,
      "loss": 0.7595,
      "step": 26540
    },
    {
      "epoch": 1.0482883878864453,
      "grad_norm": 0.7816346883773804,
      "learning_rate": 4.7587554783432705e-05,
      "loss": 0.7474,
      "step": 26550
    },
    {
      "epoch": 1.0486832234374384,
      "grad_norm": 1.0568312406539917,
      "learning_rate": 4.756781300588305e-05,
      "loss": 0.7195,
      "step": 26560
    },
    {
      "epoch": 1.0490780589884312,
      "grad_norm": 0.6340306401252747,
      "learning_rate": 4.7548071228333405e-05,
      "loss": 0.7444,
      "step": 26570
    },
    {
      "epoch": 1.0494728945394243,
      "grad_norm": 0.677359938621521,
      "learning_rate": 4.752832945078375e-05,
      "loss": 0.7291,
      "step": 26580
    },
    {
      "epoch": 1.0498677300904173,
      "grad_norm": 1.0975698232650757,
      "learning_rate": 4.75085876732341e-05,
      "loss": 0.7372,
      "step": 26590
    },
    {
      "epoch": 1.0502625656414104,
      "grad_norm": 0.6481153964996338,
      "learning_rate": 4.748884589568445e-05,
      "loss": 0.7423,
      "step": 26600
    },
    {
      "epoch": 1.0506574011924035,
      "grad_norm": 0.6108253598213196,
      "learning_rate": 4.74691041181348e-05,
      "loss": 0.7134,
      "step": 26610
    },
    {
      "epoch": 1.0510522367433963,
      "grad_norm": 0.6344373822212219,
      "learning_rate": 4.744936234058515e-05,
      "loss": 0.7155,
      "step": 26620
    },
    {
      "epoch": 1.0514470722943894,
      "grad_norm": 0.8325484991073608,
      "learning_rate": 4.74296205630355e-05,
      "loss": 0.7071,
      "step": 26630
    },
    {
      "epoch": 1.0518419078453825,
      "grad_norm": 0.7104957103729248,
      "learning_rate": 4.740987878548585e-05,
      "loss": 0.6524,
      "step": 26640
    },
    {
      "epoch": 1.0522367433963753,
      "grad_norm": 0.5828239321708679,
      "learning_rate": 4.73901370079362e-05,
      "loss": 0.7258,
      "step": 26650
    },
    {
      "epoch": 1.0526315789473684,
      "grad_norm": 0.7348014712333679,
      "learning_rate": 4.737039523038655e-05,
      "loss": 0.6937,
      "step": 26660
    },
    {
      "epoch": 1.0530264144983614,
      "grad_norm": 0.6447469592094421,
      "learning_rate": 4.73506534528369e-05,
      "loss": 0.693,
      "step": 26670
    },
    {
      "epoch": 1.0534212500493545,
      "grad_norm": 0.8762918710708618,
      "learning_rate": 4.733091167528724e-05,
      "loss": 0.7177,
      "step": 26680
    },
    {
      "epoch": 1.0538160856003476,
      "grad_norm": 0.6554642915725708,
      "learning_rate": 4.73111698977376e-05,
      "loss": 0.7081,
      "step": 26690
    },
    {
      "epoch": 1.0542109211513404,
      "grad_norm": 0.7129163146018982,
      "learning_rate": 4.729142812018794e-05,
      "loss": 0.7087,
      "step": 26700
    },
    {
      "epoch": 1.0546057567023335,
      "grad_norm": 0.6216626167297363,
      "learning_rate": 4.72716863426383e-05,
      "loss": 0.702,
      "step": 26710
    },
    {
      "epoch": 1.0550005922533265,
      "grad_norm": 0.6988326907157898,
      "learning_rate": 4.725194456508864e-05,
      "loss": 0.6983,
      "step": 26720
    },
    {
      "epoch": 1.0553954278043196,
      "grad_norm": 0.6398477554321289,
      "learning_rate": 4.723220278753899e-05,
      "loss": 0.6933,
      "step": 26730
    },
    {
      "epoch": 1.0557902633553125,
      "grad_norm": 0.6642658114433289,
      "learning_rate": 4.721246100998934e-05,
      "loss": 0.7123,
      "step": 26740
    },
    {
      "epoch": 1.0561850989063055,
      "grad_norm": 0.7212098240852356,
      "learning_rate": 4.719271923243969e-05,
      "loss": 0.7477,
      "step": 26750
    },
    {
      "epoch": 1.0565799344572986,
      "grad_norm": 0.8402842879295349,
      "learning_rate": 4.717297745489004e-05,
      "loss": 0.7322,
      "step": 26760
    },
    {
      "epoch": 1.0569747700082917,
      "grad_norm": 0.5677731037139893,
      "learning_rate": 4.715323567734039e-05,
      "loss": 0.6878,
      "step": 26770
    },
    {
      "epoch": 1.0573696055592845,
      "grad_norm": 0.7742734551429749,
      "learning_rate": 4.713349389979074e-05,
      "loss": 0.7606,
      "step": 26780
    },
    {
      "epoch": 1.0577644411102776,
      "grad_norm": 0.5487236380577087,
      "learning_rate": 4.711375212224109e-05,
      "loss": 0.7367,
      "step": 26790
    },
    {
      "epoch": 1.0581592766612706,
      "grad_norm": 0.7351383566856384,
      "learning_rate": 4.709401034469144e-05,
      "loss": 0.7569,
      "step": 26800
    },
    {
      "epoch": 1.0585541122122637,
      "grad_norm": 0.707426130771637,
      "learning_rate": 4.707426856714179e-05,
      "loss": 0.7367,
      "step": 26810
    },
    {
      "epoch": 1.0589489477632565,
      "grad_norm": 0.6243427395820618,
      "learning_rate": 4.705452678959213e-05,
      "loss": 0.7139,
      "step": 26820
    },
    {
      "epoch": 1.0593437833142496,
      "grad_norm": 0.8016042709350586,
      "learning_rate": 4.703478501204249e-05,
      "loss": 0.7502,
      "step": 26830
    },
    {
      "epoch": 1.0597386188652427,
      "grad_norm": 0.7110398411750793,
      "learning_rate": 4.701504323449283e-05,
      "loss": 0.7645,
      "step": 26840
    },
    {
      "epoch": 1.0601334544162357,
      "grad_norm": 0.7067217230796814,
      "learning_rate": 4.699530145694319e-05,
      "loss": 0.6962,
      "step": 26850
    },
    {
      "epoch": 1.0605282899672286,
      "grad_norm": 0.5897038578987122,
      "learning_rate": 4.697555967939353e-05,
      "loss": 0.6738,
      "step": 26860
    },
    {
      "epoch": 1.0609231255182217,
      "grad_norm": 0.7252286672592163,
      "learning_rate": 4.695581790184388e-05,
      "loss": 0.7205,
      "step": 26870
    },
    {
      "epoch": 1.0613179610692147,
      "grad_norm": 0.8519831299781799,
      "learning_rate": 4.693607612429423e-05,
      "loss": 0.7478,
      "step": 26880
    },
    {
      "epoch": 1.0617127966202078,
      "grad_norm": 0.7929390668869019,
      "learning_rate": 4.691633434674458e-05,
      "loss": 0.7237,
      "step": 26890
    },
    {
      "epoch": 1.0621076321712006,
      "grad_norm": 0.6554683446884155,
      "learning_rate": 4.689659256919493e-05,
      "loss": 0.7326,
      "step": 26900
    },
    {
      "epoch": 1.0625024677221937,
      "grad_norm": 0.7381708025932312,
      "learning_rate": 4.687685079164528e-05,
      "loss": 0.7421,
      "step": 26910
    },
    {
      "epoch": 1.0628973032731868,
      "grad_norm": 0.8167394399642944,
      "learning_rate": 4.685710901409563e-05,
      "loss": 0.7067,
      "step": 26920
    },
    {
      "epoch": 1.0632921388241798,
      "grad_norm": 0.8726596236228943,
      "learning_rate": 4.683736723654598e-05,
      "loss": 0.7333,
      "step": 26930
    },
    {
      "epoch": 1.0636869743751727,
      "grad_norm": 0.836656928062439,
      "learning_rate": 4.681762545899633e-05,
      "loss": 0.7532,
      "step": 26940
    },
    {
      "epoch": 1.0640818099261657,
      "grad_norm": 0.9253910779953003,
      "learning_rate": 4.679788368144668e-05,
      "loss": 0.6562,
      "step": 26950
    },
    {
      "epoch": 1.0644766454771588,
      "grad_norm": 0.6047634482383728,
      "learning_rate": 4.6778141903897024e-05,
      "loss": 0.7183,
      "step": 26960
    },
    {
      "epoch": 1.0648714810281519,
      "grad_norm": 0.6556540727615356,
      "learning_rate": 4.675840012634738e-05,
      "loss": 0.7516,
      "step": 26970
    },
    {
      "epoch": 1.0652663165791447,
      "grad_norm": 0.6090645790100098,
      "learning_rate": 4.6738658348797724e-05,
      "loss": 0.6912,
      "step": 26980
    },
    {
      "epoch": 1.0656611521301378,
      "grad_norm": 0.5394542813301086,
      "learning_rate": 4.671891657124808e-05,
      "loss": 0.6959,
      "step": 26990
    },
    {
      "epoch": 1.0660559876811309,
      "grad_norm": 0.7641823887825012,
      "learning_rate": 4.6699174793698424e-05,
      "loss": 0.7345,
      "step": 27000
    },
    {
      "epoch": 1.0660559876811309,
      "eval_loss": 0.773897647857666,
      "eval_runtime": 1218.3891,
      "eval_samples_per_second": 9.239,
      "eval_steps_per_second": 9.239,
      "step": 27000
    },
    {
      "epoch": 1.066450823232124,
      "grad_norm": 0.6849491000175476,
      "learning_rate": 4.6679433016148774e-05,
      "loss": 0.7085,
      "step": 27010
    },
    {
      "epoch": 1.0668456587831168,
      "grad_norm": 0.7095043063163757,
      "learning_rate": 4.6659691238599124e-05,
      "loss": 0.7421,
      "step": 27020
    },
    {
      "epoch": 1.0672404943341098,
      "grad_norm": 0.6476829648017883,
      "learning_rate": 4.6639949461049474e-05,
      "loss": 0.6823,
      "step": 27030
    },
    {
      "epoch": 1.067635329885103,
      "grad_norm": 0.582344651222229,
      "learning_rate": 4.6620207683499824e-05,
      "loss": 0.6595,
      "step": 27040
    },
    {
      "epoch": 1.068030165436096,
      "grad_norm": 0.7360102534294128,
      "learning_rate": 4.6600465905950174e-05,
      "loss": 0.6968,
      "step": 27050
    },
    {
      "epoch": 1.0684250009870888,
      "grad_norm": 0.6386150121688843,
      "learning_rate": 4.6580724128400523e-05,
      "loss": 0.7145,
      "step": 27060
    },
    {
      "epoch": 1.0688198365380819,
      "grad_norm": 0.6675960421562195,
      "learning_rate": 4.656098235085087e-05,
      "loss": 0.7401,
      "step": 27070
    },
    {
      "epoch": 1.069214672089075,
      "grad_norm": 0.6213105320930481,
      "learning_rate": 4.654124057330122e-05,
      "loss": 0.7178,
      "step": 27080
    },
    {
      "epoch": 1.069609507640068,
      "grad_norm": 0.8093931674957275,
      "learning_rate": 4.652149879575157e-05,
      "loss": 0.6968,
      "step": 27090
    },
    {
      "epoch": 1.0700043431910609,
      "grad_norm": 1.0721701383590698,
      "learning_rate": 4.6501757018201916e-05,
      "loss": 0.7115,
      "step": 27100
    },
    {
      "epoch": 1.070399178742054,
      "grad_norm": 0.629124641418457,
      "learning_rate": 4.648201524065227e-05,
      "loss": 0.7478,
      "step": 27110
    },
    {
      "epoch": 1.070794014293047,
      "grad_norm": 0.7420177459716797,
      "learning_rate": 4.6462273463102616e-05,
      "loss": 0.7007,
      "step": 27120
    },
    {
      "epoch": 1.07118884984404,
      "grad_norm": 0.8781113624572754,
      "learning_rate": 4.644253168555297e-05,
      "loss": 0.681,
      "step": 27130
    },
    {
      "epoch": 1.071583685395033,
      "grad_norm": 0.6023575663566589,
      "learning_rate": 4.6422789908003316e-05,
      "loss": 0.7171,
      "step": 27140
    },
    {
      "epoch": 1.071978520946026,
      "grad_norm": 0.6962303519248962,
      "learning_rate": 4.6403048130453666e-05,
      "loss": 0.7788,
      "step": 27150
    },
    {
      "epoch": 1.072373356497019,
      "grad_norm": 1.014805555343628,
      "learning_rate": 4.6383306352904016e-05,
      "loss": 0.7516,
      "step": 27160
    },
    {
      "epoch": 1.072768192048012,
      "grad_norm": 0.7398793697357178,
      "learning_rate": 4.6363564575354366e-05,
      "loss": 0.6898,
      "step": 27170
    },
    {
      "epoch": 1.073163027599005,
      "grad_norm": 0.6672147512435913,
      "learning_rate": 4.6343822797804716e-05,
      "loss": 0.706,
      "step": 27180
    },
    {
      "epoch": 1.073557863149998,
      "grad_norm": 0.8589248061180115,
      "learning_rate": 4.6324081020255066e-05,
      "loss": 0.7118,
      "step": 27190
    },
    {
      "epoch": 1.073952698700991,
      "grad_norm": 0.6269434690475464,
      "learning_rate": 4.6304339242705415e-05,
      "loss": 0.7121,
      "step": 27200
    },
    {
      "epoch": 1.0743475342519841,
      "grad_norm": 0.678225576877594,
      "learning_rate": 4.6284597465155765e-05,
      "loss": 0.6949,
      "step": 27210
    },
    {
      "epoch": 1.074742369802977,
      "grad_norm": 0.7283880710601807,
      "learning_rate": 4.6264855687606115e-05,
      "loss": 0.6918,
      "step": 27220
    },
    {
      "epoch": 1.07513720535397,
      "grad_norm": 0.6586973071098328,
      "learning_rate": 4.6245113910056465e-05,
      "loss": 0.7468,
      "step": 27230
    },
    {
      "epoch": 1.0755320409049631,
      "grad_norm": 0.6954028010368347,
      "learning_rate": 4.622537213250681e-05,
      "loss": 0.7434,
      "step": 27240
    },
    {
      "epoch": 1.0759268764559562,
      "grad_norm": 0.5514123439788818,
      "learning_rate": 4.6205630354957165e-05,
      "loss": 0.6882,
      "step": 27250
    },
    {
      "epoch": 1.076321712006949,
      "grad_norm": 0.5763728618621826,
      "learning_rate": 4.618588857740751e-05,
      "loss": 0.6883,
      "step": 27260
    },
    {
      "epoch": 1.076716547557942,
      "grad_norm": 0.6429103016853333,
      "learning_rate": 4.6166146799857865e-05,
      "loss": 0.738,
      "step": 27270
    },
    {
      "epoch": 1.0771113831089352,
      "grad_norm": 0.6429718732833862,
      "learning_rate": 4.614640502230821e-05,
      "loss": 0.7037,
      "step": 27280
    },
    {
      "epoch": 1.0775062186599282,
      "grad_norm": 0.6682242155075073,
      "learning_rate": 4.612666324475856e-05,
      "loss": 0.7391,
      "step": 27290
    },
    {
      "epoch": 1.077901054210921,
      "grad_norm": 0.6602041721343994,
      "learning_rate": 4.610692146720891e-05,
      "loss": 0.6923,
      "step": 27300
    },
    {
      "epoch": 1.0782958897619142,
      "grad_norm": 0.7893023490905762,
      "learning_rate": 4.608717968965926e-05,
      "loss": 0.7385,
      "step": 27310
    },
    {
      "epoch": 1.0786907253129072,
      "grad_norm": 0.5884254574775696,
      "learning_rate": 4.606743791210961e-05,
      "loss": 0.711,
      "step": 27320
    },
    {
      "epoch": 1.0790855608639003,
      "grad_norm": 0.8324078321456909,
      "learning_rate": 4.604769613455996e-05,
      "loss": 0.7267,
      "step": 27330
    },
    {
      "epoch": 1.0794803964148931,
      "grad_norm": 0.877825915813446,
      "learning_rate": 4.602795435701031e-05,
      "loss": 0.7539,
      "step": 27340
    },
    {
      "epoch": 1.0798752319658862,
      "grad_norm": 0.6954593062400818,
      "learning_rate": 4.600821257946066e-05,
      "loss": 0.6771,
      "step": 27350
    },
    {
      "epoch": 1.0802700675168793,
      "grad_norm": 0.638949453830719,
      "learning_rate": 4.598847080191101e-05,
      "loss": 0.7011,
      "step": 27360
    },
    {
      "epoch": 1.0806649030678723,
      "grad_norm": 0.7884819507598877,
      "learning_rate": 4.596872902436136e-05,
      "loss": 0.7148,
      "step": 27370
    },
    {
      "epoch": 1.0810597386188652,
      "grad_norm": 0.5928741097450256,
      "learning_rate": 4.59489872468117e-05,
      "loss": 0.7252,
      "step": 27380
    },
    {
      "epoch": 1.0814545741698582,
      "grad_norm": 0.7670236229896545,
      "learning_rate": 4.592924546926206e-05,
      "loss": 0.7331,
      "step": 27390
    },
    {
      "epoch": 1.0818494097208513,
      "grad_norm": 0.7681565880775452,
      "learning_rate": 4.59095036917124e-05,
      "loss": 0.7381,
      "step": 27400
    },
    {
      "epoch": 1.0822442452718444,
      "grad_norm": 0.9181767106056213,
      "learning_rate": 4.588976191416276e-05,
      "loss": 0.7183,
      "step": 27410
    },
    {
      "epoch": 1.0826390808228372,
      "grad_norm": 0.7221302390098572,
      "learning_rate": 4.58700201366131e-05,
      "loss": 0.6905,
      "step": 27420
    },
    {
      "epoch": 1.0830339163738303,
      "grad_norm": 0.5535574555397034,
      "learning_rate": 4.585027835906345e-05,
      "loss": 0.7212,
      "step": 27430
    },
    {
      "epoch": 1.0834287519248234,
      "grad_norm": 0.6501957774162292,
      "learning_rate": 4.58305365815138e-05,
      "loss": 0.7202,
      "step": 27440
    },
    {
      "epoch": 1.0838235874758164,
      "grad_norm": 1.0246351957321167,
      "learning_rate": 4.581079480396415e-05,
      "loss": 0.7325,
      "step": 27450
    },
    {
      "epoch": 1.0842184230268093,
      "grad_norm": 0.6594454050064087,
      "learning_rate": 4.57910530264145e-05,
      "loss": 0.7118,
      "step": 27460
    },
    {
      "epoch": 1.0846132585778023,
      "grad_norm": 0.8296276926994324,
      "learning_rate": 4.577131124886485e-05,
      "loss": 0.7337,
      "step": 27470
    },
    {
      "epoch": 1.0850080941287954,
      "grad_norm": 0.8347951173782349,
      "learning_rate": 4.57515694713152e-05,
      "loss": 0.7352,
      "step": 27480
    },
    {
      "epoch": 1.0854029296797885,
      "grad_norm": 0.6697707772254944,
      "learning_rate": 4.573182769376555e-05,
      "loss": 0.7235,
      "step": 27490
    },
    {
      "epoch": 1.0857977652307813,
      "grad_norm": 1.0413341522216797,
      "learning_rate": 4.57120859162159e-05,
      "loss": 0.7235,
      "step": 27500
    },
    {
      "epoch": 1.0861926007817744,
      "grad_norm": 0.6256203651428223,
      "learning_rate": 4.569234413866625e-05,
      "loss": 0.752,
      "step": 27510
    },
    {
      "epoch": 1.0865874363327674,
      "grad_norm": 0.8497620820999146,
      "learning_rate": 4.567260236111659e-05,
      "loss": 0.669,
      "step": 27520
    },
    {
      "epoch": 1.0869822718837605,
      "grad_norm": 0.6989908218383789,
      "learning_rate": 4.565286058356695e-05,
      "loss": 0.7148,
      "step": 27530
    },
    {
      "epoch": 1.0873771074347534,
      "grad_norm": 0.9094425439834595,
      "learning_rate": 4.563311880601729e-05,
      "loss": 0.7015,
      "step": 27540
    },
    {
      "epoch": 1.0877719429857464,
      "grad_norm": 0.6333603262901306,
      "learning_rate": 4.561337702846765e-05,
      "loss": 0.734,
      "step": 27550
    },
    {
      "epoch": 1.0881667785367395,
      "grad_norm": 0.8313611745834351,
      "learning_rate": 4.559363525091799e-05,
      "loss": 0.7254,
      "step": 27560
    },
    {
      "epoch": 1.0885616140877326,
      "grad_norm": 0.6983000040054321,
      "learning_rate": 4.557389347336835e-05,
      "loss": 0.7011,
      "step": 27570
    },
    {
      "epoch": 1.0889564496387254,
      "grad_norm": 0.7784873843193054,
      "learning_rate": 4.555415169581869e-05,
      "loss": 0.6714,
      "step": 27580
    },
    {
      "epoch": 1.0893512851897185,
      "grad_norm": 0.8511632680892944,
      "learning_rate": 4.553440991826904e-05,
      "loss": 0.7097,
      "step": 27590
    },
    {
      "epoch": 1.0897461207407115,
      "grad_norm": 0.535169243812561,
      "learning_rate": 4.551466814071939e-05,
      "loss": 0.7537,
      "step": 27600
    },
    {
      "epoch": 1.0901409562917046,
      "grad_norm": 0.6120785474777222,
      "learning_rate": 4.549492636316974e-05,
      "loss": 0.6927,
      "step": 27610
    },
    {
      "epoch": 1.0905357918426974,
      "grad_norm": 0.732705295085907,
      "learning_rate": 4.547518458562009e-05,
      "loss": 0.7007,
      "step": 27620
    },
    {
      "epoch": 1.0909306273936905,
      "grad_norm": 0.843048095703125,
      "learning_rate": 4.545544280807044e-05,
      "loss": 0.6686,
      "step": 27630
    },
    {
      "epoch": 1.0913254629446836,
      "grad_norm": 0.7116616368293762,
      "learning_rate": 4.543570103052079e-05,
      "loss": 0.7017,
      "step": 27640
    },
    {
      "epoch": 1.0917202984956766,
      "grad_norm": 0.7873333096504211,
      "learning_rate": 4.541595925297114e-05,
      "loss": 0.6765,
      "step": 27650
    },
    {
      "epoch": 1.0921151340466695,
      "grad_norm": 0.7463117241859436,
      "learning_rate": 4.5396217475421484e-05,
      "loss": 0.7584,
      "step": 27660
    },
    {
      "epoch": 1.0925099695976626,
      "grad_norm": 1.0588178634643555,
      "learning_rate": 4.537647569787184e-05,
      "loss": 0.7753,
      "step": 27670
    },
    {
      "epoch": 1.0929048051486556,
      "grad_norm": 0.626325249671936,
      "learning_rate": 4.5356733920322184e-05,
      "loss": 0.7109,
      "step": 27680
    },
    {
      "epoch": 1.0932996406996487,
      "grad_norm": 0.8660476207733154,
      "learning_rate": 4.533699214277254e-05,
      "loss": 0.7026,
      "step": 27690
    },
    {
      "epoch": 1.0936944762506415,
      "grad_norm": 0.9804587960243225,
      "learning_rate": 4.5317250365222884e-05,
      "loss": 0.7152,
      "step": 27700
    },
    {
      "epoch": 1.0940893118016346,
      "grad_norm": 0.6646446585655212,
      "learning_rate": 4.529750858767324e-05,
      "loss": 0.7275,
      "step": 27710
    },
    {
      "epoch": 1.0944841473526277,
      "grad_norm": 0.5987580418586731,
      "learning_rate": 4.5277766810123584e-05,
      "loss": 0.7383,
      "step": 27720
    },
    {
      "epoch": 1.0948789829036207,
      "grad_norm": 0.6551607251167297,
      "learning_rate": 4.5258025032573934e-05,
      "loss": 0.7093,
      "step": 27730
    },
    {
      "epoch": 1.0952738184546136,
      "grad_norm": 0.743806779384613,
      "learning_rate": 4.5238283255024284e-05,
      "loss": 0.7125,
      "step": 27740
    },
    {
      "epoch": 1.0956686540056066,
      "grad_norm": 0.6173561215400696,
      "learning_rate": 4.5218541477474634e-05,
      "loss": 0.6999,
      "step": 27750
    },
    {
      "epoch": 1.0960634895565997,
      "grad_norm": 0.7211037874221802,
      "learning_rate": 4.5198799699924984e-05,
      "loss": 0.746,
      "step": 27760
    },
    {
      "epoch": 1.0964583251075928,
      "grad_norm": 0.6594390869140625,
      "learning_rate": 4.5179057922375333e-05,
      "loss": 0.7328,
      "step": 27770
    },
    {
      "epoch": 1.0968531606585856,
      "grad_norm": 0.7905805110931396,
      "learning_rate": 4.515931614482568e-05,
      "loss": 0.6621,
      "step": 27780
    },
    {
      "epoch": 1.0972479962095787,
      "grad_norm": 0.7365785241127014,
      "learning_rate": 4.513957436727603e-05,
      "loss": 0.7188,
      "step": 27790
    },
    {
      "epoch": 1.0976428317605718,
      "grad_norm": 0.6297890543937683,
      "learning_rate": 4.5119832589726376e-05,
      "loss": 0.6945,
      "step": 27800
    },
    {
      "epoch": 1.0980376673115648,
      "grad_norm": 0.6589920520782471,
      "learning_rate": 4.510009081217673e-05,
      "loss": 0.7231,
      "step": 27810
    },
    {
      "epoch": 1.0984325028625577,
      "grad_norm": 0.6786764860153198,
      "learning_rate": 4.5080349034627076e-05,
      "loss": 0.7072,
      "step": 27820
    },
    {
      "epoch": 1.0988273384135507,
      "grad_norm": 0.7201002240180969,
      "learning_rate": 4.506060725707743e-05,
      "loss": 0.7073,
      "step": 27830
    },
    {
      "epoch": 1.0992221739645438,
      "grad_norm": 0.616493284702301,
      "learning_rate": 4.5040865479527776e-05,
      "loss": 0.6901,
      "step": 27840
    },
    {
      "epoch": 1.0996170095155369,
      "grad_norm": 0.6039842963218689,
      "learning_rate": 4.502112370197813e-05,
      "loss": 0.7245,
      "step": 27850
    },
    {
      "epoch": 1.1000118450665297,
      "grad_norm": 0.7002038955688477,
      "learning_rate": 4.5001381924428476e-05,
      "loss": 0.7,
      "step": 27860
    },
    {
      "epoch": 1.1004066806175228,
      "grad_norm": 0.8134871125221252,
      "learning_rate": 4.4981640146878826e-05,
      "loss": 0.7093,
      "step": 27870
    },
    {
      "epoch": 1.1008015161685158,
      "grad_norm": 0.613301157951355,
      "learning_rate": 4.4961898369329176e-05,
      "loss": 0.7345,
      "step": 27880
    },
    {
      "epoch": 1.101196351719509,
      "grad_norm": 0.7164487242698669,
      "learning_rate": 4.4942156591779526e-05,
      "loss": 0.6872,
      "step": 27890
    },
    {
      "epoch": 1.1015911872705018,
      "grad_norm": 0.6560975909233093,
      "learning_rate": 4.4922414814229876e-05,
      "loss": 0.6896,
      "step": 27900
    },
    {
      "epoch": 1.1019860228214948,
      "grad_norm": 0.8194178342819214,
      "learning_rate": 4.4902673036680225e-05,
      "loss": 0.6811,
      "step": 27910
    },
    {
      "epoch": 1.102380858372488,
      "grad_norm": 0.8297710418701172,
      "learning_rate": 4.4882931259130575e-05,
      "loss": 0.7216,
      "step": 27920
    },
    {
      "epoch": 1.102775693923481,
      "grad_norm": 0.8881620764732361,
      "learning_rate": 4.4863189481580925e-05,
      "loss": 0.7492,
      "step": 27930
    },
    {
      "epoch": 1.1031705294744738,
      "grad_norm": 0.6756368279457092,
      "learning_rate": 4.484344770403127e-05,
      "loss": 0.76,
      "step": 27940
    },
    {
      "epoch": 1.1035653650254669,
      "grad_norm": 0.6530833840370178,
      "learning_rate": 4.4823705926481625e-05,
      "loss": 0.7191,
      "step": 27950
    },
    {
      "epoch": 1.10396020057646,
      "grad_norm": 0.7623187303543091,
      "learning_rate": 4.480396414893197e-05,
      "loss": 0.7131,
      "step": 27960
    },
    {
      "epoch": 1.104355036127453,
      "grad_norm": 0.8259134888648987,
      "learning_rate": 4.4784222371382325e-05,
      "loss": 0.7068,
      "step": 27970
    },
    {
      "epoch": 1.1047498716784458,
      "grad_norm": 0.7948561906814575,
      "learning_rate": 4.476448059383267e-05,
      "loss": 0.7382,
      "step": 27980
    },
    {
      "epoch": 1.105144707229439,
      "grad_norm": 0.6167152523994446,
      "learning_rate": 4.4744738816283025e-05,
      "loss": 0.6773,
      "step": 27990
    },
    {
      "epoch": 1.105539542780432,
      "grad_norm": 0.7840978503227234,
      "learning_rate": 4.472499703873337e-05,
      "loss": 0.7594,
      "step": 28000
    },
    {
      "epoch": 1.105539542780432,
      "eval_loss": 0.772599458694458,
      "eval_runtime": 1215.9463,
      "eval_samples_per_second": 9.258,
      "eval_steps_per_second": 9.258,
      "step": 28000
    },
    {
      "epoch": 1.105934378331425,
      "grad_norm": 0.6994721293449402,
      "learning_rate": 4.470525526118372e-05,
      "loss": 0.7309,
      "step": 28010
    },
    {
      "epoch": 1.106329213882418,
      "grad_norm": 0.7175682187080383,
      "learning_rate": 4.468551348363407e-05,
      "loss": 0.7504,
      "step": 28020
    },
    {
      "epoch": 1.106724049433411,
      "grad_norm": 0.6550137996673584,
      "learning_rate": 4.466577170608442e-05,
      "loss": 0.7573,
      "step": 28030
    },
    {
      "epoch": 1.107118884984404,
      "grad_norm": 0.9070919752120972,
      "learning_rate": 4.464602992853477e-05,
      "loss": 0.6782,
      "step": 28040
    },
    {
      "epoch": 1.107513720535397,
      "grad_norm": 0.5876547694206238,
      "learning_rate": 4.462628815098512e-05,
      "loss": 0.6646,
      "step": 28050
    },
    {
      "epoch": 1.10790855608639,
      "grad_norm": 0.6870309114456177,
      "learning_rate": 4.460654637343547e-05,
      "loss": 0.7022,
      "step": 28060
    },
    {
      "epoch": 1.108303391637383,
      "grad_norm": 0.9260609149932861,
      "learning_rate": 4.458680459588582e-05,
      "loss": 0.7395,
      "step": 28070
    },
    {
      "epoch": 1.108698227188376,
      "grad_norm": 0.6492661833763123,
      "learning_rate": 4.456706281833616e-05,
      "loss": 0.6872,
      "step": 28080
    },
    {
      "epoch": 1.1090930627393691,
      "grad_norm": 0.7285547852516174,
      "learning_rate": 4.454732104078652e-05,
      "loss": 0.7069,
      "step": 28090
    },
    {
      "epoch": 1.109487898290362,
      "grad_norm": 0.6302974820137024,
      "learning_rate": 4.452757926323686e-05,
      "loss": 0.6898,
      "step": 28100
    },
    {
      "epoch": 1.109882733841355,
      "grad_norm": 0.6829454898834229,
      "learning_rate": 4.450783748568722e-05,
      "loss": 0.7121,
      "step": 28110
    },
    {
      "epoch": 1.1102775693923481,
      "grad_norm": 0.6897620558738708,
      "learning_rate": 4.448809570813756e-05,
      "loss": 0.7289,
      "step": 28120
    },
    {
      "epoch": 1.1106724049433412,
      "grad_norm": 0.7304121255874634,
      "learning_rate": 4.446835393058792e-05,
      "loss": 0.7188,
      "step": 28130
    },
    {
      "epoch": 1.111067240494334,
      "grad_norm": 0.6761592626571655,
      "learning_rate": 4.444861215303826e-05,
      "loss": 0.7106,
      "step": 28140
    },
    {
      "epoch": 1.111462076045327,
      "grad_norm": 0.6937708854675293,
      "learning_rate": 4.442887037548861e-05,
      "loss": 0.6987,
      "step": 28150
    },
    {
      "epoch": 1.1118569115963202,
      "grad_norm": 0.6845769286155701,
      "learning_rate": 4.440912859793896e-05,
      "loss": 0.7292,
      "step": 28160
    },
    {
      "epoch": 1.1122517471473132,
      "grad_norm": 0.719979465007782,
      "learning_rate": 4.438938682038931e-05,
      "loss": 0.7244,
      "step": 28170
    },
    {
      "epoch": 1.112646582698306,
      "grad_norm": 0.6499848365783691,
      "learning_rate": 4.436964504283966e-05,
      "loss": 0.7037,
      "step": 28180
    },
    {
      "epoch": 1.1130414182492991,
      "grad_norm": 0.6957079768180847,
      "learning_rate": 4.434990326529001e-05,
      "loss": 0.709,
      "step": 28190
    },
    {
      "epoch": 1.1134362538002922,
      "grad_norm": 0.887430727481842,
      "learning_rate": 4.433016148774036e-05,
      "loss": 0.7214,
      "step": 28200
    },
    {
      "epoch": 1.1138310893512853,
      "grad_norm": 0.7784826755523682,
      "learning_rate": 4.431041971019071e-05,
      "loss": 0.733,
      "step": 28210
    },
    {
      "epoch": 1.1142259249022781,
      "grad_norm": 0.7959589958190918,
      "learning_rate": 4.429067793264106e-05,
      "loss": 0.7189,
      "step": 28220
    },
    {
      "epoch": 1.1146207604532712,
      "grad_norm": 0.8111825585365295,
      "learning_rate": 4.427093615509141e-05,
      "loss": 0.6876,
      "step": 28230
    },
    {
      "epoch": 1.1150155960042643,
      "grad_norm": 0.6799155473709106,
      "learning_rate": 4.425119437754175e-05,
      "loss": 0.7566,
      "step": 28240
    },
    {
      "epoch": 1.1154104315552573,
      "grad_norm": 0.7770408987998962,
      "learning_rate": 4.423145259999211e-05,
      "loss": 0.7073,
      "step": 28250
    },
    {
      "epoch": 1.1158052671062502,
      "grad_norm": 0.7280828356742859,
      "learning_rate": 4.421171082244245e-05,
      "loss": 0.7366,
      "step": 28260
    },
    {
      "epoch": 1.1162001026572432,
      "grad_norm": 0.6545889377593994,
      "learning_rate": 4.419196904489281e-05,
      "loss": 0.7311,
      "step": 28270
    },
    {
      "epoch": 1.1165949382082363,
      "grad_norm": 0.6795562505722046,
      "learning_rate": 4.417222726734315e-05,
      "loss": 0.7513,
      "step": 28280
    },
    {
      "epoch": 1.1169897737592294,
      "grad_norm": 0.7632910013198853,
      "learning_rate": 4.41524854897935e-05,
      "loss": 0.6713,
      "step": 28290
    },
    {
      "epoch": 1.1173846093102222,
      "grad_norm": 0.8168066143989563,
      "learning_rate": 4.413274371224385e-05,
      "loss": 0.7508,
      "step": 28300
    },
    {
      "epoch": 1.1177794448612153,
      "grad_norm": 0.8395574688911438,
      "learning_rate": 4.41130019346942e-05,
      "loss": 0.6997,
      "step": 28310
    },
    {
      "epoch": 1.1181742804122083,
      "grad_norm": 1.0066486597061157,
      "learning_rate": 4.409326015714455e-05,
      "loss": 0.7106,
      "step": 28320
    },
    {
      "epoch": 1.1185691159632014,
      "grad_norm": 0.9193525910377502,
      "learning_rate": 4.40735183795949e-05,
      "loss": 0.7034,
      "step": 28330
    },
    {
      "epoch": 1.1189639515141943,
      "grad_norm": 0.7359071373939514,
      "learning_rate": 4.405377660204525e-05,
      "loss": 0.7065,
      "step": 28340
    },
    {
      "epoch": 1.1193587870651873,
      "grad_norm": 0.7311928272247314,
      "learning_rate": 4.40340348244956e-05,
      "loss": 0.6927,
      "step": 28350
    },
    {
      "epoch": 1.1197536226161804,
      "grad_norm": 0.6389646530151367,
      "learning_rate": 4.401429304694595e-05,
      "loss": 0.7319,
      "step": 28360
    },
    {
      "epoch": 1.1201484581671735,
      "grad_norm": 1.0811715126037598,
      "learning_rate": 4.39945512693963e-05,
      "loss": 0.7269,
      "step": 28370
    },
    {
      "epoch": 1.1205432937181663,
      "grad_norm": 0.7578041553497314,
      "learning_rate": 4.3974809491846644e-05,
      "loss": 0.6954,
      "step": 28380
    },
    {
      "epoch": 1.1209381292691594,
      "grad_norm": 0.9387365579605103,
      "learning_rate": 4.3955067714297e-05,
      "loss": 0.7627,
      "step": 28390
    },
    {
      "epoch": 1.1213329648201524,
      "grad_norm": 0.6077672839164734,
      "learning_rate": 4.3935325936747344e-05,
      "loss": 0.7563,
      "step": 28400
    },
    {
      "epoch": 1.1217278003711455,
      "grad_norm": 0.6801128387451172,
      "learning_rate": 4.39155841591977e-05,
      "loss": 0.7341,
      "step": 28410
    },
    {
      "epoch": 1.1221226359221383,
      "grad_norm": 0.8174123167991638,
      "learning_rate": 4.3895842381648044e-05,
      "loss": 0.7146,
      "step": 28420
    },
    {
      "epoch": 1.1225174714731314,
      "grad_norm": 0.8808111548423767,
      "learning_rate": 4.3876100604098394e-05,
      "loss": 0.7091,
      "step": 28430
    },
    {
      "epoch": 1.1229123070241245,
      "grad_norm": 0.7713537216186523,
      "learning_rate": 4.3856358826548744e-05,
      "loss": 0.7348,
      "step": 28440
    },
    {
      "epoch": 1.1233071425751175,
      "grad_norm": 0.8000394105911255,
      "learning_rate": 4.3836617048999094e-05,
      "loss": 0.7196,
      "step": 28450
    },
    {
      "epoch": 1.1237019781261104,
      "grad_norm": 0.688529372215271,
      "learning_rate": 4.3816875271449444e-05,
      "loss": 0.6859,
      "step": 28460
    },
    {
      "epoch": 1.1240968136771035,
      "grad_norm": 0.716003954410553,
      "learning_rate": 4.3797133493899794e-05,
      "loss": 0.7042,
      "step": 28470
    },
    {
      "epoch": 1.1244916492280965,
      "grad_norm": 0.7253708243370056,
      "learning_rate": 4.3777391716350143e-05,
      "loss": 0.7129,
      "step": 28480
    },
    {
      "epoch": 1.1248864847790896,
      "grad_norm": 0.7478386163711548,
      "learning_rate": 4.375764993880049e-05,
      "loss": 0.7384,
      "step": 28490
    },
    {
      "epoch": 1.1252813203300824,
      "grad_norm": 0.6883314847946167,
      "learning_rate": 4.373790816125084e-05,
      "loss": 0.7284,
      "step": 28500
    },
    {
      "epoch": 1.1256761558810755,
      "grad_norm": 0.6723697781562805,
      "learning_rate": 4.371816638370119e-05,
      "loss": 0.76,
      "step": 28510
    },
    {
      "epoch": 1.1260709914320686,
      "grad_norm": 0.6720019578933716,
      "learning_rate": 4.3698424606151536e-05,
      "loss": 0.7377,
      "step": 28520
    },
    {
      "epoch": 1.1264658269830616,
      "grad_norm": 0.684577465057373,
      "learning_rate": 4.367868282860189e-05,
      "loss": 0.7295,
      "step": 28530
    },
    {
      "epoch": 1.1268606625340545,
      "grad_norm": 0.8353940844535828,
      "learning_rate": 4.3658941051052236e-05,
      "loss": 0.7655,
      "step": 28540
    },
    {
      "epoch": 1.1272554980850475,
      "grad_norm": 0.8958239555358887,
      "learning_rate": 4.363919927350259e-05,
      "loss": 0.6859,
      "step": 28550
    },
    {
      "epoch": 1.1276503336360406,
      "grad_norm": 0.6791879534721375,
      "learning_rate": 4.3619457495952936e-05,
      "loss": 0.669,
      "step": 28560
    },
    {
      "epoch": 1.1280451691870337,
      "grad_norm": 0.7632009983062744,
      "learning_rate": 4.3599715718403286e-05,
      "loss": 0.7279,
      "step": 28570
    },
    {
      "epoch": 1.1284400047380265,
      "grad_norm": 0.7762399911880493,
      "learning_rate": 4.3579973940853636e-05,
      "loss": 0.7108,
      "step": 28580
    },
    {
      "epoch": 1.1288348402890196,
      "grad_norm": 0.7680285573005676,
      "learning_rate": 4.3560232163303986e-05,
      "loss": 0.7646,
      "step": 28590
    },
    {
      "epoch": 1.1292296758400127,
      "grad_norm": 0.8141369223594666,
      "learning_rate": 4.3540490385754336e-05,
      "loss": 0.7165,
      "step": 28600
    },
    {
      "epoch": 1.1296245113910057,
      "grad_norm": 0.709025502204895,
      "learning_rate": 4.3520748608204686e-05,
      "loss": 0.7121,
      "step": 28610
    },
    {
      "epoch": 1.1300193469419986,
      "grad_norm": 0.827741801738739,
      "learning_rate": 4.3501006830655035e-05,
      "loss": 0.7202,
      "step": 28620
    },
    {
      "epoch": 1.1304141824929916,
      "grad_norm": 0.6873925924301147,
      "learning_rate": 4.3481265053105385e-05,
      "loss": 0.6813,
      "step": 28630
    },
    {
      "epoch": 1.1308090180439847,
      "grad_norm": 0.6432833671569824,
      "learning_rate": 4.3461523275555735e-05,
      "loss": 0.6855,
      "step": 28640
    },
    {
      "epoch": 1.1312038535949778,
      "grad_norm": 1.1478710174560547,
      "learning_rate": 4.3441781498006085e-05,
      "loss": 0.7213,
      "step": 28650
    },
    {
      "epoch": 1.1315986891459706,
      "grad_norm": 0.7399161458015442,
      "learning_rate": 4.342203972045643e-05,
      "loss": 0.7542,
      "step": 28660
    },
    {
      "epoch": 1.1319935246969637,
      "grad_norm": 0.7667418718338013,
      "learning_rate": 4.3402297942906785e-05,
      "loss": 0.7205,
      "step": 28670
    },
    {
      "epoch": 1.1323883602479567,
      "grad_norm": 0.7631151676177979,
      "learning_rate": 4.338255616535713e-05,
      "loss": 0.7081,
      "step": 28680
    },
    {
      "epoch": 1.1327831957989498,
      "grad_norm": 0.7980221509933472,
      "learning_rate": 4.3362814387807485e-05,
      "loss": 0.6896,
      "step": 28690
    },
    {
      "epoch": 1.1331780313499427,
      "grad_norm": 0.6696130037307739,
      "learning_rate": 4.334307261025783e-05,
      "loss": 0.7133,
      "step": 28700
    },
    {
      "epoch": 1.1335728669009357,
      "grad_norm": 0.7259399890899658,
      "learning_rate": 4.332333083270818e-05,
      "loss": 0.69,
      "step": 28710
    },
    {
      "epoch": 1.1339677024519288,
      "grad_norm": 0.6835403442382812,
      "learning_rate": 4.330358905515853e-05,
      "loss": 0.7071,
      "step": 28720
    },
    {
      "epoch": 1.1343625380029219,
      "grad_norm": 0.6203415393829346,
      "learning_rate": 4.328384727760888e-05,
      "loss": 0.7161,
      "step": 28730
    },
    {
      "epoch": 1.1347573735539147,
      "grad_norm": 0.5852274894714355,
      "learning_rate": 4.326410550005923e-05,
      "loss": 0.6994,
      "step": 28740
    },
    {
      "epoch": 1.1351522091049078,
      "grad_norm": 0.6438193917274475,
      "learning_rate": 4.324436372250958e-05,
      "loss": 0.6806,
      "step": 28750
    },
    {
      "epoch": 1.1355470446559008,
      "grad_norm": 0.7645137310028076,
      "learning_rate": 4.322462194495993e-05,
      "loss": 0.712,
      "step": 28760
    },
    {
      "epoch": 1.135941880206894,
      "grad_norm": 0.6100319623947144,
      "learning_rate": 4.320488016741028e-05,
      "loss": 0.718,
      "step": 28770
    },
    {
      "epoch": 1.1363367157578868,
      "grad_norm": 0.706709623336792,
      "learning_rate": 4.318513838986063e-05,
      "loss": 0.7009,
      "step": 28780
    },
    {
      "epoch": 1.1367315513088798,
      "grad_norm": 0.7012051939964294,
      "learning_rate": 4.316539661231098e-05,
      "loss": 0.7175,
      "step": 28790
    },
    {
      "epoch": 1.1371263868598729,
      "grad_norm": 0.8235303163528442,
      "learning_rate": 4.314565483476132e-05,
      "loss": 0.7269,
      "step": 28800
    },
    {
      "epoch": 1.137521222410866,
      "grad_norm": 0.8216093182563782,
      "learning_rate": 4.312591305721168e-05,
      "loss": 0.7352,
      "step": 28810
    },
    {
      "epoch": 1.1379160579618588,
      "grad_norm": 0.813254177570343,
      "learning_rate": 4.310617127966202e-05,
      "loss": 0.747,
      "step": 28820
    },
    {
      "epoch": 1.1383108935128519,
      "grad_norm": 0.7530931234359741,
      "learning_rate": 4.308642950211238e-05,
      "loss": 0.6868,
      "step": 28830
    },
    {
      "epoch": 1.138705729063845,
      "grad_norm": 0.7113767862319946,
      "learning_rate": 4.306668772456272e-05,
      "loss": 0.767,
      "step": 28840
    },
    {
      "epoch": 1.139100564614838,
      "grad_norm": 0.9219872951507568,
      "learning_rate": 4.304694594701307e-05,
      "loss": 0.7237,
      "step": 28850
    },
    {
      "epoch": 1.1394954001658308,
      "grad_norm": 0.6972434520721436,
      "learning_rate": 4.302720416946342e-05,
      "loss": 0.7559,
      "step": 28860
    },
    {
      "epoch": 1.139890235716824,
      "grad_norm": 0.6790257096290588,
      "learning_rate": 4.300746239191377e-05,
      "loss": 0.705,
      "step": 28870
    },
    {
      "epoch": 1.140285071267817,
      "grad_norm": 0.9558070302009583,
      "learning_rate": 4.298772061436412e-05,
      "loss": 0.7293,
      "step": 28880
    },
    {
      "epoch": 1.14067990681881,
      "grad_norm": 0.7287107110023499,
      "learning_rate": 4.296797883681447e-05,
      "loss": 0.7083,
      "step": 28890
    },
    {
      "epoch": 1.1410747423698029,
      "grad_norm": 1.1926020383834839,
      "learning_rate": 4.294823705926482e-05,
      "loss": 0.7834,
      "step": 28900
    },
    {
      "epoch": 1.141469577920796,
      "grad_norm": 0.5713692307472229,
      "learning_rate": 4.292849528171517e-05,
      "loss": 0.713,
      "step": 28910
    },
    {
      "epoch": 1.141864413471789,
      "grad_norm": 0.7418673038482666,
      "learning_rate": 4.290875350416552e-05,
      "loss": 0.7086,
      "step": 28920
    },
    {
      "epoch": 1.142259249022782,
      "grad_norm": 1.232978105545044,
      "learning_rate": 4.288901172661587e-05,
      "loss": 0.7562,
      "step": 28930
    },
    {
      "epoch": 1.142654084573775,
      "grad_norm": 0.6705382466316223,
      "learning_rate": 4.286926994906621e-05,
      "loss": 0.7297,
      "step": 28940
    },
    {
      "epoch": 1.143048920124768,
      "grad_norm": 0.744253933429718,
      "learning_rate": 4.284952817151657e-05,
      "loss": 0.7345,
      "step": 28950
    },
    {
      "epoch": 1.143443755675761,
      "grad_norm": 0.6603188514709473,
      "learning_rate": 4.282978639396691e-05,
      "loss": 0.6839,
      "step": 28960
    },
    {
      "epoch": 1.1438385912267541,
      "grad_norm": 0.8632189631462097,
      "learning_rate": 4.281004461641727e-05,
      "loss": 0.7461,
      "step": 28970
    },
    {
      "epoch": 1.144233426777747,
      "grad_norm": 0.9859277606010437,
      "learning_rate": 4.279030283886761e-05,
      "loss": 0.7169,
      "step": 28980
    },
    {
      "epoch": 1.14462826232874,
      "grad_norm": 0.6938681602478027,
      "learning_rate": 4.277056106131796e-05,
      "loss": 0.6515,
      "step": 28990
    },
    {
      "epoch": 1.145023097879733,
      "grad_norm": 0.8341150283813477,
      "learning_rate": 4.275081928376831e-05,
      "loss": 0.7213,
      "step": 29000
    },
    {
      "epoch": 1.145023097879733,
      "eval_loss": 0.7723854780197144,
      "eval_runtime": 1217.9835,
      "eval_samples_per_second": 9.242,
      "eval_steps_per_second": 9.242,
      "step": 29000
    },
    {
      "epoch": 1.1454179334307262,
      "grad_norm": 0.6281573176383972,
      "learning_rate": 4.273107750621866e-05,
      "loss": 0.6651,
      "step": 29010
    },
    {
      "epoch": 1.145812768981719,
      "grad_norm": 0.7645733952522278,
      "learning_rate": 4.271133572866901e-05,
      "loss": 0.685,
      "step": 29020
    },
    {
      "epoch": 1.146207604532712,
      "grad_norm": 0.8795298933982849,
      "learning_rate": 4.269159395111936e-05,
      "loss": 0.7515,
      "step": 29030
    },
    {
      "epoch": 1.1466024400837052,
      "grad_norm": 0.6906418204307556,
      "learning_rate": 4.267185217356971e-05,
      "loss": 0.746,
      "step": 29040
    },
    {
      "epoch": 1.1469972756346982,
      "grad_norm": 0.688812255859375,
      "learning_rate": 4.265211039602006e-05,
      "loss": 0.6879,
      "step": 29050
    },
    {
      "epoch": 1.147392111185691,
      "grad_norm": 0.7488932609558105,
      "learning_rate": 4.263236861847041e-05,
      "loss": 0.7519,
      "step": 29060
    },
    {
      "epoch": 1.1477869467366841,
      "grad_norm": 0.6689545512199402,
      "learning_rate": 4.261262684092076e-05,
      "loss": 0.7245,
      "step": 29070
    },
    {
      "epoch": 1.1481817822876772,
      "grad_norm": 0.6519635319709778,
      "learning_rate": 4.2592885063371104e-05,
      "loss": 0.6951,
      "step": 29080
    },
    {
      "epoch": 1.1485766178386703,
      "grad_norm": 0.8382364511489868,
      "learning_rate": 4.257314328582146e-05,
      "loss": 0.7409,
      "step": 29090
    },
    {
      "epoch": 1.148971453389663,
      "grad_norm": 0.72476726770401,
      "learning_rate": 4.2553401508271804e-05,
      "loss": 0.7259,
      "step": 29100
    },
    {
      "epoch": 1.1493662889406562,
      "grad_norm": 0.8003036379814148,
      "learning_rate": 4.253365973072216e-05,
      "loss": 0.6877,
      "step": 29110
    },
    {
      "epoch": 1.1497611244916492,
      "grad_norm": 0.6511353850364685,
      "learning_rate": 4.2513917953172504e-05,
      "loss": 0.7154,
      "step": 29120
    },
    {
      "epoch": 1.1501559600426423,
      "grad_norm": 0.6449432969093323,
      "learning_rate": 4.2494176175622854e-05,
      "loss": 0.7285,
      "step": 29130
    },
    {
      "epoch": 1.1505507955936352,
      "grad_norm": 0.6485321521759033,
      "learning_rate": 4.2474434398073204e-05,
      "loss": 0.7303,
      "step": 29140
    },
    {
      "epoch": 1.1509456311446282,
      "grad_norm": 0.7139671444892883,
      "learning_rate": 4.2454692620523554e-05,
      "loss": 0.7377,
      "step": 29150
    },
    {
      "epoch": 1.1513404666956213,
      "grad_norm": 0.7119637727737427,
      "learning_rate": 4.2434950842973904e-05,
      "loss": 0.7522,
      "step": 29160
    },
    {
      "epoch": 1.1517353022466144,
      "grad_norm": 0.8331828713417053,
      "learning_rate": 4.2415209065424254e-05,
      "loss": 0.73,
      "step": 29170
    },
    {
      "epoch": 1.1521301377976072,
      "grad_norm": 0.7226681113243103,
      "learning_rate": 4.2395467287874604e-05,
      "loss": 0.6675,
      "step": 29180
    },
    {
      "epoch": 1.1525249733486003,
      "grad_norm": 0.6430936455726624,
      "learning_rate": 4.2375725510324953e-05,
      "loss": 0.7163,
      "step": 29190
    },
    {
      "epoch": 1.1529198088995933,
      "grad_norm": 0.8284331560134888,
      "learning_rate": 4.23559837327753e-05,
      "loss": 0.7101,
      "step": 29200
    },
    {
      "epoch": 1.1533146444505864,
      "grad_norm": 0.708416223526001,
      "learning_rate": 4.233624195522565e-05,
      "loss": 0.7283,
      "step": 29210
    },
    {
      "epoch": 1.1537094800015792,
      "grad_norm": 0.7986346483230591,
      "learning_rate": 4.2316500177675996e-05,
      "loss": 0.7561,
      "step": 29220
    },
    {
      "epoch": 1.1541043155525723,
      "grad_norm": 0.5155981779098511,
      "learning_rate": 4.229675840012635e-05,
      "loss": 0.715,
      "step": 29230
    },
    {
      "epoch": 1.1544991511035654,
      "grad_norm": 0.7291030883789062,
      "learning_rate": 4.2277016622576696e-05,
      "loss": 0.6919,
      "step": 29240
    },
    {
      "epoch": 1.1548939866545584,
      "grad_norm": 0.5929808616638184,
      "learning_rate": 4.225727484502705e-05,
      "loss": 0.7042,
      "step": 29250
    },
    {
      "epoch": 1.1552888222055513,
      "grad_norm": 0.8079078197479248,
      "learning_rate": 4.2237533067477396e-05,
      "loss": 0.7005,
      "step": 29260
    },
    {
      "epoch": 1.1556836577565444,
      "grad_norm": 0.8181087374687195,
      "learning_rate": 4.221779128992775e-05,
      "loss": 0.7507,
      "step": 29270
    },
    {
      "epoch": 1.1560784933075374,
      "grad_norm": 0.7233055830001831,
      "learning_rate": 4.2198049512378096e-05,
      "loss": 0.6733,
      "step": 29280
    },
    {
      "epoch": 1.1564733288585305,
      "grad_norm": 0.6872578263282776,
      "learning_rate": 4.2178307734828446e-05,
      "loss": 0.684,
      "step": 29290
    },
    {
      "epoch": 1.1568681644095236,
      "grad_norm": 0.6467658281326294,
      "learning_rate": 4.2158565957278796e-05,
      "loss": 0.7393,
      "step": 29300
    },
    {
      "epoch": 1.1572629999605164,
      "grad_norm": 0.7613760232925415,
      "learning_rate": 4.2138824179729146e-05,
      "loss": 0.6965,
      "step": 29310
    },
    {
      "epoch": 1.1576578355115095,
      "grad_norm": 0.7166652083396912,
      "learning_rate": 4.2119082402179496e-05,
      "loss": 0.7266,
      "step": 29320
    },
    {
      "epoch": 1.1580526710625025,
      "grad_norm": 0.8646122217178345,
      "learning_rate": 4.2099340624629845e-05,
      "loss": 0.7112,
      "step": 29330
    },
    {
      "epoch": 1.1584475066134954,
      "grad_norm": 0.7963073253631592,
      "learning_rate": 4.2079598847080195e-05,
      "loss": 0.6989,
      "step": 29340
    },
    {
      "epoch": 1.1588423421644884,
      "grad_norm": 0.9227445125579834,
      "learning_rate": 4.2059857069530545e-05,
      "loss": 0.7294,
      "step": 29350
    },
    {
      "epoch": 1.1592371777154815,
      "grad_norm": 0.680535078048706,
      "learning_rate": 4.204011529198089e-05,
      "loss": 0.6913,
      "step": 29360
    },
    {
      "epoch": 1.1596320132664746,
      "grad_norm": 0.7252795696258545,
      "learning_rate": 4.2020373514431245e-05,
      "loss": 0.6908,
      "step": 29370
    },
    {
      "epoch": 1.1600268488174676,
      "grad_norm": 0.7824450135231018,
      "learning_rate": 4.200063173688159e-05,
      "loss": 0.6907,
      "step": 29380
    },
    {
      "epoch": 1.1604216843684605,
      "grad_norm": 0.6303339600563049,
      "learning_rate": 4.1980889959331945e-05,
      "loss": 0.7359,
      "step": 29390
    },
    {
      "epoch": 1.1608165199194536,
      "grad_norm": 1.1333096027374268,
      "learning_rate": 4.196114818178229e-05,
      "loss": 0.734,
      "step": 29400
    },
    {
      "epoch": 1.1612113554704466,
      "grad_norm": 0.7351979613304138,
      "learning_rate": 4.1941406404232645e-05,
      "loss": 0.6974,
      "step": 29410
    },
    {
      "epoch": 1.1616061910214395,
      "grad_norm": 0.6834392547607422,
      "learning_rate": 4.192166462668299e-05,
      "loss": 0.7003,
      "step": 29420
    },
    {
      "epoch": 1.1620010265724325,
      "grad_norm": 0.6276105046272278,
      "learning_rate": 4.190192284913334e-05,
      "loss": 0.7222,
      "step": 29430
    },
    {
      "epoch": 1.1623958621234256,
      "grad_norm": 0.9120824933052063,
      "learning_rate": 4.188218107158369e-05,
      "loss": 0.7274,
      "step": 29440
    },
    {
      "epoch": 1.1627906976744187,
      "grad_norm": 0.6887070536613464,
      "learning_rate": 4.186243929403404e-05,
      "loss": 0.7208,
      "step": 29450
    },
    {
      "epoch": 1.1631855332254117,
      "grad_norm": 0.8998098969459534,
      "learning_rate": 4.184269751648439e-05,
      "loss": 0.692,
      "step": 29460
    },
    {
      "epoch": 1.1635803687764046,
      "grad_norm": 0.6562933921813965,
      "learning_rate": 4.182295573893474e-05,
      "loss": 0.6916,
      "step": 29470
    },
    {
      "epoch": 1.1639752043273977,
      "grad_norm": 0.7598223686218262,
      "learning_rate": 4.180321396138509e-05,
      "loss": 0.6835,
      "step": 29480
    },
    {
      "epoch": 1.1643700398783907,
      "grad_norm": 0.8085982203483582,
      "learning_rate": 4.178347218383544e-05,
      "loss": 0.6997,
      "step": 29490
    },
    {
      "epoch": 1.1647648754293836,
      "grad_norm": 0.7644619345664978,
      "learning_rate": 4.176373040628578e-05,
      "loss": 0.7028,
      "step": 29500
    },
    {
      "epoch": 1.1651597109803766,
      "grad_norm": 0.8935216069221497,
      "learning_rate": 4.174398862873614e-05,
      "loss": 0.7393,
      "step": 29510
    },
    {
      "epoch": 1.1655545465313697,
      "grad_norm": 0.7975622415542603,
      "learning_rate": 4.172424685118648e-05,
      "loss": 0.6996,
      "step": 29520
    },
    {
      "epoch": 1.1659493820823628,
      "grad_norm": 0.6593167185783386,
      "learning_rate": 4.170450507363684e-05,
      "loss": 0.6957,
      "step": 29530
    },
    {
      "epoch": 1.1663442176333558,
      "grad_norm": 0.720492422580719,
      "learning_rate": 4.168476329608718e-05,
      "loss": 0.6953,
      "step": 29540
    },
    {
      "epoch": 1.1667390531843487,
      "grad_norm": 0.7070003747940063,
      "learning_rate": 4.166502151853754e-05,
      "loss": 0.6762,
      "step": 29550
    },
    {
      "epoch": 1.1671338887353417,
      "grad_norm": 0.7170095443725586,
      "learning_rate": 4.164527974098788e-05,
      "loss": 0.7618,
      "step": 29560
    },
    {
      "epoch": 1.1675287242863348,
      "grad_norm": 0.8908891081809998,
      "learning_rate": 4.162553796343823e-05,
      "loss": 0.7553,
      "step": 29570
    },
    {
      "epoch": 1.1679235598373277,
      "grad_norm": 0.634813666343689,
      "learning_rate": 4.160579618588858e-05,
      "loss": 0.7295,
      "step": 29580
    },
    {
      "epoch": 1.1683183953883207,
      "grad_norm": 0.7110533714294434,
      "learning_rate": 4.158605440833893e-05,
      "loss": 0.7469,
      "step": 29590
    },
    {
      "epoch": 1.1687132309393138,
      "grad_norm": 0.8729596734046936,
      "learning_rate": 4.156631263078928e-05,
      "loss": 0.7595,
      "step": 29600
    },
    {
      "epoch": 1.1691080664903069,
      "grad_norm": 0.7537528276443481,
      "learning_rate": 4.154657085323963e-05,
      "loss": 0.7462,
      "step": 29610
    },
    {
      "epoch": 1.1695029020413,
      "grad_norm": 0.663101851940155,
      "learning_rate": 4.152682907568998e-05,
      "loss": 0.6963,
      "step": 29620
    },
    {
      "epoch": 1.1698977375922928,
      "grad_norm": 0.7265164852142334,
      "learning_rate": 4.150708729814033e-05,
      "loss": 0.6986,
      "step": 29630
    },
    {
      "epoch": 1.1702925731432858,
      "grad_norm": 0.7798610329627991,
      "learning_rate": 4.148734552059067e-05,
      "loss": 0.7224,
      "step": 29640
    },
    {
      "epoch": 1.170687408694279,
      "grad_norm": 0.7276996970176697,
      "learning_rate": 4.146760374304103e-05,
      "loss": 0.7094,
      "step": 29650
    },
    {
      "epoch": 1.1710822442452717,
      "grad_norm": 0.6927380561828613,
      "learning_rate": 4.144786196549137e-05,
      "loss": 0.671,
      "step": 29660
    },
    {
      "epoch": 1.1714770797962648,
      "grad_norm": 0.8437902331352234,
      "learning_rate": 4.142812018794173e-05,
      "loss": 0.6967,
      "step": 29670
    },
    {
      "epoch": 1.1718719153472579,
      "grad_norm": 0.7370715141296387,
      "learning_rate": 4.140837841039207e-05,
      "loss": 0.7133,
      "step": 29680
    },
    {
      "epoch": 1.172266750898251,
      "grad_norm": 0.8586921095848083,
      "learning_rate": 4.138863663284243e-05,
      "loss": 0.7272,
      "step": 29690
    },
    {
      "epoch": 1.172661586449244,
      "grad_norm": 0.6108343601226807,
      "learning_rate": 4.136889485529277e-05,
      "loss": 0.6684,
      "step": 29700
    },
    {
      "epoch": 1.1730564220002369,
      "grad_norm": 0.6805786490440369,
      "learning_rate": 4.134915307774312e-05,
      "loss": 0.7266,
      "step": 29710
    },
    {
      "epoch": 1.17345125755123,
      "grad_norm": 0.70382159948349,
      "learning_rate": 4.132941130019347e-05,
      "loss": 0.7203,
      "step": 29720
    },
    {
      "epoch": 1.173846093102223,
      "grad_norm": 0.7696460485458374,
      "learning_rate": 4.130966952264382e-05,
      "loss": 0.6933,
      "step": 29730
    },
    {
      "epoch": 1.1742409286532158,
      "grad_norm": 0.7206432223320007,
      "learning_rate": 4.128992774509417e-05,
      "loss": 0.7115,
      "step": 29740
    },
    {
      "epoch": 1.174635764204209,
      "grad_norm": 0.9083197116851807,
      "learning_rate": 4.127018596754452e-05,
      "loss": 0.7121,
      "step": 29750
    },
    {
      "epoch": 1.175030599755202,
      "grad_norm": 0.5702319741249084,
      "learning_rate": 4.125044418999487e-05,
      "loss": 0.6862,
      "step": 29760
    },
    {
      "epoch": 1.175425435306195,
      "grad_norm": 0.925861120223999,
      "learning_rate": 4.123070241244522e-05,
      "loss": 0.6985,
      "step": 29770
    },
    {
      "epoch": 1.175820270857188,
      "grad_norm": 0.6537246108055115,
      "learning_rate": 4.121096063489557e-05,
      "loss": 0.7065,
      "step": 29780
    },
    {
      "epoch": 1.176215106408181,
      "grad_norm": 0.7873780727386475,
      "learning_rate": 4.119121885734592e-05,
      "loss": 0.6995,
      "step": 29790
    },
    {
      "epoch": 1.176609941959174,
      "grad_norm": 0.828796923160553,
      "learning_rate": 4.1171477079796264e-05,
      "loss": 0.6834,
      "step": 29800
    },
    {
      "epoch": 1.177004777510167,
      "grad_norm": 0.8381232619285583,
      "learning_rate": 4.115173530224662e-05,
      "loss": 0.7148,
      "step": 29810
    },
    {
      "epoch": 1.17739961306116,
      "grad_norm": 0.6661152243614197,
      "learning_rate": 4.1131993524696964e-05,
      "loss": 0.713,
      "step": 29820
    },
    {
      "epoch": 1.177794448612153,
      "grad_norm": 0.7265788912773132,
      "learning_rate": 4.111225174714732e-05,
      "loss": 0.7194,
      "step": 29830
    },
    {
      "epoch": 1.178189284163146,
      "grad_norm": 0.7655320167541504,
      "learning_rate": 4.1092509969597664e-05,
      "loss": 0.747,
      "step": 29840
    },
    {
      "epoch": 1.1785841197141391,
      "grad_norm": 0.7739152908325195,
      "learning_rate": 4.1072768192048014e-05,
      "loss": 0.7159,
      "step": 29850
    },
    {
      "epoch": 1.1789789552651322,
      "grad_norm": 0.7598598599433899,
      "learning_rate": 4.1053026414498364e-05,
      "loss": 0.722,
      "step": 29860
    },
    {
      "epoch": 1.179373790816125,
      "grad_norm": 0.7503426671028137,
      "learning_rate": 4.103328463694871e-05,
      "loss": 0.6902,
      "step": 29870
    },
    {
      "epoch": 1.179768626367118,
      "grad_norm": 0.8422060608863831,
      "learning_rate": 4.1013542859399064e-05,
      "loss": 0.6813,
      "step": 29880
    },
    {
      "epoch": 1.1801634619181112,
      "grad_norm": 0.8643458485603333,
      "learning_rate": 4.099380108184941e-05,
      "loss": 0.7282,
      "step": 29890
    },
    {
      "epoch": 1.180558297469104,
      "grad_norm": 0.8827998638153076,
      "learning_rate": 4.0974059304299763e-05,
      "loss": 0.7106,
      "step": 29900
    },
    {
      "epoch": 1.180953133020097,
      "grad_norm": 0.5848124623298645,
      "learning_rate": 4.0954317526750107e-05,
      "loss": 0.7353,
      "step": 29910
    },
    {
      "epoch": 1.1813479685710901,
      "grad_norm": 1.067871332168579,
      "learning_rate": 4.093457574920046e-05,
      "loss": 0.7342,
      "step": 29920
    },
    {
      "epoch": 1.1817428041220832,
      "grad_norm": 0.6849157214164734,
      "learning_rate": 4.0914833971650806e-05,
      "loss": 0.7177,
      "step": 29930
    },
    {
      "epoch": 1.1821376396730763,
      "grad_norm": 0.6373343467712402,
      "learning_rate": 4.0895092194101156e-05,
      "loss": 0.6842,
      "step": 29940
    },
    {
      "epoch": 1.1825324752240691,
      "grad_norm": 0.9892172813415527,
      "learning_rate": 4.0875350416551506e-05,
      "loss": 0.7309,
      "step": 29950
    },
    {
      "epoch": 1.1829273107750622,
      "grad_norm": 0.9829723834991455,
      "learning_rate": 4.0855608639001856e-05,
      "loss": 0.7026,
      "step": 29960
    },
    {
      "epoch": 1.1833221463260553,
      "grad_norm": 0.7864218354225159,
      "learning_rate": 4.0835866861452206e-05,
      "loss": 0.6939,
      "step": 29970
    },
    {
      "epoch": 1.183716981877048,
      "grad_norm": 0.8677445650100708,
      "learning_rate": 4.0816125083902556e-05,
      "loss": 0.7267,
      "step": 29980
    },
    {
      "epoch": 1.1841118174280412,
      "grad_norm": 0.7631906867027283,
      "learning_rate": 4.0796383306352906e-05,
      "loss": 0.7596,
      "step": 29990
    },
    {
      "epoch": 1.1845066529790342,
      "grad_norm": 0.6236861348152161,
      "learning_rate": 4.0776641528803256e-05,
      "loss": 0.7075,
      "step": 30000
    },
    {
      "epoch": 1.1845066529790342,
      "eval_loss": 0.7709037661552429,
      "eval_runtime": 1217.7293,
      "eval_samples_per_second": 9.244,
      "eval_steps_per_second": 9.244,
      "step": 30000
    },
    {
      "epoch": 1.1849014885300273,
      "grad_norm": 0.9882060289382935,
      "learning_rate": 4.07568997512536e-05,
      "loss": 0.7009,
      "step": 30010
    },
    {
      "epoch": 1.1852963240810204,
      "grad_norm": 0.8111284971237183,
      "learning_rate": 4.0737157973703956e-05,
      "loss": 0.7564,
      "step": 30020
    },
    {
      "epoch": 1.1856911596320132,
      "grad_norm": 0.9930235743522644,
      "learning_rate": 4.07174161961543e-05,
      "loss": 0.6945,
      "step": 30030
    },
    {
      "epoch": 1.1860859951830063,
      "grad_norm": 0.9524889588356018,
      "learning_rate": 4.0697674418604655e-05,
      "loss": 0.7087,
      "step": 30040
    },
    {
      "epoch": 1.1864808307339993,
      "grad_norm": 0.8373411297798157,
      "learning_rate": 4.0677932641055e-05,
      "loss": 0.6981,
      "step": 30050
    },
    {
      "epoch": 1.1868756662849922,
      "grad_norm": 0.8421812057495117,
      "learning_rate": 4.0658190863505355e-05,
      "loss": 0.7589,
      "step": 30060
    },
    {
      "epoch": 1.1872705018359853,
      "grad_norm": 0.6624682545661926,
      "learning_rate": 4.06384490859557e-05,
      "loss": 0.687,
      "step": 30070
    },
    {
      "epoch": 1.1876653373869783,
      "grad_norm": 0.8289124369621277,
      "learning_rate": 4.061870730840605e-05,
      "loss": 0.739,
      "step": 30080
    },
    {
      "epoch": 1.1880601729379714,
      "grad_norm": 0.8535651564598083,
      "learning_rate": 4.05989655308564e-05,
      "loss": 0.7314,
      "step": 30090
    },
    {
      "epoch": 1.1884550084889645,
      "grad_norm": 0.7366442084312439,
      "learning_rate": 4.057922375330675e-05,
      "loss": 0.7228,
      "step": 30100
    },
    {
      "epoch": 1.1888498440399573,
      "grad_norm": 0.6875633001327515,
      "learning_rate": 4.05594819757571e-05,
      "loss": 0.7482,
      "step": 30110
    },
    {
      "epoch": 1.1892446795909504,
      "grad_norm": 0.693162739276886,
      "learning_rate": 4.053974019820745e-05,
      "loss": 0.7288,
      "step": 30120
    },
    {
      "epoch": 1.1896395151419434,
      "grad_norm": 0.7550169229507446,
      "learning_rate": 4.05199984206578e-05,
      "loss": 0.7109,
      "step": 30130
    },
    {
      "epoch": 1.1900343506929363,
      "grad_norm": 0.7462242245674133,
      "learning_rate": 4.050025664310815e-05,
      "loss": 0.7234,
      "step": 30140
    },
    {
      "epoch": 1.1904291862439293,
      "grad_norm": 0.7716476917266846,
      "learning_rate": 4.048051486555849e-05,
      "loss": 0.7089,
      "step": 30150
    },
    {
      "epoch": 1.1908240217949224,
      "grad_norm": 0.666239857673645,
      "learning_rate": 4.046077308800885e-05,
      "loss": 0.6677,
      "step": 30160
    },
    {
      "epoch": 1.1912188573459155,
      "grad_norm": 0.6693254709243774,
      "learning_rate": 4.044103131045919e-05,
      "loss": 0.7388,
      "step": 30170
    },
    {
      "epoch": 1.1916136928969085,
      "grad_norm": 0.8717870712280273,
      "learning_rate": 4.042128953290955e-05,
      "loss": 0.6867,
      "step": 30180
    },
    {
      "epoch": 1.1920085284479014,
      "grad_norm": 0.9533044099807739,
      "learning_rate": 4.040154775535989e-05,
      "loss": 0.7437,
      "step": 30190
    },
    {
      "epoch": 1.1924033639988945,
      "grad_norm": 0.7543408870697021,
      "learning_rate": 4.038180597781025e-05,
      "loss": 0.7532,
      "step": 30200
    },
    {
      "epoch": 1.1927981995498875,
      "grad_norm": 0.9113190770149231,
      "learning_rate": 4.036206420026059e-05,
      "loss": 0.7811,
      "step": 30210
    },
    {
      "epoch": 1.1931930351008804,
      "grad_norm": 0.7212689518928528,
      "learning_rate": 4.034232242271094e-05,
      "loss": 0.6995,
      "step": 30220
    },
    {
      "epoch": 1.1935878706518734,
      "grad_norm": 0.8462594747543335,
      "learning_rate": 4.032258064516129e-05,
      "loss": 0.6696,
      "step": 30230
    },
    {
      "epoch": 1.1939827062028665,
      "grad_norm": 0.7318055033683777,
      "learning_rate": 4.030283886761164e-05,
      "loss": 0.7033,
      "step": 30240
    },
    {
      "epoch": 1.1943775417538596,
      "grad_norm": 0.6667013168334961,
      "learning_rate": 4.028309709006199e-05,
      "loss": 0.7457,
      "step": 30250
    },
    {
      "epoch": 1.1947723773048526,
      "grad_norm": 0.7106568813323975,
      "learning_rate": 4.026335531251234e-05,
      "loss": 0.6847,
      "step": 30260
    },
    {
      "epoch": 1.1951672128558455,
      "grad_norm": 0.8505116701126099,
      "learning_rate": 4.024361353496269e-05,
      "loss": 0.7165,
      "step": 30270
    },
    {
      "epoch": 1.1955620484068386,
      "grad_norm": 0.7443385124206543,
      "learning_rate": 4.022387175741304e-05,
      "loss": 0.7225,
      "step": 30280
    },
    {
      "epoch": 1.1959568839578316,
      "grad_norm": 0.8313573598861694,
      "learning_rate": 4.020412997986338e-05,
      "loss": 0.7103,
      "step": 30290
    },
    {
      "epoch": 1.1963517195088245,
      "grad_norm": 0.6578382849693298,
      "learning_rate": 4.018438820231374e-05,
      "loss": 0.7244,
      "step": 30300
    },
    {
      "epoch": 1.1967465550598175,
      "grad_norm": 0.6649301052093506,
      "learning_rate": 4.016464642476408e-05,
      "loss": 0.6844,
      "step": 30310
    },
    {
      "epoch": 1.1971413906108106,
      "grad_norm": 0.7520667910575867,
      "learning_rate": 4.014490464721444e-05,
      "loss": 0.7019,
      "step": 30320
    },
    {
      "epoch": 1.1975362261618037,
      "grad_norm": 0.822127103805542,
      "learning_rate": 4.012516286966478e-05,
      "loss": 0.7249,
      "step": 30330
    },
    {
      "epoch": 1.1979310617127967,
      "grad_norm": 0.9400036334991455,
      "learning_rate": 4.010542109211514e-05,
      "loss": 0.7368,
      "step": 30340
    },
    {
      "epoch": 1.1983258972637896,
      "grad_norm": 0.7024474143981934,
      "learning_rate": 4.008567931456548e-05,
      "loss": 0.6732,
      "step": 30350
    },
    {
      "epoch": 1.1987207328147826,
      "grad_norm": 0.7266488671302795,
      "learning_rate": 4.006593753701583e-05,
      "loss": 0.6914,
      "step": 30360
    },
    {
      "epoch": 1.1991155683657757,
      "grad_norm": 0.7786797881126404,
      "learning_rate": 4.004619575946618e-05,
      "loss": 0.7315,
      "step": 30370
    },
    {
      "epoch": 1.1995104039167686,
      "grad_norm": 0.8292233347892761,
      "learning_rate": 4.002645398191653e-05,
      "loss": 0.6929,
      "step": 30380
    },
    {
      "epoch": 1.1999052394677616,
      "grad_norm": 0.8679463863372803,
      "learning_rate": 4.000671220436688e-05,
      "loss": 0.7046,
      "step": 30390
    },
    {
      "epoch": 1.2003000750187547,
      "grad_norm": 0.7563527226448059,
      "learning_rate": 3.998697042681723e-05,
      "loss": 0.7475,
      "step": 30400
    },
    {
      "epoch": 1.2006949105697478,
      "grad_norm": 0.7181946635246277,
      "learning_rate": 3.996722864926758e-05,
      "loss": 0.7357,
      "step": 30410
    },
    {
      "epoch": 1.2010897461207408,
      "grad_norm": 0.8148647546768188,
      "learning_rate": 3.994748687171793e-05,
      "loss": 0.7213,
      "step": 30420
    },
    {
      "epoch": 1.2014845816717337,
      "grad_norm": 0.7212578654289246,
      "learning_rate": 3.992774509416828e-05,
      "loss": 0.6813,
      "step": 30430
    },
    {
      "epoch": 1.2018794172227267,
      "grad_norm": 0.8215177655220032,
      "learning_rate": 3.990800331661863e-05,
      "loss": 0.6833,
      "step": 30440
    },
    {
      "epoch": 1.2022742527737198,
      "grad_norm": 0.8171785473823547,
      "learning_rate": 3.9888261539068975e-05,
      "loss": 0.7162,
      "step": 30450
    },
    {
      "epoch": 1.2026690883247126,
      "grad_norm": 0.6780521869659424,
      "learning_rate": 3.986851976151933e-05,
      "loss": 0.6643,
      "step": 30460
    },
    {
      "epoch": 1.2030639238757057,
      "grad_norm": 0.7640284299850464,
      "learning_rate": 3.9848777983969675e-05,
      "loss": 0.7147,
      "step": 30470
    },
    {
      "epoch": 1.2034587594266988,
      "grad_norm": 0.7378247976303101,
      "learning_rate": 3.982903620642003e-05,
      "loss": 0.7078,
      "step": 30480
    },
    {
      "epoch": 1.2038535949776918,
      "grad_norm": 0.6927576661109924,
      "learning_rate": 3.9809294428870374e-05,
      "loss": 0.71,
      "step": 30490
    },
    {
      "epoch": 1.204248430528685,
      "grad_norm": 0.7828414440155029,
      "learning_rate": 3.9789552651320724e-05,
      "loss": 0.7247,
      "step": 30500
    },
    {
      "epoch": 1.2046432660796778,
      "grad_norm": 0.8190808296203613,
      "learning_rate": 3.9769810873771074e-05,
      "loss": 0.7086,
      "step": 30510
    },
    {
      "epoch": 1.2050381016306708,
      "grad_norm": 0.9399859309196472,
      "learning_rate": 3.9750069096221424e-05,
      "loss": 0.6956,
      "step": 30520
    },
    {
      "epoch": 1.2054329371816639,
      "grad_norm": 0.8405714631080627,
      "learning_rate": 3.9730327318671774e-05,
      "loss": 0.71,
      "step": 30530
    },
    {
      "epoch": 1.2058277727326567,
      "grad_norm": 0.7201347351074219,
      "learning_rate": 3.9710585541122124e-05,
      "loss": 0.7187,
      "step": 30540
    },
    {
      "epoch": 1.2062226082836498,
      "grad_norm": 0.8362877368927002,
      "learning_rate": 3.9690843763572474e-05,
      "loss": 0.7295,
      "step": 30550
    },
    {
      "epoch": 1.2066174438346429,
      "grad_norm": 0.5972827076911926,
      "learning_rate": 3.9671101986022824e-05,
      "loss": 0.6872,
      "step": 30560
    },
    {
      "epoch": 1.207012279385636,
      "grad_norm": 0.8632805347442627,
      "learning_rate": 3.9651360208473174e-05,
      "loss": 0.7593,
      "step": 30570
    },
    {
      "epoch": 1.207407114936629,
      "grad_norm": 0.6928533911705017,
      "learning_rate": 3.9631618430923524e-05,
      "loss": 0.7105,
      "step": 30580
    },
    {
      "epoch": 1.2078019504876218,
      "grad_norm": 0.9339527487754822,
      "learning_rate": 3.961187665337387e-05,
      "loss": 0.6927,
      "step": 30590
    },
    {
      "epoch": 1.208196786038615,
      "grad_norm": 0.9420372247695923,
      "learning_rate": 3.9592134875824224e-05,
      "loss": 0.727,
      "step": 30600
    },
    {
      "epoch": 1.208591621589608,
      "grad_norm": 0.6848968863487244,
      "learning_rate": 3.957239309827457e-05,
      "loss": 0.7093,
      "step": 30610
    },
    {
      "epoch": 1.2089864571406008,
      "grad_norm": 0.8593690395355225,
      "learning_rate": 3.955265132072492e-05,
      "loss": 0.7203,
      "step": 30620
    },
    {
      "epoch": 1.209381292691594,
      "grad_norm": 0.681100606918335,
      "learning_rate": 3.9532909543175267e-05,
      "loss": 0.6884,
      "step": 30630
    },
    {
      "epoch": 1.209776128242587,
      "grad_norm": 0.7310976982116699,
      "learning_rate": 3.9513167765625616e-05,
      "loss": 0.7283,
      "step": 30640
    },
    {
      "epoch": 1.21017096379358,
      "grad_norm": 0.6817885637283325,
      "learning_rate": 3.9493425988075966e-05,
      "loss": 0.7416,
      "step": 30650
    },
    {
      "epoch": 1.210565799344573,
      "grad_norm": 0.958562433719635,
      "learning_rate": 3.9473684210526316e-05,
      "loss": 0.7377,
      "step": 30660
    },
    {
      "epoch": 1.210960634895566,
      "grad_norm": 0.6650480628013611,
      "learning_rate": 3.9453942432976666e-05,
      "loss": 0.7363,
      "step": 30670
    },
    {
      "epoch": 1.211355470446559,
      "grad_norm": 0.7118818163871765,
      "learning_rate": 3.9434200655427016e-05,
      "loss": 0.7221,
      "step": 30680
    },
    {
      "epoch": 1.211750305997552,
      "grad_norm": 0.7834190726280212,
      "learning_rate": 3.9414458877877366e-05,
      "loss": 0.6854,
      "step": 30690
    },
    {
      "epoch": 1.212145141548545,
      "grad_norm": 0.8733275532722473,
      "learning_rate": 3.9394717100327716e-05,
      "loss": 0.68,
      "step": 30700
    },
    {
      "epoch": 1.212539977099538,
      "grad_norm": 0.635925829410553,
      "learning_rate": 3.9374975322778066e-05,
      "loss": 0.6996,
      "step": 30710
    },
    {
      "epoch": 1.212934812650531,
      "grad_norm": 0.7773478627204895,
      "learning_rate": 3.9355233545228416e-05,
      "loss": 0.7553,
      "step": 30720
    },
    {
      "epoch": 1.2133296482015241,
      "grad_norm": 0.7144297957420349,
      "learning_rate": 3.933549176767876e-05,
      "loss": 0.6894,
      "step": 30730
    },
    {
      "epoch": 1.2137244837525172,
      "grad_norm": 0.9000787138938904,
      "learning_rate": 3.9315749990129116e-05,
      "loss": 0.7381,
      "step": 30740
    },
    {
      "epoch": 1.21411931930351,
      "grad_norm": 0.9787129163742065,
      "learning_rate": 3.929600821257946e-05,
      "loss": 0.7164,
      "step": 30750
    },
    {
      "epoch": 1.214514154854503,
      "grad_norm": 0.791519820690155,
      "learning_rate": 3.9276266435029815e-05,
      "loss": 0.7124,
      "step": 30760
    },
    {
      "epoch": 1.2149089904054962,
      "grad_norm": 0.5954875349998474,
      "learning_rate": 3.925652465748016e-05,
      "loss": 0.7249,
      "step": 30770
    },
    {
      "epoch": 1.215303825956489,
      "grad_norm": 0.6045212149620056,
      "learning_rate": 3.923678287993051e-05,
      "loss": 0.6936,
      "step": 30780
    },
    {
      "epoch": 1.215698661507482,
      "grad_norm": 0.8263545036315918,
      "learning_rate": 3.921704110238086e-05,
      "loss": 0.7174,
      "step": 30790
    },
    {
      "epoch": 1.2160934970584751,
      "grad_norm": 0.6749036908149719,
      "learning_rate": 3.919729932483121e-05,
      "loss": 0.6654,
      "step": 30800
    },
    {
      "epoch": 1.2164883326094682,
      "grad_norm": 0.8703361749649048,
      "learning_rate": 3.917755754728156e-05,
      "loss": 0.7476,
      "step": 30810
    },
    {
      "epoch": 1.2168831681604613,
      "grad_norm": 0.8428391218185425,
      "learning_rate": 3.915781576973191e-05,
      "loss": 0.6877,
      "step": 30820
    },
    {
      "epoch": 1.2172780037114541,
      "grad_norm": 0.5739829540252686,
      "learning_rate": 3.913807399218226e-05,
      "loss": 0.6786,
      "step": 30830
    },
    {
      "epoch": 1.2176728392624472,
      "grad_norm": 0.7957761287689209,
      "learning_rate": 3.911833221463261e-05,
      "loss": 0.7114,
      "step": 30840
    },
    {
      "epoch": 1.2180676748134402,
      "grad_norm": 0.7598222494125366,
      "learning_rate": 3.909859043708296e-05,
      "loss": 0.7118,
      "step": 30850
    },
    {
      "epoch": 1.218462510364433,
      "grad_norm": 0.7425572276115417,
      "learning_rate": 3.907884865953331e-05,
      "loss": 0.7335,
      "step": 30860
    },
    {
      "epoch": 1.2188573459154262,
      "grad_norm": 0.9515673518180847,
      "learning_rate": 3.905910688198365e-05,
      "loss": 0.7531,
      "step": 30870
    },
    {
      "epoch": 1.2192521814664192,
      "grad_norm": 0.7830434441566467,
      "learning_rate": 3.903936510443401e-05,
      "loss": 0.717,
      "step": 30880
    },
    {
      "epoch": 1.2196470170174123,
      "grad_norm": 0.7902470231056213,
      "learning_rate": 3.901962332688435e-05,
      "loss": 0.7305,
      "step": 30890
    },
    {
      "epoch": 1.2200418525684054,
      "grad_norm": 0.6196580529212952,
      "learning_rate": 3.899988154933471e-05,
      "loss": 0.7039,
      "step": 30900
    },
    {
      "epoch": 1.2204366881193982,
      "grad_norm": 0.826622486114502,
      "learning_rate": 3.898013977178505e-05,
      "loss": 0.7168,
      "step": 30910
    },
    {
      "epoch": 1.2208315236703913,
      "grad_norm": 0.7694684863090515,
      "learning_rate": 3.89603979942354e-05,
      "loss": 0.7086,
      "step": 30920
    },
    {
      "epoch": 1.2212263592213843,
      "grad_norm": 0.765627920627594,
      "learning_rate": 3.894065621668575e-05,
      "loss": 0.6925,
      "step": 30930
    },
    {
      "epoch": 1.2216211947723772,
      "grad_norm": 0.7247629165649414,
      "learning_rate": 3.89209144391361e-05,
      "loss": 0.6806,
      "step": 30940
    },
    {
      "epoch": 1.2220160303233703,
      "grad_norm": 0.7774859666824341,
      "learning_rate": 3.890117266158645e-05,
      "loss": 0.6913,
      "step": 30950
    },
    {
      "epoch": 1.2224108658743633,
      "grad_norm": 0.7009470462799072,
      "learning_rate": 3.88814308840368e-05,
      "loss": 0.7316,
      "step": 30960
    },
    {
      "epoch": 1.2228057014253564,
      "grad_norm": 0.7588658928871155,
      "learning_rate": 3.886168910648715e-05,
      "loss": 0.7113,
      "step": 30970
    },
    {
      "epoch": 1.2232005369763495,
      "grad_norm": 0.6042113304138184,
      "learning_rate": 3.88419473289375e-05,
      "loss": 0.7055,
      "step": 30980
    },
    {
      "epoch": 1.2235953725273423,
      "grad_norm": 0.6905024647712708,
      "learning_rate": 3.882220555138785e-05,
      "loss": 0.709,
      "step": 30990
    },
    {
      "epoch": 1.2239902080783354,
      "grad_norm": 1.0099061727523804,
      "learning_rate": 3.88024637738382e-05,
      "loss": 0.671,
      "step": 31000
    },
    {
      "epoch": 1.2239902080783354,
      "eval_loss": 0.7701523900032043,
      "eval_runtime": 1216.5813,
      "eval_samples_per_second": 9.253,
      "eval_steps_per_second": 9.253,
      "step": 31000
    },
    {
      "epoch": 1.2243850436293284,
      "grad_norm": 0.7458354234695435,
      "learning_rate": 3.878272199628854e-05,
      "loss": 0.7214,
      "step": 31010
    },
    {
      "epoch": 1.2247798791803213,
      "grad_norm": 0.8109974265098572,
      "learning_rate": 3.87629802187389e-05,
      "loss": 0.7155,
      "step": 31020
    },
    {
      "epoch": 1.2251747147313143,
      "grad_norm": 0.7541417479515076,
      "learning_rate": 3.874323844118924e-05,
      "loss": 0.6748,
      "step": 31030
    },
    {
      "epoch": 1.2255695502823074,
      "grad_norm": 0.5436872839927673,
      "learning_rate": 3.87234966636396e-05,
      "loss": 0.6905,
      "step": 31040
    },
    {
      "epoch": 1.2259643858333005,
      "grad_norm": 0.8970993757247925,
      "learning_rate": 3.870375488608994e-05,
      "loss": 0.7068,
      "step": 31050
    },
    {
      "epoch": 1.2263592213842935,
      "grad_norm": 0.7521165013313293,
      "learning_rate": 3.868401310854029e-05,
      "loss": 0.7536,
      "step": 31060
    },
    {
      "epoch": 1.2267540569352864,
      "grad_norm": 0.801868736743927,
      "learning_rate": 3.866427133099064e-05,
      "loss": 0.699,
      "step": 31070
    },
    {
      "epoch": 1.2271488924862795,
      "grad_norm": 0.8625969886779785,
      "learning_rate": 3.864452955344099e-05,
      "loss": 0.6983,
      "step": 31080
    },
    {
      "epoch": 1.2275437280372725,
      "grad_norm": 0.761867880821228,
      "learning_rate": 3.862478777589134e-05,
      "loss": 0.6942,
      "step": 31090
    },
    {
      "epoch": 1.2279385635882654,
      "grad_norm": 0.748313844203949,
      "learning_rate": 3.860504599834169e-05,
      "loss": 0.6538,
      "step": 31100
    },
    {
      "epoch": 1.2283333991392584,
      "grad_norm": 0.700972318649292,
      "learning_rate": 3.858530422079204e-05,
      "loss": 0.6674,
      "step": 31110
    },
    {
      "epoch": 1.2287282346902515,
      "grad_norm": 0.7345759272575378,
      "learning_rate": 3.856556244324239e-05,
      "loss": 0.7131,
      "step": 31120
    },
    {
      "epoch": 1.2291230702412446,
      "grad_norm": 0.8102253675460815,
      "learning_rate": 3.854582066569274e-05,
      "loss": 0.6859,
      "step": 31130
    },
    {
      "epoch": 1.2295179057922376,
      "grad_norm": 0.6780027151107788,
      "learning_rate": 3.852607888814309e-05,
      "loss": 0.7206,
      "step": 31140
    },
    {
      "epoch": 1.2299127413432305,
      "grad_norm": 0.8247958421707153,
      "learning_rate": 3.8506337110593435e-05,
      "loss": 0.7544,
      "step": 31150
    },
    {
      "epoch": 1.2303075768942235,
      "grad_norm": 0.527827799320221,
      "learning_rate": 3.848659533304379e-05,
      "loss": 0.7189,
      "step": 31160
    },
    {
      "epoch": 1.2307024124452166,
      "grad_norm": 0.6776025891304016,
      "learning_rate": 3.8466853555494135e-05,
      "loss": 0.7542,
      "step": 31170
    },
    {
      "epoch": 1.2310972479962095,
      "grad_norm": 0.8066858053207397,
      "learning_rate": 3.844711177794449e-05,
      "loss": 0.7407,
      "step": 31180
    },
    {
      "epoch": 1.2314920835472025,
      "grad_norm": 0.602797269821167,
      "learning_rate": 3.8427370000394835e-05,
      "loss": 0.7099,
      "step": 31190
    },
    {
      "epoch": 1.2318869190981956,
      "grad_norm": 0.7253260612487793,
      "learning_rate": 3.8407628222845184e-05,
      "loss": 0.7601,
      "step": 31200
    },
    {
      "epoch": 1.2322817546491887,
      "grad_norm": 0.9842286705970764,
      "learning_rate": 3.8387886445295534e-05,
      "loss": 0.7058,
      "step": 31210
    },
    {
      "epoch": 1.2326765902001817,
      "grad_norm": 0.7577259540557861,
      "learning_rate": 3.8368144667745884e-05,
      "loss": 0.7244,
      "step": 31220
    },
    {
      "epoch": 1.2330714257511746,
      "grad_norm": 0.7409034371376038,
      "learning_rate": 3.8348402890196234e-05,
      "loss": 0.707,
      "step": 31230
    },
    {
      "epoch": 1.2334662613021676,
      "grad_norm": 0.9633641839027405,
      "learning_rate": 3.8328661112646584e-05,
      "loss": 0.6914,
      "step": 31240
    },
    {
      "epoch": 1.2338610968531607,
      "grad_norm": 0.6339063048362732,
      "learning_rate": 3.8308919335096934e-05,
      "loss": 0.7694,
      "step": 31250
    },
    {
      "epoch": 1.2342559324041538,
      "grad_norm": 0.5506696701049805,
      "learning_rate": 3.8289177557547284e-05,
      "loss": 0.6974,
      "step": 31260
    },
    {
      "epoch": 1.2346507679551466,
      "grad_norm": 0.7543303966522217,
      "learning_rate": 3.8269435779997634e-05,
      "loss": 0.7281,
      "step": 31270
    },
    {
      "epoch": 1.2350456035061397,
      "grad_norm": 0.7635960578918457,
      "learning_rate": 3.8249694002447984e-05,
      "loss": 0.6888,
      "step": 31280
    },
    {
      "epoch": 1.2354404390571327,
      "grad_norm": 0.7238757610321045,
      "learning_rate": 3.822995222489833e-05,
      "loss": 0.6919,
      "step": 31290
    },
    {
      "epoch": 1.2358352746081258,
      "grad_norm": 0.6644021272659302,
      "learning_rate": 3.8210210447348684e-05,
      "loss": 0.6807,
      "step": 31300
    },
    {
      "epoch": 1.2362301101591187,
      "grad_norm": 0.7746444940567017,
      "learning_rate": 3.819046866979903e-05,
      "loss": 0.7148,
      "step": 31310
    },
    {
      "epoch": 1.2366249457101117,
      "grad_norm": 0.7810925841331482,
      "learning_rate": 3.8170726892249383e-05,
      "loss": 0.739,
      "step": 31320
    },
    {
      "epoch": 1.2370197812611048,
      "grad_norm": 0.7720481753349304,
      "learning_rate": 3.8150985114699727e-05,
      "loss": 0.7299,
      "step": 31330
    },
    {
      "epoch": 1.2374146168120979,
      "grad_norm": 0.9665821194648743,
      "learning_rate": 3.813124333715008e-05,
      "loss": 0.6998,
      "step": 31340
    },
    {
      "epoch": 1.2378094523630907,
      "grad_norm": 1.1859673261642456,
      "learning_rate": 3.8111501559600426e-05,
      "loss": 0.7135,
      "step": 31350
    },
    {
      "epoch": 1.2382042879140838,
      "grad_norm": 0.8182046413421631,
      "learning_rate": 3.8091759782050776e-05,
      "loss": 0.7158,
      "step": 31360
    },
    {
      "epoch": 1.2385991234650768,
      "grad_norm": 0.8416717052459717,
      "learning_rate": 3.8072018004501126e-05,
      "loss": 0.698,
      "step": 31370
    },
    {
      "epoch": 1.23899395901607,
      "grad_norm": 0.6431536674499512,
      "learning_rate": 3.8052276226951476e-05,
      "loss": 0.686,
      "step": 31380
    },
    {
      "epoch": 1.2393887945670627,
      "grad_norm": 0.8542884588241577,
      "learning_rate": 3.8032534449401826e-05,
      "loss": 0.727,
      "step": 31390
    },
    {
      "epoch": 1.2397836301180558,
      "grad_norm": 0.6871069669723511,
      "learning_rate": 3.8012792671852176e-05,
      "loss": 0.703,
      "step": 31400
    },
    {
      "epoch": 1.2401784656690489,
      "grad_norm": 0.7087810635566711,
      "learning_rate": 3.7993050894302526e-05,
      "loss": 0.697,
      "step": 31410
    },
    {
      "epoch": 1.240573301220042,
      "grad_norm": 0.9791362881660461,
      "learning_rate": 3.7973309116752876e-05,
      "loss": 0.7591,
      "step": 31420
    },
    {
      "epoch": 1.2409681367710348,
      "grad_norm": 0.7315360307693481,
      "learning_rate": 3.795356733920322e-05,
      "loss": 0.7033,
      "step": 31430
    },
    {
      "epoch": 1.2413629723220279,
      "grad_norm": 0.8017212152481079,
      "learning_rate": 3.7933825561653576e-05,
      "loss": 0.7083,
      "step": 31440
    },
    {
      "epoch": 1.241757807873021,
      "grad_norm": 0.9649490714073181,
      "learning_rate": 3.791408378410392e-05,
      "loss": 0.7526,
      "step": 31450
    },
    {
      "epoch": 1.242152643424014,
      "grad_norm": 0.7211592793464661,
      "learning_rate": 3.7894342006554275e-05,
      "loss": 0.7087,
      "step": 31460
    },
    {
      "epoch": 1.2425474789750068,
      "grad_norm": 0.7056304812431335,
      "learning_rate": 3.787460022900462e-05,
      "loss": 0.718,
      "step": 31470
    },
    {
      "epoch": 1.242942314526,
      "grad_norm": 0.803557276725769,
      "learning_rate": 3.7854858451454975e-05,
      "loss": 0.7264,
      "step": 31480
    },
    {
      "epoch": 1.243337150076993,
      "grad_norm": 0.829719603061676,
      "learning_rate": 3.783511667390532e-05,
      "loss": 0.7307,
      "step": 31490
    },
    {
      "epoch": 1.243731985627986,
      "grad_norm": 0.8902660012245178,
      "learning_rate": 3.781537489635567e-05,
      "loss": 0.7266,
      "step": 31500
    },
    {
      "epoch": 1.2441268211789789,
      "grad_norm": 0.6717310547828674,
      "learning_rate": 3.779563311880602e-05,
      "loss": 0.7182,
      "step": 31510
    },
    {
      "epoch": 1.244521656729972,
      "grad_norm": 0.86384117603302,
      "learning_rate": 3.777589134125637e-05,
      "loss": 0.7534,
      "step": 31520
    },
    {
      "epoch": 1.244916492280965,
      "grad_norm": 0.8698650598526001,
      "learning_rate": 3.775614956370672e-05,
      "loss": 0.6763,
      "step": 31530
    },
    {
      "epoch": 1.245311327831958,
      "grad_norm": 0.8300707936286926,
      "learning_rate": 3.773640778615707e-05,
      "loss": 0.6771,
      "step": 31540
    },
    {
      "epoch": 1.245706163382951,
      "grad_norm": 0.7046005725860596,
      "learning_rate": 3.771666600860742e-05,
      "loss": 0.7056,
      "step": 31550
    },
    {
      "epoch": 1.246100998933944,
      "grad_norm": 0.768756628036499,
      "learning_rate": 3.769692423105777e-05,
      "loss": 0.7766,
      "step": 31560
    },
    {
      "epoch": 1.246495834484937,
      "grad_norm": 0.6308900713920593,
      "learning_rate": 3.767718245350811e-05,
      "loss": 0.6831,
      "step": 31570
    },
    {
      "epoch": 1.2468906700359301,
      "grad_norm": 0.702415406703949,
      "learning_rate": 3.765744067595847e-05,
      "loss": 0.7238,
      "step": 31580
    },
    {
      "epoch": 1.247285505586923,
      "grad_norm": 0.8524916768074036,
      "learning_rate": 3.763769889840881e-05,
      "loss": 0.7003,
      "step": 31590
    },
    {
      "epoch": 1.247680341137916,
      "grad_norm": 0.657834529876709,
      "learning_rate": 3.761795712085917e-05,
      "loss": 0.6938,
      "step": 31600
    },
    {
      "epoch": 1.248075176688909,
      "grad_norm": 0.7381209135055542,
      "learning_rate": 3.759821534330951e-05,
      "loss": 0.6812,
      "step": 31610
    },
    {
      "epoch": 1.2484700122399022,
      "grad_norm": 0.7872114181518555,
      "learning_rate": 3.757847356575987e-05,
      "loss": 0.6683,
      "step": 31620
    },
    {
      "epoch": 1.248864847790895,
      "grad_norm": 0.7828728556632996,
      "learning_rate": 3.755873178821021e-05,
      "loss": 0.7095,
      "step": 31630
    },
    {
      "epoch": 1.249259683341888,
      "grad_norm": 0.7492111921310425,
      "learning_rate": 3.753899001066056e-05,
      "loss": 0.7485,
      "step": 31640
    },
    {
      "epoch": 1.2496545188928811,
      "grad_norm": 0.7554277777671814,
      "learning_rate": 3.751924823311091e-05,
      "loss": 0.7482,
      "step": 31650
    },
    {
      "epoch": 1.250049354443874,
      "grad_norm": 0.762712299823761,
      "learning_rate": 3.749950645556126e-05,
      "loss": 0.7146,
      "step": 31660
    },
    {
      "epoch": 1.250444189994867,
      "grad_norm": 0.6274010539054871,
      "learning_rate": 3.747976467801161e-05,
      "loss": 0.6686,
      "step": 31670
    },
    {
      "epoch": 1.2508390255458601,
      "grad_norm": 0.939217746257782,
      "learning_rate": 3.746002290046196e-05,
      "loss": 0.7037,
      "step": 31680
    },
    {
      "epoch": 1.2512338610968532,
      "grad_norm": 1.0525709390640259,
      "learning_rate": 3.744028112291231e-05,
      "loss": 0.7525,
      "step": 31690
    },
    {
      "epoch": 1.2516286966478463,
      "grad_norm": 0.7213260531425476,
      "learning_rate": 3.742053934536266e-05,
      "loss": 0.7457,
      "step": 31700
    },
    {
      "epoch": 1.252023532198839,
      "grad_norm": 0.7577037215232849,
      "learning_rate": 3.7400797567813e-05,
      "loss": 0.7069,
      "step": 31710
    },
    {
      "epoch": 1.2524183677498322,
      "grad_norm": 0.6807892322540283,
      "learning_rate": 3.738105579026336e-05,
      "loss": 0.6857,
      "step": 31720
    },
    {
      "epoch": 1.2528132033008252,
      "grad_norm": 0.7231897711753845,
      "learning_rate": 3.73613140127137e-05,
      "loss": 0.7103,
      "step": 31730
    },
    {
      "epoch": 1.253208038851818,
      "grad_norm": 0.9677861332893372,
      "learning_rate": 3.734157223516406e-05,
      "loss": 0.723,
      "step": 31740
    },
    {
      "epoch": 1.2536028744028112,
      "grad_norm": 0.9079488515853882,
      "learning_rate": 3.73218304576144e-05,
      "loss": 0.7316,
      "step": 31750
    },
    {
      "epoch": 1.2539977099538042,
      "grad_norm": 0.6927598714828491,
      "learning_rate": 3.730208868006476e-05,
      "loss": 0.7026,
      "step": 31760
    },
    {
      "epoch": 1.2543925455047973,
      "grad_norm": 0.6872807741165161,
      "learning_rate": 3.72823469025151e-05,
      "loss": 0.6942,
      "step": 31770
    },
    {
      "epoch": 1.2547873810557904,
      "grad_norm": 0.8550640940666199,
      "learning_rate": 3.726260512496545e-05,
      "loss": 0.7046,
      "step": 31780
    },
    {
      "epoch": 1.2551822166067832,
      "grad_norm": 0.6229859590530396,
      "learning_rate": 3.72428633474158e-05,
      "loss": 0.724,
      "step": 31790
    },
    {
      "epoch": 1.2555770521577763,
      "grad_norm": 0.6647154688835144,
      "learning_rate": 3.722312156986615e-05,
      "loss": 0.6998,
      "step": 31800
    },
    {
      "epoch": 1.2559718877087693,
      "grad_norm": 0.7307873368263245,
      "learning_rate": 3.72033797923165e-05,
      "loss": 0.6909,
      "step": 31810
    },
    {
      "epoch": 1.2563667232597622,
      "grad_norm": 0.6660546660423279,
      "learning_rate": 3.718363801476685e-05,
      "loss": 0.7776,
      "step": 31820
    },
    {
      "epoch": 1.2567615588107552,
      "grad_norm": 0.8374914526939392,
      "learning_rate": 3.71638962372172e-05,
      "loss": 0.7258,
      "step": 31830
    },
    {
      "epoch": 1.2571563943617483,
      "grad_norm": 0.7636149525642395,
      "learning_rate": 3.714415445966755e-05,
      "loss": 0.7433,
      "step": 31840
    },
    {
      "epoch": 1.2575512299127414,
      "grad_norm": 0.44837743043899536,
      "learning_rate": 3.7124412682117895e-05,
      "loss": 0.6873,
      "step": 31850
    },
    {
      "epoch": 1.2579460654637344,
      "grad_norm": 0.6727634072303772,
      "learning_rate": 3.710467090456825e-05,
      "loss": 0.7143,
      "step": 31860
    },
    {
      "epoch": 1.2583409010147273,
      "grad_norm": 0.684034526348114,
      "learning_rate": 3.7084929127018595e-05,
      "loss": 0.7449,
      "step": 31870
    },
    {
      "epoch": 1.2587357365657204,
      "grad_norm": 0.8081958293914795,
      "learning_rate": 3.706518734946895e-05,
      "loss": 0.7348,
      "step": 31880
    },
    {
      "epoch": 1.2591305721167134,
      "grad_norm": 0.6700004935264587,
      "learning_rate": 3.7045445571919295e-05,
      "loss": 0.7291,
      "step": 31890
    },
    {
      "epoch": 1.2595254076677063,
      "grad_norm": 0.9024060964584351,
      "learning_rate": 3.702570379436965e-05,
      "loss": 0.6941,
      "step": 31900
    },
    {
      "epoch": 1.2599202432186993,
      "grad_norm": 0.7618961334228516,
      "learning_rate": 3.7005962016819994e-05,
      "loss": 0.6902,
      "step": 31910
    },
    {
      "epoch": 1.2603150787696924,
      "grad_norm": 0.5903547406196594,
      "learning_rate": 3.6986220239270344e-05,
      "loss": 0.7324,
      "step": 31920
    },
    {
      "epoch": 1.2607099143206855,
      "grad_norm": 0.6803640127182007,
      "learning_rate": 3.6966478461720694e-05,
      "loss": 0.7499,
      "step": 31930
    },
    {
      "epoch": 1.2611047498716785,
      "grad_norm": 0.8201068639755249,
      "learning_rate": 3.6946736684171044e-05,
      "loss": 0.7185,
      "step": 31940
    },
    {
      "epoch": 1.2614995854226714,
      "grad_norm": 0.7601255774497986,
      "learning_rate": 3.6926994906621394e-05,
      "loss": 0.7499,
      "step": 31950
    },
    {
      "epoch": 1.2618944209736644,
      "grad_norm": 0.7696624994277954,
      "learning_rate": 3.6907253129071744e-05,
      "loss": 0.7087,
      "step": 31960
    },
    {
      "epoch": 1.2622892565246575,
      "grad_norm": 0.7422149777412415,
      "learning_rate": 3.6887511351522094e-05,
      "loss": 0.7174,
      "step": 31970
    },
    {
      "epoch": 1.2626840920756504,
      "grad_norm": 0.7047266364097595,
      "learning_rate": 3.6867769573972444e-05,
      "loss": 0.7014,
      "step": 31980
    },
    {
      "epoch": 1.2630789276266434,
      "grad_norm": 0.7078582048416138,
      "learning_rate": 3.6848027796422794e-05,
      "loss": 0.7181,
      "step": 31990
    },
    {
      "epoch": 1.2634737631776365,
      "grad_norm": 0.7457965016365051,
      "learning_rate": 3.6828286018873144e-05,
      "loss": 0.708,
      "step": 32000
    },
    {
      "epoch": 1.2634737631776365,
      "eval_loss": 0.768667995929718,
      "eval_runtime": 1219.937,
      "eval_samples_per_second": 9.228,
      "eval_steps_per_second": 9.228,
      "step": 32000
    },
    {
      "epoch": 1.2638685987286296,
      "grad_norm": 1.070136308670044,
      "learning_rate": 3.680854424132349e-05,
      "loss": 0.7582,
      "step": 32010
    },
    {
      "epoch": 1.2642634342796226,
      "grad_norm": 0.7707621455192566,
      "learning_rate": 3.6788802463773844e-05,
      "loss": 0.7683,
      "step": 32020
    },
    {
      "epoch": 1.2646582698306155,
      "grad_norm": 0.7445360422134399,
      "learning_rate": 3.676906068622419e-05,
      "loss": 0.7275,
      "step": 32030
    },
    {
      "epoch": 1.2650531053816085,
      "grad_norm": 0.91856449842453,
      "learning_rate": 3.674931890867454e-05,
      "loss": 0.693,
      "step": 32040
    },
    {
      "epoch": 1.2654479409326016,
      "grad_norm": 0.7148177623748779,
      "learning_rate": 3.6729577131124887e-05,
      "loss": 0.7049,
      "step": 32050
    },
    {
      "epoch": 1.2658427764835944,
      "grad_norm": 0.90745609998703,
      "learning_rate": 3.6709835353575236e-05,
      "loss": 0.7099,
      "step": 32060
    },
    {
      "epoch": 1.2662376120345875,
      "grad_norm": 0.7411761283874512,
      "learning_rate": 3.6690093576025586e-05,
      "loss": 0.7258,
      "step": 32070
    },
    {
      "epoch": 1.2666324475855806,
      "grad_norm": 0.7585841417312622,
      "learning_rate": 3.6670351798475936e-05,
      "loss": 0.7333,
      "step": 32080
    },
    {
      "epoch": 1.2670272831365736,
      "grad_norm": 0.9372621178627014,
      "learning_rate": 3.6650610020926286e-05,
      "loss": 0.7137,
      "step": 32090
    },
    {
      "epoch": 1.2674221186875667,
      "grad_norm": 0.6856043338775635,
      "learning_rate": 3.6630868243376636e-05,
      "loss": 0.6897,
      "step": 32100
    },
    {
      "epoch": 1.2678169542385596,
      "grad_norm": 0.7660462260246277,
      "learning_rate": 3.6611126465826986e-05,
      "loss": 0.7495,
      "step": 32110
    },
    {
      "epoch": 1.2682117897895526,
      "grad_norm": 0.7871591448783875,
      "learning_rate": 3.6591384688277336e-05,
      "loss": 0.778,
      "step": 32120
    },
    {
      "epoch": 1.2686066253405457,
      "grad_norm": 1.0285489559173584,
      "learning_rate": 3.6571642910727686e-05,
      "loss": 0.7478,
      "step": 32130
    },
    {
      "epoch": 1.2690014608915388,
      "grad_norm": 1.0295032262802124,
      "learning_rate": 3.6551901133178036e-05,
      "loss": 0.7235,
      "step": 32140
    },
    {
      "epoch": 1.2693962964425318,
      "grad_norm": 0.8076234459877014,
      "learning_rate": 3.653215935562838e-05,
      "loss": 0.7103,
      "step": 32150
    },
    {
      "epoch": 1.2697911319935247,
      "grad_norm": 0.8421769142150879,
      "learning_rate": 3.6512417578078736e-05,
      "loss": 0.7102,
      "step": 32160
    },
    {
      "epoch": 1.2701859675445177,
      "grad_norm": 0.7916907668113708,
      "learning_rate": 3.649267580052908e-05,
      "loss": 0.7017,
      "step": 32170
    },
    {
      "epoch": 1.2705808030955108,
      "grad_norm": 1.424241304397583,
      "learning_rate": 3.6472934022979435e-05,
      "loss": 0.747,
      "step": 32180
    },
    {
      "epoch": 1.2709756386465036,
      "grad_norm": 0.8049941062927246,
      "learning_rate": 3.645319224542978e-05,
      "loss": 0.7562,
      "step": 32190
    },
    {
      "epoch": 1.2713704741974967,
      "grad_norm": 0.7464780211448669,
      "learning_rate": 3.643345046788013e-05,
      "loss": 0.713,
      "step": 32200
    },
    {
      "epoch": 1.2717653097484898,
      "grad_norm": 0.8331634402275085,
      "learning_rate": 3.641370869033048e-05,
      "loss": 0.7188,
      "step": 32210
    },
    {
      "epoch": 1.2721601452994828,
      "grad_norm": 0.8364920020103455,
      "learning_rate": 3.639396691278083e-05,
      "loss": 0.7309,
      "step": 32220
    },
    {
      "epoch": 1.272554980850476,
      "grad_norm": 0.8120725154876709,
      "learning_rate": 3.637422513523118e-05,
      "loss": 0.6901,
      "step": 32230
    },
    {
      "epoch": 1.2729498164014688,
      "grad_norm": 0.7384560108184814,
      "learning_rate": 3.635448335768153e-05,
      "loss": 0.7214,
      "step": 32240
    },
    {
      "epoch": 1.2733446519524618,
      "grad_norm": 0.8072701096534729,
      "learning_rate": 3.633474158013188e-05,
      "loss": 0.6871,
      "step": 32250
    },
    {
      "epoch": 1.273739487503455,
      "grad_norm": 0.6902682185173035,
      "learning_rate": 3.631499980258223e-05,
      "loss": 0.7024,
      "step": 32260
    },
    {
      "epoch": 1.2741343230544477,
      "grad_norm": 0.7890617251396179,
      "learning_rate": 3.629525802503258e-05,
      "loss": 0.7205,
      "step": 32270
    },
    {
      "epoch": 1.2745291586054408,
      "grad_norm": 0.7995176315307617,
      "learning_rate": 3.627551624748293e-05,
      "loss": 0.7177,
      "step": 32280
    },
    {
      "epoch": 1.2749239941564339,
      "grad_norm": 0.7958890199661255,
      "learning_rate": 3.625577446993327e-05,
      "loss": 0.7319,
      "step": 32290
    },
    {
      "epoch": 1.275318829707427,
      "grad_norm": 0.9581893682479858,
      "learning_rate": 3.623603269238363e-05,
      "loss": 0.7248,
      "step": 32300
    },
    {
      "epoch": 1.27571366525842,
      "grad_norm": 0.6874106526374817,
      "learning_rate": 3.621629091483397e-05,
      "loss": 0.758,
      "step": 32310
    },
    {
      "epoch": 1.2761085008094128,
      "grad_norm": 0.7820799946784973,
      "learning_rate": 3.619654913728433e-05,
      "loss": 0.7032,
      "step": 32320
    },
    {
      "epoch": 1.276503336360406,
      "grad_norm": 1.0311609506607056,
      "learning_rate": 3.617680735973467e-05,
      "loss": 0.739,
      "step": 32330
    },
    {
      "epoch": 1.276898171911399,
      "grad_norm": 0.847846269607544,
      "learning_rate": 3.615706558218502e-05,
      "loss": 0.7365,
      "step": 32340
    },
    {
      "epoch": 1.2772930074623918,
      "grad_norm": 0.7890461683273315,
      "learning_rate": 3.613732380463537e-05,
      "loss": 0.678,
      "step": 32350
    },
    {
      "epoch": 1.277687843013385,
      "grad_norm": 0.962379515171051,
      "learning_rate": 3.611758202708572e-05,
      "loss": 0.6801,
      "step": 32360
    },
    {
      "epoch": 1.278082678564378,
      "grad_norm": 0.803582489490509,
      "learning_rate": 3.609784024953607e-05,
      "loss": 0.6826,
      "step": 32370
    },
    {
      "epoch": 1.278477514115371,
      "grad_norm": 0.7152155637741089,
      "learning_rate": 3.607809847198642e-05,
      "loss": 0.7287,
      "step": 32380
    },
    {
      "epoch": 1.278872349666364,
      "grad_norm": 0.7965822815895081,
      "learning_rate": 3.605835669443677e-05,
      "loss": 0.7302,
      "step": 32390
    },
    {
      "epoch": 1.279267185217357,
      "grad_norm": 0.6605339050292969,
      "learning_rate": 3.603861491688712e-05,
      "loss": 0.7011,
      "step": 32400
    },
    {
      "epoch": 1.27966202076835,
      "grad_norm": 1.168077826499939,
      "learning_rate": 3.601887313933747e-05,
      "loss": 0.721,
      "step": 32410
    },
    {
      "epoch": 1.280056856319343,
      "grad_norm": 0.8150404095649719,
      "learning_rate": 3.599913136178782e-05,
      "loss": 0.7098,
      "step": 32420
    },
    {
      "epoch": 1.280451691870336,
      "grad_norm": 0.8029215335845947,
      "learning_rate": 3.597938958423816e-05,
      "loss": 0.7294,
      "step": 32430
    },
    {
      "epoch": 1.280846527421329,
      "grad_norm": 0.9422559142112732,
      "learning_rate": 3.595964780668852e-05,
      "loss": 0.7568,
      "step": 32440
    },
    {
      "epoch": 1.281241362972322,
      "grad_norm": 0.9618743062019348,
      "learning_rate": 3.593990602913886e-05,
      "loss": 0.6842,
      "step": 32450
    },
    {
      "epoch": 1.2816361985233151,
      "grad_norm": 0.8445326685905457,
      "learning_rate": 3.592016425158922e-05,
      "loss": 0.7047,
      "step": 32460
    },
    {
      "epoch": 1.2820310340743082,
      "grad_norm": 0.7594175934791565,
      "learning_rate": 3.590042247403956e-05,
      "loss": 0.7032,
      "step": 32470
    },
    {
      "epoch": 1.282425869625301,
      "grad_norm": 0.6419410705566406,
      "learning_rate": 3.588068069648991e-05,
      "loss": 0.6623,
      "step": 32480
    },
    {
      "epoch": 1.282820705176294,
      "grad_norm": 0.7393196821212769,
      "learning_rate": 3.586093891894026e-05,
      "loss": 0.7481,
      "step": 32490
    },
    {
      "epoch": 1.2832155407272872,
      "grad_norm": 0.8259454965591431,
      "learning_rate": 3.584119714139061e-05,
      "loss": 0.7402,
      "step": 32500
    },
    {
      "epoch": 1.28361037627828,
      "grad_norm": 0.8312186002731323,
      "learning_rate": 3.582145536384096e-05,
      "loss": 0.7299,
      "step": 32510
    },
    {
      "epoch": 1.284005211829273,
      "grad_norm": 1.1435635089874268,
      "learning_rate": 3.580171358629131e-05,
      "loss": 0.7499,
      "step": 32520
    },
    {
      "epoch": 1.2844000473802661,
      "grad_norm": 0.7587941288948059,
      "learning_rate": 3.578197180874166e-05,
      "loss": 0.6981,
      "step": 32530
    },
    {
      "epoch": 1.2847948829312592,
      "grad_norm": 0.6973093748092651,
      "learning_rate": 3.576223003119201e-05,
      "loss": 0.7067,
      "step": 32540
    },
    {
      "epoch": 1.2851897184822523,
      "grad_norm": 1.0645339488983154,
      "learning_rate": 3.574248825364236e-05,
      "loss": 0.7197,
      "step": 32550
    },
    {
      "epoch": 1.2855845540332451,
      "grad_norm": 0.731555163860321,
      "learning_rate": 3.572274647609271e-05,
      "loss": 0.7192,
      "step": 32560
    },
    {
      "epoch": 1.2859793895842382,
      "grad_norm": 0.7953199744224548,
      "learning_rate": 3.5703004698543055e-05,
      "loss": 0.6953,
      "step": 32570
    },
    {
      "epoch": 1.2863742251352313,
      "grad_norm": 0.8222201466560364,
      "learning_rate": 3.568326292099341e-05,
      "loss": 0.7666,
      "step": 32580
    },
    {
      "epoch": 1.286769060686224,
      "grad_norm": 0.6661884784698486,
      "learning_rate": 3.5663521143443755e-05,
      "loss": 0.7295,
      "step": 32590
    },
    {
      "epoch": 1.2871638962372172,
      "grad_norm": 0.6482901573181152,
      "learning_rate": 3.564377936589411e-05,
      "loss": 0.7075,
      "step": 32600
    },
    {
      "epoch": 1.2875587317882102,
      "grad_norm": 1.1125859022140503,
      "learning_rate": 3.5624037588344455e-05,
      "loss": 0.7311,
      "step": 32610
    },
    {
      "epoch": 1.2879535673392033,
      "grad_norm": 0.6282872557640076,
      "learning_rate": 3.5604295810794805e-05,
      "loss": 0.7008,
      "step": 32620
    },
    {
      "epoch": 1.2883484028901964,
      "grad_norm": 0.8209827542304993,
      "learning_rate": 3.5584554033245154e-05,
      "loss": 0.7172,
      "step": 32630
    },
    {
      "epoch": 1.2887432384411892,
      "grad_norm": 0.9208583831787109,
      "learning_rate": 3.5564812255695504e-05,
      "loss": 0.72,
      "step": 32640
    },
    {
      "epoch": 1.2891380739921823,
      "grad_norm": 0.6883429288864136,
      "learning_rate": 3.5545070478145854e-05,
      "loss": 0.663,
      "step": 32650
    },
    {
      "epoch": 1.2895329095431753,
      "grad_norm": 0.841503918170929,
      "learning_rate": 3.5525328700596204e-05,
      "loss": 0.7712,
      "step": 32660
    },
    {
      "epoch": 1.2899277450941682,
      "grad_norm": 0.7503730058670044,
      "learning_rate": 3.5505586923046554e-05,
      "loss": 0.715,
      "step": 32670
    },
    {
      "epoch": 1.2903225806451613,
      "grad_norm": 0.6525678634643555,
      "learning_rate": 3.5485845145496904e-05,
      "loss": 0.6862,
      "step": 32680
    },
    {
      "epoch": 1.2907174161961543,
      "grad_norm": 0.9243394136428833,
      "learning_rate": 3.5466103367947254e-05,
      "loss": 0.7209,
      "step": 32690
    },
    {
      "epoch": 1.2911122517471474,
      "grad_norm": 0.5125670433044434,
      "learning_rate": 3.5446361590397604e-05,
      "loss": 0.6937,
      "step": 32700
    },
    {
      "epoch": 1.2915070872981405,
      "grad_norm": 0.9100840091705322,
      "learning_rate": 3.542661981284795e-05,
      "loss": 0.7521,
      "step": 32710
    },
    {
      "epoch": 1.2919019228491333,
      "grad_norm": 0.6302988529205322,
      "learning_rate": 3.5406878035298304e-05,
      "loss": 0.7156,
      "step": 32720
    },
    {
      "epoch": 1.2922967584001264,
      "grad_norm": 0.7779831290245056,
      "learning_rate": 3.538713625774865e-05,
      "loss": 0.6986,
      "step": 32730
    },
    {
      "epoch": 1.2926915939511194,
      "grad_norm": 0.7495604157447815,
      "learning_rate": 3.5367394480199003e-05,
      "loss": 0.6936,
      "step": 32740
    },
    {
      "epoch": 1.2930864295021123,
      "grad_norm": 0.8199266195297241,
      "learning_rate": 3.5347652702649347e-05,
      "loss": 0.7612,
      "step": 32750
    },
    {
      "epoch": 1.2934812650531053,
      "grad_norm": 0.7627349495887756,
      "learning_rate": 3.5327910925099697e-05,
      "loss": 0.6681,
      "step": 32760
    },
    {
      "epoch": 1.2938761006040984,
      "grad_norm": 0.7150886058807373,
      "learning_rate": 3.5308169147550046e-05,
      "loss": 0.6698,
      "step": 32770
    },
    {
      "epoch": 1.2942709361550915,
      "grad_norm": 0.9085655808448792,
      "learning_rate": 3.5288427370000396e-05,
      "loss": 0.7645,
      "step": 32780
    },
    {
      "epoch": 1.2946657717060845,
      "grad_norm": 0.5908188223838806,
      "learning_rate": 3.5268685592450746e-05,
      "loss": 0.6813,
      "step": 32790
    },
    {
      "epoch": 1.2950606072570774,
      "grad_norm": 0.928162157535553,
      "learning_rate": 3.5248943814901096e-05,
      "loss": 0.6985,
      "step": 32800
    },
    {
      "epoch": 1.2954554428080705,
      "grad_norm": 0.8247794508934021,
      "learning_rate": 3.5229202037351446e-05,
      "loss": 0.7067,
      "step": 32810
    },
    {
      "epoch": 1.2958502783590635,
      "grad_norm": 0.6771246790885925,
      "learning_rate": 3.5209460259801796e-05,
      "loss": 0.7678,
      "step": 32820
    },
    {
      "epoch": 1.2962451139100564,
      "grad_norm": 0.7474112510681152,
      "learning_rate": 3.5189718482252146e-05,
      "loss": 0.6976,
      "step": 32830
    },
    {
      "epoch": 1.2966399494610494,
      "grad_norm": 0.8263770341873169,
      "learning_rate": 3.5169976704702496e-05,
      "loss": 0.7232,
      "step": 32840
    },
    {
      "epoch": 1.2970347850120425,
      "grad_norm": 0.7202840447425842,
      "learning_rate": 3.515023492715284e-05,
      "loss": 0.7171,
      "step": 32850
    },
    {
      "epoch": 1.2974296205630356,
      "grad_norm": 1.108296275138855,
      "learning_rate": 3.5130493149603196e-05,
      "loss": 0.7511,
      "step": 32860
    },
    {
      "epoch": 1.2978244561140286,
      "grad_norm": 0.8750802278518677,
      "learning_rate": 3.511075137205354e-05,
      "loss": 0.7051,
      "step": 32870
    },
    {
      "epoch": 1.2982192916650215,
      "grad_norm": 0.7032634019851685,
      "learning_rate": 3.5091009594503895e-05,
      "loss": 0.7504,
      "step": 32880
    },
    {
      "epoch": 1.2986141272160145,
      "grad_norm": 1.0758637189865112,
      "learning_rate": 3.507126781695424e-05,
      "loss": 0.6758,
      "step": 32890
    },
    {
      "epoch": 1.2990089627670076,
      "grad_norm": 0.7255128026008606,
      "learning_rate": 3.505152603940459e-05,
      "loss": 0.7389,
      "step": 32900
    },
    {
      "epoch": 1.2994037983180005,
      "grad_norm": 0.7080369591712952,
      "learning_rate": 3.503178426185494e-05,
      "loss": 0.7312,
      "step": 32910
    },
    {
      "epoch": 1.2997986338689935,
      "grad_norm": 0.7455881237983704,
      "learning_rate": 3.501204248430529e-05,
      "loss": 0.7164,
      "step": 32920
    },
    {
      "epoch": 1.3001934694199866,
      "grad_norm": 0.7582306861877441,
      "learning_rate": 3.499230070675564e-05,
      "loss": 0.7069,
      "step": 32930
    },
    {
      "epoch": 1.3005883049709797,
      "grad_norm": 0.8366675972938538,
      "learning_rate": 3.497255892920599e-05,
      "loss": 0.7111,
      "step": 32940
    },
    {
      "epoch": 1.3009831405219727,
      "grad_norm": 0.6602511405944824,
      "learning_rate": 3.495281715165634e-05,
      "loss": 0.6512,
      "step": 32950
    },
    {
      "epoch": 1.3013779760729656,
      "grad_norm": 0.6912769079208374,
      "learning_rate": 3.493307537410669e-05,
      "loss": 0.7234,
      "step": 32960
    },
    {
      "epoch": 1.3017728116239586,
      "grad_norm": 0.9770160913467407,
      "learning_rate": 3.491333359655704e-05,
      "loss": 0.6962,
      "step": 32970
    },
    {
      "epoch": 1.3021676471749517,
      "grad_norm": 0.8511463403701782,
      "learning_rate": 3.489359181900739e-05,
      "loss": 0.7017,
      "step": 32980
    },
    {
      "epoch": 1.3025624827259445,
      "grad_norm": 0.9110668301582336,
      "learning_rate": 3.487385004145773e-05,
      "loss": 0.7277,
      "step": 32990
    },
    {
      "epoch": 1.3029573182769376,
      "grad_norm": 0.7982496619224548,
      "learning_rate": 3.485410826390809e-05,
      "loss": 0.765,
      "step": 33000
    },
    {
      "epoch": 1.3029573182769376,
      "eval_loss": 0.7679662704467773,
      "eval_runtime": 1220.354,
      "eval_samples_per_second": 9.224,
      "eval_steps_per_second": 9.224,
      "step": 33000
    },
    {
      "epoch": 1.3033521538279307,
      "grad_norm": 0.8816893696784973,
      "learning_rate": 3.483436648635843e-05,
      "loss": 0.744,
      "step": 33010
    },
    {
      "epoch": 1.3037469893789237,
      "grad_norm": 0.6779281497001648,
      "learning_rate": 3.481462470880879e-05,
      "loss": 0.7025,
      "step": 33020
    },
    {
      "epoch": 1.3041418249299168,
      "grad_norm": 0.7010039687156677,
      "learning_rate": 3.479488293125913e-05,
      "loss": 0.7255,
      "step": 33030
    },
    {
      "epoch": 1.3045366604809097,
      "grad_norm": 0.7985633015632629,
      "learning_rate": 3.477514115370949e-05,
      "loss": 0.7427,
      "step": 33040
    },
    {
      "epoch": 1.3049314960319027,
      "grad_norm": 0.9779862761497498,
      "learning_rate": 3.475539937615983e-05,
      "loss": 0.7052,
      "step": 33050
    },
    {
      "epoch": 1.3053263315828958,
      "grad_norm": 0.8900579810142517,
      "learning_rate": 3.473565759861018e-05,
      "loss": 0.7073,
      "step": 33060
    },
    {
      "epoch": 1.3057211671338886,
      "grad_norm": 0.8801579475402832,
      "learning_rate": 3.471591582106053e-05,
      "loss": 0.7305,
      "step": 33070
    },
    {
      "epoch": 1.3061160026848817,
      "grad_norm": 0.7486023306846619,
      "learning_rate": 3.469617404351088e-05,
      "loss": 0.6894,
      "step": 33080
    },
    {
      "epoch": 1.3065108382358748,
      "grad_norm": 0.7426456212997437,
      "learning_rate": 3.467643226596123e-05,
      "loss": 0.7571,
      "step": 33090
    },
    {
      "epoch": 1.3069056737868678,
      "grad_norm": 1.026151180267334,
      "learning_rate": 3.465669048841158e-05,
      "loss": 0.7518,
      "step": 33100
    },
    {
      "epoch": 1.307300509337861,
      "grad_norm": 0.7546917200088501,
      "learning_rate": 3.463694871086193e-05,
      "loss": 0.7061,
      "step": 33110
    },
    {
      "epoch": 1.3076953448888538,
      "grad_norm": 0.724833607673645,
      "learning_rate": 3.461720693331228e-05,
      "loss": 0.6953,
      "step": 33120
    },
    {
      "epoch": 1.3080901804398468,
      "grad_norm": 0.825596034526825,
      "learning_rate": 3.459746515576262e-05,
      "loss": 0.7074,
      "step": 33130
    },
    {
      "epoch": 1.3084850159908399,
      "grad_norm": 0.7950263023376465,
      "learning_rate": 3.457772337821298e-05,
      "loss": 0.7034,
      "step": 33140
    },
    {
      "epoch": 1.3088798515418327,
      "grad_norm": 0.9574018716812134,
      "learning_rate": 3.455798160066332e-05,
      "loss": 0.7697,
      "step": 33150
    },
    {
      "epoch": 1.3092746870928258,
      "grad_norm": 0.7364956736564636,
      "learning_rate": 3.453823982311368e-05,
      "loss": 0.7225,
      "step": 33160
    },
    {
      "epoch": 1.3096695226438189,
      "grad_norm": 0.7780880331993103,
      "learning_rate": 3.451849804556402e-05,
      "loss": 0.6578,
      "step": 33170
    },
    {
      "epoch": 1.310064358194812,
      "grad_norm": 0.7681106925010681,
      "learning_rate": 3.449875626801438e-05,
      "loss": 0.6752,
      "step": 33180
    },
    {
      "epoch": 1.310459193745805,
      "grad_norm": 0.8748906850814819,
      "learning_rate": 3.447901449046472e-05,
      "loss": 0.7248,
      "step": 33190
    },
    {
      "epoch": 1.3108540292967978,
      "grad_norm": 0.6303648948669434,
      "learning_rate": 3.445927271291507e-05,
      "loss": 0.7192,
      "step": 33200
    },
    {
      "epoch": 1.311248864847791,
      "grad_norm": 0.6471295952796936,
      "learning_rate": 3.443953093536542e-05,
      "loss": 0.6989,
      "step": 33210
    },
    {
      "epoch": 1.311643700398784,
      "grad_norm": 0.8356936573982239,
      "learning_rate": 3.441978915781577e-05,
      "loss": 0.7158,
      "step": 33220
    },
    {
      "epoch": 1.3120385359497768,
      "grad_norm": 0.8511989116668701,
      "learning_rate": 3.440004738026612e-05,
      "loss": 0.6663,
      "step": 33230
    },
    {
      "epoch": 1.3124333715007699,
      "grad_norm": 0.706761360168457,
      "learning_rate": 3.438030560271647e-05,
      "loss": 0.679,
      "step": 33240
    },
    {
      "epoch": 1.312828207051763,
      "grad_norm": 0.8012385964393616,
      "learning_rate": 3.436056382516682e-05,
      "loss": 0.7274,
      "step": 33250
    },
    {
      "epoch": 1.313223042602756,
      "grad_norm": 0.7530196905136108,
      "learning_rate": 3.434082204761717e-05,
      "loss": 0.6885,
      "step": 33260
    },
    {
      "epoch": 1.313617878153749,
      "grad_norm": 0.6912813782691956,
      "learning_rate": 3.4321080270067515e-05,
      "loss": 0.743,
      "step": 33270
    },
    {
      "epoch": 1.314012713704742,
      "grad_norm": 0.7920769453048706,
      "learning_rate": 3.430133849251787e-05,
      "loss": 0.6995,
      "step": 33280
    },
    {
      "epoch": 1.314407549255735,
      "grad_norm": 0.7529811263084412,
      "learning_rate": 3.4281596714968215e-05,
      "loss": 0.7517,
      "step": 33290
    },
    {
      "epoch": 1.314802384806728,
      "grad_norm": 0.6857923269271851,
      "learning_rate": 3.426185493741857e-05,
      "loss": 0.704,
      "step": 33300
    },
    {
      "epoch": 1.315197220357721,
      "grad_norm": 0.6994165182113647,
      "learning_rate": 3.4242113159868915e-05,
      "loss": 0.7276,
      "step": 33310
    },
    {
      "epoch": 1.315592055908714,
      "grad_norm": 0.7747858166694641,
      "learning_rate": 3.422237138231927e-05,
      "loss": 0.6873,
      "step": 33320
    },
    {
      "epoch": 1.315986891459707,
      "grad_norm": 0.7095138430595398,
      "learning_rate": 3.4202629604769615e-05,
      "loss": 0.7104,
      "step": 33330
    },
    {
      "epoch": 1.3163817270107,
      "grad_norm": 0.6347941756248474,
      "learning_rate": 3.4182887827219964e-05,
      "loss": 0.7077,
      "step": 33340
    },
    {
      "epoch": 1.3167765625616932,
      "grad_norm": 0.6984022259712219,
      "learning_rate": 3.4163146049670314e-05,
      "loss": 0.7003,
      "step": 33350
    },
    {
      "epoch": 1.317171398112686,
      "grad_norm": 0.8997972011566162,
      "learning_rate": 3.4143404272120664e-05,
      "loss": 0.7311,
      "step": 33360
    },
    {
      "epoch": 1.317566233663679,
      "grad_norm": 0.8436357378959656,
      "learning_rate": 3.4123662494571014e-05,
      "loss": 0.6738,
      "step": 33370
    },
    {
      "epoch": 1.3179610692146722,
      "grad_norm": 0.8936707973480225,
      "learning_rate": 3.4103920717021364e-05,
      "loss": 0.7303,
      "step": 33380
    },
    {
      "epoch": 1.318355904765665,
      "grad_norm": 0.8070521354675293,
      "learning_rate": 3.4084178939471714e-05,
      "loss": 0.713,
      "step": 33390
    },
    {
      "epoch": 1.318750740316658,
      "grad_norm": 0.6980406045913696,
      "learning_rate": 3.4064437161922064e-05,
      "loss": 0.6717,
      "step": 33400
    },
    {
      "epoch": 1.3191455758676511,
      "grad_norm": 0.741555392742157,
      "learning_rate": 3.404469538437241e-05,
      "loss": 0.7183,
      "step": 33410
    },
    {
      "epoch": 1.3195404114186442,
      "grad_norm": 0.7329072952270508,
      "learning_rate": 3.4024953606822764e-05,
      "loss": 0.7555,
      "step": 33420
    },
    {
      "epoch": 1.3199352469696373,
      "grad_norm": 0.6048296689987183,
      "learning_rate": 3.400521182927311e-05,
      "loss": 0.7328,
      "step": 33430
    },
    {
      "epoch": 1.32033008252063,
      "grad_norm": 0.7697684168815613,
      "learning_rate": 3.3985470051723464e-05,
      "loss": 0.6886,
      "step": 33440
    },
    {
      "epoch": 1.3207249180716232,
      "grad_norm": 0.8304633498191833,
      "learning_rate": 3.396572827417381e-05,
      "loss": 0.7152,
      "step": 33450
    },
    {
      "epoch": 1.3211197536226162,
      "grad_norm": 1.1643084287643433,
      "learning_rate": 3.394598649662416e-05,
      "loss": 0.7169,
      "step": 33460
    },
    {
      "epoch": 1.321514589173609,
      "grad_norm": 0.6196455359458923,
      "learning_rate": 3.3926244719074507e-05,
      "loss": 0.6737,
      "step": 33470
    },
    {
      "epoch": 1.3219094247246022,
      "grad_norm": 0.779089629650116,
      "learning_rate": 3.3906502941524856e-05,
      "loss": 0.7012,
      "step": 33480
    },
    {
      "epoch": 1.3223042602755952,
      "grad_norm": 0.7933708429336548,
      "learning_rate": 3.3886761163975206e-05,
      "loss": 0.7351,
      "step": 33490
    },
    {
      "epoch": 1.3226990958265883,
      "grad_norm": 0.6711064577102661,
      "learning_rate": 3.3867019386425556e-05,
      "loss": 0.7181,
      "step": 33500
    },
    {
      "epoch": 1.3230939313775814,
      "grad_norm": 0.942064642906189,
      "learning_rate": 3.3847277608875906e-05,
      "loss": 0.7676,
      "step": 33510
    },
    {
      "epoch": 1.3234887669285742,
      "grad_norm": 0.9102448225021362,
      "learning_rate": 3.3827535831326256e-05,
      "loss": 0.7049,
      "step": 33520
    },
    {
      "epoch": 1.3238836024795673,
      "grad_norm": 0.7304357886314392,
      "learning_rate": 3.3807794053776606e-05,
      "loss": 0.6758,
      "step": 33530
    },
    {
      "epoch": 1.3242784380305603,
      "grad_norm": 0.6399295330047607,
      "learning_rate": 3.3788052276226956e-05,
      "loss": 0.7005,
      "step": 33540
    },
    {
      "epoch": 1.3246732735815532,
      "grad_norm": 0.6808239221572876,
      "learning_rate": 3.3768310498677306e-05,
      "loss": 0.7332,
      "step": 33550
    },
    {
      "epoch": 1.3250681091325462,
      "grad_norm": 0.7464627623558044,
      "learning_rate": 3.3748568721127656e-05,
      "loss": 0.6806,
      "step": 33560
    },
    {
      "epoch": 1.3254629446835393,
      "grad_norm": 0.6648033261299133,
      "learning_rate": 3.3728826943578e-05,
      "loss": 0.7399,
      "step": 33570
    },
    {
      "epoch": 1.3258577802345324,
      "grad_norm": 1.5524049997329712,
      "learning_rate": 3.3709085166028356e-05,
      "loss": 0.7826,
      "step": 33580
    },
    {
      "epoch": 1.3262526157855254,
      "grad_norm": 0.8402047157287598,
      "learning_rate": 3.36893433884787e-05,
      "loss": 0.7149,
      "step": 33590
    },
    {
      "epoch": 1.3266474513365183,
      "grad_norm": 0.8149782419204712,
      "learning_rate": 3.3669601610929055e-05,
      "loss": 0.7114,
      "step": 33600
    },
    {
      "epoch": 1.3270422868875114,
      "grad_norm": 0.6694438457489014,
      "learning_rate": 3.36498598333794e-05,
      "loss": 0.6751,
      "step": 33610
    },
    {
      "epoch": 1.3274371224385044,
      "grad_norm": 0.6120983362197876,
      "learning_rate": 3.363011805582975e-05,
      "loss": 0.7486,
      "step": 33620
    },
    {
      "epoch": 1.3278319579894973,
      "grad_norm": 0.6834012866020203,
      "learning_rate": 3.36103762782801e-05,
      "loss": 0.7084,
      "step": 33630
    },
    {
      "epoch": 1.3282267935404903,
      "grad_norm": 0.7819698452949524,
      "learning_rate": 3.359063450073045e-05,
      "loss": 0.7682,
      "step": 33640
    },
    {
      "epoch": 1.3286216290914834,
      "grad_norm": 0.6753125786781311,
      "learning_rate": 3.35708927231808e-05,
      "loss": 0.6971,
      "step": 33650
    },
    {
      "epoch": 1.3290164646424765,
      "grad_norm": 0.6112338900566101,
      "learning_rate": 3.355115094563114e-05,
      "loss": 0.6771,
      "step": 33660
    },
    {
      "epoch": 1.3294113001934695,
      "grad_norm": 0.7464151382446289,
      "learning_rate": 3.35314091680815e-05,
      "loss": 0.7066,
      "step": 33670
    },
    {
      "epoch": 1.3298061357444624,
      "grad_norm": 0.8664688467979431,
      "learning_rate": 3.351166739053184e-05,
      "loss": 0.7419,
      "step": 33680
    },
    {
      "epoch": 1.3302009712954554,
      "grad_norm": 0.6945672631263733,
      "learning_rate": 3.34919256129822e-05,
      "loss": 0.7162,
      "step": 33690
    },
    {
      "epoch": 1.3305958068464485,
      "grad_norm": 0.7545846104621887,
      "learning_rate": 3.347218383543254e-05,
      "loss": 0.6812,
      "step": 33700
    },
    {
      "epoch": 1.3309906423974414,
      "grad_norm": 0.5797557234764099,
      "learning_rate": 3.345244205788289e-05,
      "loss": 0.649,
      "step": 33710
    },
    {
      "epoch": 1.3313854779484344,
      "grad_norm": 0.7248045206069946,
      "learning_rate": 3.343270028033324e-05,
      "loss": 0.7586,
      "step": 33720
    },
    {
      "epoch": 1.3317803134994275,
      "grad_norm": 0.9332888722419739,
      "learning_rate": 3.341295850278359e-05,
      "loss": 0.6961,
      "step": 33730
    },
    {
      "epoch": 1.3321751490504206,
      "grad_norm": 0.9513741731643677,
      "learning_rate": 3.339321672523394e-05,
      "loss": 0.7279,
      "step": 33740
    },
    {
      "epoch": 1.3325699846014136,
      "grad_norm": 0.8248876929283142,
      "learning_rate": 3.337347494768429e-05,
      "loss": 0.7072,
      "step": 33750
    },
    {
      "epoch": 1.3329648201524065,
      "grad_norm": 0.6968623399734497,
      "learning_rate": 3.335373317013464e-05,
      "loss": 0.7165,
      "step": 33760
    },
    {
      "epoch": 1.3333596557033995,
      "grad_norm": 0.920339047908783,
      "learning_rate": 3.333399139258499e-05,
      "loss": 0.7617,
      "step": 33770
    },
    {
      "epoch": 1.3337544912543926,
      "grad_norm": 0.739216148853302,
      "learning_rate": 3.3314249615035334e-05,
      "loss": 0.7211,
      "step": 33780
    },
    {
      "epoch": 1.3341493268053854,
      "grad_norm": 0.7683604955673218,
      "learning_rate": 3.329450783748569e-05,
      "loss": 0.7343,
      "step": 33790
    },
    {
      "epoch": 1.3345441623563785,
      "grad_norm": 0.8166167736053467,
      "learning_rate": 3.327476605993603e-05,
      "loss": 0.7095,
      "step": 33800
    },
    {
      "epoch": 1.3349389979073716,
      "grad_norm": 0.7416499257087708,
      "learning_rate": 3.325502428238639e-05,
      "loss": 0.6818,
      "step": 33810
    },
    {
      "epoch": 1.3353338334583646,
      "grad_norm": 0.7748787999153137,
      "learning_rate": 3.323528250483673e-05,
      "loss": 0.7333,
      "step": 33820
    },
    {
      "epoch": 1.3357286690093577,
      "grad_norm": 0.6416032910346985,
      "learning_rate": 3.321554072728709e-05,
      "loss": 0.6935,
      "step": 33830
    },
    {
      "epoch": 1.3361235045603506,
      "grad_norm": 0.7266501188278198,
      "learning_rate": 3.319579894973743e-05,
      "loss": 0.7251,
      "step": 33840
    },
    {
      "epoch": 1.3365183401113436,
      "grad_norm": 0.6584497690200806,
      "learning_rate": 3.317605717218778e-05,
      "loss": 0.7191,
      "step": 33850
    },
    {
      "epoch": 1.3369131756623367,
      "grad_norm": 0.7187111377716064,
      "learning_rate": 3.315631539463813e-05,
      "loss": 0.6741,
      "step": 33860
    },
    {
      "epoch": 1.3373080112133295,
      "grad_norm": 0.7345609664916992,
      "learning_rate": 3.313657361708848e-05,
      "loss": 0.7109,
      "step": 33870
    },
    {
      "epoch": 1.3377028467643226,
      "grad_norm": 0.7693212032318115,
      "learning_rate": 3.311683183953883e-05,
      "loss": 0.6828,
      "step": 33880
    },
    {
      "epoch": 1.3380976823153157,
      "grad_norm": 0.723067581653595,
      "learning_rate": 3.309709006198918e-05,
      "loss": 0.7389,
      "step": 33890
    },
    {
      "epoch": 1.3384925178663087,
      "grad_norm": 0.7311731576919556,
      "learning_rate": 3.307734828443953e-05,
      "loss": 0.7018,
      "step": 33900
    },
    {
      "epoch": 1.3388873534173018,
      "grad_norm": 0.7938753366470337,
      "learning_rate": 3.305760650688988e-05,
      "loss": 0.6989,
      "step": 33910
    },
    {
      "epoch": 1.3392821889682947,
      "grad_norm": 0.6471623182296753,
      "learning_rate": 3.3037864729340226e-05,
      "loss": 0.6847,
      "step": 33920
    },
    {
      "epoch": 1.3396770245192877,
      "grad_norm": 0.6520810127258301,
      "learning_rate": 3.301812295179058e-05,
      "loss": 0.7351,
      "step": 33930
    },
    {
      "epoch": 1.3400718600702808,
      "grad_norm": 0.7987967729568481,
      "learning_rate": 3.2998381174240925e-05,
      "loss": 0.6954,
      "step": 33940
    },
    {
      "epoch": 1.3404666956212736,
      "grad_norm": 0.6976425051689148,
      "learning_rate": 3.297863939669128e-05,
      "loss": 0.6657,
      "step": 33950
    },
    {
      "epoch": 1.3408615311722667,
      "grad_norm": 0.8422870635986328,
      "learning_rate": 3.2958897619141625e-05,
      "loss": 0.7116,
      "step": 33960
    },
    {
      "epoch": 1.3412563667232598,
      "grad_norm": 0.6443798542022705,
      "learning_rate": 3.293915584159198e-05,
      "loss": 0.7501,
      "step": 33970
    },
    {
      "epoch": 1.3416512022742528,
      "grad_norm": 0.49968937039375305,
      "learning_rate": 3.2919414064042325e-05,
      "loss": 0.6989,
      "step": 33980
    },
    {
      "epoch": 1.342046037825246,
      "grad_norm": 0.790427029132843,
      "learning_rate": 3.2899672286492675e-05,
      "loss": 0.7012,
      "step": 33990
    },
    {
      "epoch": 1.3424408733762387,
      "grad_norm": 0.7362437844276428,
      "learning_rate": 3.2879930508943025e-05,
      "loss": 0.685,
      "step": 34000
    },
    {
      "epoch": 1.3424408733762387,
      "eval_loss": 0.7664983868598938,
      "eval_runtime": 1216.405,
      "eval_samples_per_second": 9.254,
      "eval_steps_per_second": 9.254,
      "step": 34000
    },
    {
      "epoch": 1.3428357089272318,
      "grad_norm": 0.7687358260154724,
      "learning_rate": 3.2860188731393375e-05,
      "loss": 0.6796,
      "step": 34010
    },
    {
      "epoch": 1.3432305444782249,
      "grad_norm": 0.7019081115722656,
      "learning_rate": 3.2840446953843725e-05,
      "loss": 0.7047,
      "step": 34020
    },
    {
      "epoch": 1.3436253800292177,
      "grad_norm": 0.6556189060211182,
      "learning_rate": 3.2820705176294075e-05,
      "loss": 0.6996,
      "step": 34030
    },
    {
      "epoch": 1.3440202155802108,
      "grad_norm": 0.5731749534606934,
      "learning_rate": 3.2800963398744425e-05,
      "loss": 0.7276,
      "step": 34040
    },
    {
      "epoch": 1.3444150511312039,
      "grad_norm": 0.8024553060531616,
      "learning_rate": 3.2781221621194774e-05,
      "loss": 0.749,
      "step": 34050
    },
    {
      "epoch": 1.344809886682197,
      "grad_norm": 0.6609485149383545,
      "learning_rate": 3.276147984364512e-05,
      "loss": 0.712,
      "step": 34060
    },
    {
      "epoch": 1.34520472223319,
      "grad_norm": 0.8687366843223572,
      "learning_rate": 3.2741738066095474e-05,
      "loss": 0.7422,
      "step": 34070
    },
    {
      "epoch": 1.3455995577841828,
      "grad_norm": 0.9518833756446838,
      "learning_rate": 3.272199628854582e-05,
      "loss": 0.7282,
      "step": 34080
    },
    {
      "epoch": 1.345994393335176,
      "grad_norm": 0.6920703053474426,
      "learning_rate": 3.2702254510996174e-05,
      "loss": 0.7149,
      "step": 34090
    },
    {
      "epoch": 1.346389228886169,
      "grad_norm": 0.7178682088851929,
      "learning_rate": 3.268251273344652e-05,
      "loss": 0.6743,
      "step": 34100
    },
    {
      "epoch": 1.3467840644371618,
      "grad_norm": 0.6642773151397705,
      "learning_rate": 3.2662770955896874e-05,
      "loss": 0.6641,
      "step": 34110
    },
    {
      "epoch": 1.3471788999881549,
      "grad_norm": 0.7713853120803833,
      "learning_rate": 3.264302917834722e-05,
      "loss": 0.6847,
      "step": 34120
    },
    {
      "epoch": 1.347573735539148,
      "grad_norm": 1.0000100135803223,
      "learning_rate": 3.262328740079757e-05,
      "loss": 0.6893,
      "step": 34130
    },
    {
      "epoch": 1.347968571090141,
      "grad_norm": 0.8472684025764465,
      "learning_rate": 3.260354562324792e-05,
      "loss": 0.7018,
      "step": 34140
    },
    {
      "epoch": 1.348363406641134,
      "grad_norm": 0.7157900929450989,
      "learning_rate": 3.258380384569827e-05,
      "loss": 0.7285,
      "step": 34150
    },
    {
      "epoch": 1.348758242192127,
      "grad_norm": 0.6742936968803406,
      "learning_rate": 3.256406206814862e-05,
      "loss": 0.7322,
      "step": 34160
    },
    {
      "epoch": 1.34915307774312,
      "grad_norm": 0.9437055587768555,
      "learning_rate": 3.2544320290598967e-05,
      "loss": 0.6659,
      "step": 34170
    },
    {
      "epoch": 1.349547913294113,
      "grad_norm": 0.7298562526702881,
      "learning_rate": 3.2524578513049317e-05,
      "loss": 0.732,
      "step": 34180
    },
    {
      "epoch": 1.349942748845106,
      "grad_norm": 0.6703334450721741,
      "learning_rate": 3.2504836735499666e-05,
      "loss": 0.7051,
      "step": 34190
    },
    {
      "epoch": 1.350337584396099,
      "grad_norm": 0.9297476410865784,
      "learning_rate": 3.2485094957950016e-05,
      "loss": 0.7027,
      "step": 34200
    },
    {
      "epoch": 1.350732419947092,
      "grad_norm": 0.7334281206130981,
      "learning_rate": 3.2465353180400366e-05,
      "loss": 0.7011,
      "step": 34210
    },
    {
      "epoch": 1.351127255498085,
      "grad_norm": 0.845245361328125,
      "learning_rate": 3.244561140285071e-05,
      "loss": 0.6886,
      "step": 34220
    },
    {
      "epoch": 1.3515220910490782,
      "grad_norm": 0.7043275237083435,
      "learning_rate": 3.2425869625301066e-05,
      "loss": 0.7285,
      "step": 34230
    },
    {
      "epoch": 1.351916926600071,
      "grad_norm": 0.6843281984329224,
      "learning_rate": 3.240612784775141e-05,
      "loss": 0.7222,
      "step": 34240
    },
    {
      "epoch": 1.352311762151064,
      "grad_norm": 0.6808263659477234,
      "learning_rate": 3.2386386070201766e-05,
      "loss": 0.7299,
      "step": 34250
    },
    {
      "epoch": 1.3527065977020571,
      "grad_norm": 0.7197237610816956,
      "learning_rate": 3.236664429265211e-05,
      "loss": 0.6706,
      "step": 34260
    },
    {
      "epoch": 1.35310143325305,
      "grad_norm": 0.6993738412857056,
      "learning_rate": 3.234690251510246e-05,
      "loss": 0.6876,
      "step": 34270
    },
    {
      "epoch": 1.353496268804043,
      "grad_norm": 0.7238065600395203,
      "learning_rate": 3.232716073755281e-05,
      "loss": 0.701,
      "step": 34280
    },
    {
      "epoch": 1.3538911043550361,
      "grad_norm": 0.7713837623596191,
      "learning_rate": 3.230741896000316e-05,
      "loss": 0.7303,
      "step": 34290
    },
    {
      "epoch": 1.3542859399060292,
      "grad_norm": 0.7789732217788696,
      "learning_rate": 3.228767718245351e-05,
      "loss": 0.6996,
      "step": 34300
    },
    {
      "epoch": 1.3546807754570223,
      "grad_norm": 0.8090287446975708,
      "learning_rate": 3.226793540490386e-05,
      "loss": 0.7062,
      "step": 34310
    },
    {
      "epoch": 1.355075611008015,
      "grad_norm": 0.7425891757011414,
      "learning_rate": 3.224819362735421e-05,
      "loss": 0.6672,
      "step": 34320
    },
    {
      "epoch": 1.3554704465590082,
      "grad_norm": 0.6317662596702576,
      "learning_rate": 3.222845184980456e-05,
      "loss": 0.6875,
      "step": 34330
    },
    {
      "epoch": 1.3558652821100012,
      "grad_norm": 0.7228212952613831,
      "learning_rate": 3.220871007225491e-05,
      "loss": 0.7346,
      "step": 34340
    },
    {
      "epoch": 1.356260117660994,
      "grad_norm": 0.6305888295173645,
      "learning_rate": 3.218896829470526e-05,
      "loss": 0.7362,
      "step": 34350
    },
    {
      "epoch": 1.3566549532119871,
      "grad_norm": 0.7781316637992859,
      "learning_rate": 3.21692265171556e-05,
      "loss": 0.6792,
      "step": 34360
    },
    {
      "epoch": 1.3570497887629802,
      "grad_norm": 0.7217084169387817,
      "learning_rate": 3.214948473960596e-05,
      "loss": 0.7101,
      "step": 34370
    },
    {
      "epoch": 1.3574446243139733,
      "grad_norm": 0.7115039229393005,
      "learning_rate": 3.21297429620563e-05,
      "loss": 0.6974,
      "step": 34380
    },
    {
      "epoch": 1.3578394598649663,
      "grad_norm": 0.7185164093971252,
      "learning_rate": 3.211000118450666e-05,
      "loss": 0.6886,
      "step": 34390
    },
    {
      "epoch": 1.3582342954159592,
      "grad_norm": 0.7871456742286682,
      "learning_rate": 3.2090259406957e-05,
      "loss": 0.7001,
      "step": 34400
    },
    {
      "epoch": 1.3586291309669523,
      "grad_norm": 0.7448736429214478,
      "learning_rate": 3.207051762940735e-05,
      "loss": 0.6741,
      "step": 34410
    },
    {
      "epoch": 1.3590239665179453,
      "grad_norm": 0.6973951458930969,
      "learning_rate": 3.20507758518577e-05,
      "loss": 0.713,
      "step": 34420
    },
    {
      "epoch": 1.3594188020689382,
      "grad_norm": 0.9793325066566467,
      "learning_rate": 3.203103407430805e-05,
      "loss": 0.7284,
      "step": 34430
    },
    {
      "epoch": 1.3598136376199312,
      "grad_norm": 0.7677395343780518,
      "learning_rate": 3.20112922967584e-05,
      "loss": 0.7209,
      "step": 34440
    },
    {
      "epoch": 1.3602084731709243,
      "grad_norm": 0.817908525466919,
      "learning_rate": 3.199155051920875e-05,
      "loss": 0.7327,
      "step": 34450
    },
    {
      "epoch": 1.3606033087219174,
      "grad_norm": 0.6956977844238281,
      "learning_rate": 3.19718087416591e-05,
      "loss": 0.696,
      "step": 34460
    },
    {
      "epoch": 1.3609981442729104,
      "grad_norm": 0.8781453967094421,
      "learning_rate": 3.195206696410945e-05,
      "loss": 0.6607,
      "step": 34470
    },
    {
      "epoch": 1.3613929798239033,
      "grad_norm": 0.6520624160766602,
      "learning_rate": 3.19323251865598e-05,
      "loss": 0.7162,
      "step": 34480
    },
    {
      "epoch": 1.3617878153748963,
      "grad_norm": 0.7808340787887573,
      "learning_rate": 3.191258340901015e-05,
      "loss": 0.7404,
      "step": 34490
    },
    {
      "epoch": 1.3621826509258894,
      "grad_norm": 0.7198070287704468,
      "learning_rate": 3.1892841631460493e-05,
      "loss": 0.6504,
      "step": 34500
    },
    {
      "epoch": 1.3625774864768823,
      "grad_norm": 0.832673966884613,
      "learning_rate": 3.187309985391085e-05,
      "loss": 0.7077,
      "step": 34510
    },
    {
      "epoch": 1.3629723220278753,
      "grad_norm": 0.8415895104408264,
      "learning_rate": 3.185335807636119e-05,
      "loss": 0.7328,
      "step": 34520
    },
    {
      "epoch": 1.3633671575788684,
      "grad_norm": 0.6976339221000671,
      "learning_rate": 3.183361629881155e-05,
      "loss": 0.7122,
      "step": 34530
    },
    {
      "epoch": 1.3637619931298615,
      "grad_norm": 0.6445708870887756,
      "learning_rate": 3.181387452126189e-05,
      "loss": 0.7295,
      "step": 34540
    },
    {
      "epoch": 1.3641568286808545,
      "grad_norm": 0.8357150554656982,
      "learning_rate": 3.179413274371224e-05,
      "loss": 0.7391,
      "step": 34550
    },
    {
      "epoch": 1.3645516642318474,
      "grad_norm": 0.9367699027061462,
      "learning_rate": 3.177439096616259e-05,
      "loss": 0.6946,
      "step": 34560
    },
    {
      "epoch": 1.3649464997828404,
      "grad_norm": 0.7453479170799255,
      "learning_rate": 3.175464918861294e-05,
      "loss": 0.7017,
      "step": 34570
    },
    {
      "epoch": 1.3653413353338335,
      "grad_norm": 0.9528185725212097,
      "learning_rate": 3.173490741106329e-05,
      "loss": 0.699,
      "step": 34580
    },
    {
      "epoch": 1.3657361708848264,
      "grad_norm": 0.7235633134841919,
      "learning_rate": 3.171516563351364e-05,
      "loss": 0.7001,
      "step": 34590
    },
    {
      "epoch": 1.3661310064358194,
      "grad_norm": 0.9574000835418701,
      "learning_rate": 3.169542385596399e-05,
      "loss": 0.7269,
      "step": 34600
    },
    {
      "epoch": 1.3665258419868125,
      "grad_norm": 0.7966901063919067,
      "learning_rate": 3.167568207841434e-05,
      "loss": 0.7088,
      "step": 34610
    },
    {
      "epoch": 1.3669206775378056,
      "grad_norm": 0.6770614981651306,
      "learning_rate": 3.165594030086469e-05,
      "loss": 0.7249,
      "step": 34620
    },
    {
      "epoch": 1.3673155130887986,
      "grad_norm": 0.7267922759056091,
      "learning_rate": 3.163619852331504e-05,
      "loss": 0.7068,
      "step": 34630
    },
    {
      "epoch": 1.3677103486397915,
      "grad_norm": 0.7307364344596863,
      "learning_rate": 3.1616456745765385e-05,
      "loss": 0.7412,
      "step": 34640
    },
    {
      "epoch": 1.3681051841907845,
      "grad_norm": 0.8423089385032654,
      "learning_rate": 3.159671496821574e-05,
      "loss": 0.6911,
      "step": 34650
    },
    {
      "epoch": 1.3685000197417776,
      "grad_norm": 1.00625479221344,
      "learning_rate": 3.1576973190666085e-05,
      "loss": 0.6969,
      "step": 34660
    },
    {
      "epoch": 1.3688948552927704,
      "grad_norm": 0.9894999861717224,
      "learning_rate": 3.155723141311644e-05,
      "loss": 0.7428,
      "step": 34670
    },
    {
      "epoch": 1.3692896908437635,
      "grad_norm": 0.8053999543190002,
      "learning_rate": 3.1537489635566785e-05,
      "loss": 0.776,
      "step": 34680
    },
    {
      "epoch": 1.3696845263947566,
      "grad_norm": 0.9964807033538818,
      "learning_rate": 3.1517747858017135e-05,
      "loss": 0.7411,
      "step": 34690
    },
    {
      "epoch": 1.3700793619457496,
      "grad_norm": 0.6157358884811401,
      "learning_rate": 3.1498006080467485e-05,
      "loss": 0.6841,
      "step": 34700
    },
    {
      "epoch": 1.3704741974967427,
      "grad_norm": 0.7782538533210754,
      "learning_rate": 3.1478264302917835e-05,
      "loss": 0.7162,
      "step": 34710
    },
    {
      "epoch": 1.3708690330477356,
      "grad_norm": 0.7268357276916504,
      "learning_rate": 3.1458522525368185e-05,
      "loss": 0.7053,
      "step": 34720
    },
    {
      "epoch": 1.3712638685987286,
      "grad_norm": 0.7180348038673401,
      "learning_rate": 3.1438780747818535e-05,
      "loss": 0.7168,
      "step": 34730
    },
    {
      "epoch": 1.3716587041497217,
      "grad_norm": 0.6778231859207153,
      "learning_rate": 3.1419038970268885e-05,
      "loss": 0.7155,
      "step": 34740
    },
    {
      "epoch": 1.3720535397007145,
      "grad_norm": 0.636323094367981,
      "learning_rate": 3.1399297192719235e-05,
      "loss": 0.6662,
      "step": 34750
    },
    {
      "epoch": 1.3724483752517076,
      "grad_norm": 0.849612832069397,
      "learning_rate": 3.1379555415169584e-05,
      "loss": 0.7084,
      "step": 34760
    },
    {
      "epoch": 1.3728432108027007,
      "grad_norm": 0.9446296095848083,
      "learning_rate": 3.1359813637619934e-05,
      "loss": 0.7283,
      "step": 34770
    },
    {
      "epoch": 1.3732380463536937,
      "grad_norm": 0.6634436845779419,
      "learning_rate": 3.134007186007028e-05,
      "loss": 0.6694,
      "step": 34780
    },
    {
      "epoch": 1.3736328819046868,
      "grad_norm": 1.1013962030410767,
      "learning_rate": 3.1320330082520634e-05,
      "loss": 0.6914,
      "step": 34790
    },
    {
      "epoch": 1.3740277174556796,
      "grad_norm": 0.6350129842758179,
      "learning_rate": 3.130058830497098e-05,
      "loss": 0.7658,
      "step": 34800
    },
    {
      "epoch": 1.3744225530066727,
      "grad_norm": 0.7908412218093872,
      "learning_rate": 3.1280846527421334e-05,
      "loss": 0.7168,
      "step": 34810
    },
    {
      "epoch": 1.3748173885576658,
      "grad_norm": 0.5782409310340881,
      "learning_rate": 3.126110474987168e-05,
      "loss": 0.6605,
      "step": 34820
    },
    {
      "epoch": 1.3752122241086586,
      "grad_norm": 0.9693073630332947,
      "learning_rate": 3.124136297232203e-05,
      "loss": 0.7213,
      "step": 34830
    },
    {
      "epoch": 1.3756070596596517,
      "grad_norm": 0.7203149795532227,
      "learning_rate": 3.122162119477238e-05,
      "loss": 0.715,
      "step": 34840
    },
    {
      "epoch": 1.3760018952106448,
      "grad_norm": 0.7143664360046387,
      "learning_rate": 3.120187941722273e-05,
      "loss": 0.6737,
      "step": 34850
    },
    {
      "epoch": 1.3763967307616378,
      "grad_norm": 0.716745138168335,
      "learning_rate": 3.118213763967308e-05,
      "loss": 0.7263,
      "step": 34860
    },
    {
      "epoch": 1.3767915663126309,
      "grad_norm": 0.6410129070281982,
      "learning_rate": 3.116239586212343e-05,
      "loss": 0.6979,
      "step": 34870
    },
    {
      "epoch": 1.3771864018636237,
      "grad_norm": 0.7431676983833313,
      "learning_rate": 3.114265408457378e-05,
      "loss": 0.7343,
      "step": 34880
    },
    {
      "epoch": 1.3775812374146168,
      "grad_norm": 0.9912253618240356,
      "learning_rate": 3.1122912307024127e-05,
      "loss": 0.6647,
      "step": 34890
    },
    {
      "epoch": 1.3779760729656099,
      "grad_norm": 0.7113734483718872,
      "learning_rate": 3.1103170529474476e-05,
      "loss": 0.7308,
      "step": 34900
    },
    {
      "epoch": 1.3783709085166027,
      "grad_norm": 1.1388535499572754,
      "learning_rate": 3.1083428751924826e-05,
      "loss": 0.6954,
      "step": 34910
    },
    {
      "epoch": 1.3787657440675958,
      "grad_norm": 0.838797390460968,
      "learning_rate": 3.106368697437517e-05,
      "loss": 0.7344,
      "step": 34920
    },
    {
      "epoch": 1.3791605796185888,
      "grad_norm": 0.7593907713890076,
      "learning_rate": 3.1043945196825526e-05,
      "loss": 0.7425,
      "step": 34930
    },
    {
      "epoch": 1.379555415169582,
      "grad_norm": 0.6737447381019592,
      "learning_rate": 3.102420341927587e-05,
      "loss": 0.7205,
      "step": 34940
    },
    {
      "epoch": 1.379950250720575,
      "grad_norm": 0.9322470426559448,
      "learning_rate": 3.1004461641726226e-05,
      "loss": 0.7398,
      "step": 34950
    },
    {
      "epoch": 1.3803450862715678,
      "grad_norm": 0.7349722981452942,
      "learning_rate": 3.098471986417657e-05,
      "loss": 0.6919,
      "step": 34960
    },
    {
      "epoch": 1.380739921822561,
      "grad_norm": 0.9190786480903625,
      "learning_rate": 3.096497808662692e-05,
      "loss": 0.7104,
      "step": 34970
    },
    {
      "epoch": 1.381134757373554,
      "grad_norm": 0.9978435635566711,
      "learning_rate": 3.094523630907727e-05,
      "loss": 0.7264,
      "step": 34980
    },
    {
      "epoch": 1.3815295929245468,
      "grad_norm": 0.7858573794364929,
      "learning_rate": 3.092549453152762e-05,
      "loss": 0.6781,
      "step": 34990
    },
    {
      "epoch": 1.3819244284755399,
      "grad_norm": 0.8589054942131042,
      "learning_rate": 3.090575275397797e-05,
      "loss": 0.6925,
      "step": 35000
    },
    {
      "epoch": 1.3819244284755399,
      "eval_loss": 0.7662808299064636,
      "eval_runtime": 1218.2414,
      "eval_samples_per_second": 9.24,
      "eval_steps_per_second": 9.24,
      "step": 35000
    },
    {
      "epoch": 1.382319264026533,
      "grad_norm": 0.8132546544075012,
      "learning_rate": 3.088601097642832e-05,
      "loss": 0.7393,
      "step": 35010
    },
    {
      "epoch": 1.382714099577526,
      "grad_norm": 1.1567797660827637,
      "learning_rate": 3.086626919887867e-05,
      "loss": 0.7599,
      "step": 35020
    },
    {
      "epoch": 1.383108935128519,
      "grad_norm": 0.8429380655288696,
      "learning_rate": 3.084652742132902e-05,
      "loss": 0.7146,
      "step": 35030
    },
    {
      "epoch": 1.383503770679512,
      "grad_norm": 0.7536012530326843,
      "learning_rate": 3.082678564377937e-05,
      "loss": 0.7267,
      "step": 35040
    },
    {
      "epoch": 1.383898606230505,
      "grad_norm": 1.516585111618042,
      "learning_rate": 3.080704386622972e-05,
      "loss": 0.777,
      "step": 35050
    },
    {
      "epoch": 1.384293441781498,
      "grad_norm": 0.766909658908844,
      "learning_rate": 3.078730208868006e-05,
      "loss": 0.6747,
      "step": 35060
    },
    {
      "epoch": 1.384688277332491,
      "grad_norm": 0.899590790271759,
      "learning_rate": 3.076756031113042e-05,
      "loss": 0.7013,
      "step": 35070
    },
    {
      "epoch": 1.385083112883484,
      "grad_norm": 0.7036729454994202,
      "learning_rate": 3.074781853358076e-05,
      "loss": 0.71,
      "step": 35080
    },
    {
      "epoch": 1.385477948434477,
      "grad_norm": 0.6859838962554932,
      "learning_rate": 3.072807675603112e-05,
      "loss": 0.7105,
      "step": 35090
    },
    {
      "epoch": 1.38587278398547,
      "grad_norm": 0.733327329158783,
      "learning_rate": 3.070833497848146e-05,
      "loss": 0.6599,
      "step": 35100
    },
    {
      "epoch": 1.3862676195364632,
      "grad_norm": 0.8779726028442383,
      "learning_rate": 3.068859320093181e-05,
      "loss": 0.7694,
      "step": 35110
    },
    {
      "epoch": 1.386662455087456,
      "grad_norm": 0.6306389570236206,
      "learning_rate": 3.066885142338216e-05,
      "loss": 0.7079,
      "step": 35120
    },
    {
      "epoch": 1.387057290638449,
      "grad_norm": 0.9047890305519104,
      "learning_rate": 3.064910964583251e-05,
      "loss": 0.7222,
      "step": 35130
    },
    {
      "epoch": 1.3874521261894421,
      "grad_norm": 0.6290932297706604,
      "learning_rate": 3.062936786828286e-05,
      "loss": 0.6815,
      "step": 35140
    },
    {
      "epoch": 1.387846961740435,
      "grad_norm": 0.7003613710403442,
      "learning_rate": 3.060962609073321e-05,
      "loss": 0.713,
      "step": 35150
    },
    {
      "epoch": 1.388241797291428,
      "grad_norm": 0.7034545540809631,
      "learning_rate": 3.058988431318356e-05,
      "loss": 0.7199,
      "step": 35160
    },
    {
      "epoch": 1.3886366328424211,
      "grad_norm": 0.6616125106811523,
      "learning_rate": 3.057014253563391e-05,
      "loss": 0.7007,
      "step": 35170
    },
    {
      "epoch": 1.3890314683934142,
      "grad_norm": 0.6929308772087097,
      "learning_rate": 3.055040075808426e-05,
      "loss": 0.6776,
      "step": 35180
    },
    {
      "epoch": 1.3894263039444072,
      "grad_norm": 0.7429335713386536,
      "learning_rate": 3.053065898053461e-05,
      "loss": 0.7152,
      "step": 35190
    },
    {
      "epoch": 1.3898211394954,
      "grad_norm": 0.6243844628334045,
      "learning_rate": 3.0510917202984957e-05,
      "loss": 0.7147,
      "step": 35200
    },
    {
      "epoch": 1.3902159750463932,
      "grad_norm": 0.6623775959014893,
      "learning_rate": 3.049117542543531e-05,
      "loss": 0.6948,
      "step": 35210
    },
    {
      "epoch": 1.3906108105973862,
      "grad_norm": 0.6835333704948425,
      "learning_rate": 3.0471433647885657e-05,
      "loss": 0.7316,
      "step": 35220
    },
    {
      "epoch": 1.391005646148379,
      "grad_norm": 0.8085545301437378,
      "learning_rate": 3.0451691870336007e-05,
      "loss": 0.691,
      "step": 35230
    },
    {
      "epoch": 1.3914004816993721,
      "grad_norm": 0.8173379898071289,
      "learning_rate": 3.0431950092786353e-05,
      "loss": 0.6801,
      "step": 35240
    },
    {
      "epoch": 1.3917953172503652,
      "grad_norm": 0.7089573740959167,
      "learning_rate": 3.0412208315236706e-05,
      "loss": 0.6984,
      "step": 35250
    },
    {
      "epoch": 1.3921901528013583,
      "grad_norm": 0.9393007159233093,
      "learning_rate": 3.0392466537687053e-05,
      "loss": 0.7039,
      "step": 35260
    },
    {
      "epoch": 1.3925849883523513,
      "grad_norm": 0.719855785369873,
      "learning_rate": 3.0372724760137406e-05,
      "loss": 0.6902,
      "step": 35270
    },
    {
      "epoch": 1.3929798239033442,
      "grad_norm": 0.8946349620819092,
      "learning_rate": 3.0352982982587753e-05,
      "loss": 0.6966,
      "step": 35280
    },
    {
      "epoch": 1.3933746594543372,
      "grad_norm": 0.746956467628479,
      "learning_rate": 3.0333241205038103e-05,
      "loss": 0.7462,
      "step": 35290
    },
    {
      "epoch": 1.3937694950053303,
      "grad_norm": 0.6669699549674988,
      "learning_rate": 3.031349942748845e-05,
      "loss": 0.7071,
      "step": 35300
    },
    {
      "epoch": 1.3941643305563232,
      "grad_norm": 1.0277820825576782,
      "learning_rate": 3.0293757649938803e-05,
      "loss": 0.736,
      "step": 35310
    },
    {
      "epoch": 1.3945591661073162,
      "grad_norm": 0.8328551650047302,
      "learning_rate": 3.027401587238915e-05,
      "loss": 0.6809,
      "step": 35320
    },
    {
      "epoch": 1.3949540016583093,
      "grad_norm": 0.7702121734619141,
      "learning_rate": 3.0254274094839502e-05,
      "loss": 0.7265,
      "step": 35330
    },
    {
      "epoch": 1.3953488372093024,
      "grad_norm": 0.6939008831977844,
      "learning_rate": 3.023453231728985e-05,
      "loss": 0.7036,
      "step": 35340
    },
    {
      "epoch": 1.3957436727602954,
      "grad_norm": 1.0361413955688477,
      "learning_rate": 3.0214790539740202e-05,
      "loss": 0.7152,
      "step": 35350
    },
    {
      "epoch": 1.3961385083112883,
      "grad_norm": 0.8458849191665649,
      "learning_rate": 3.019504876219055e-05,
      "loss": 0.7234,
      "step": 35360
    },
    {
      "epoch": 1.3965333438622813,
      "grad_norm": 0.7468644380569458,
      "learning_rate": 3.01753069846409e-05,
      "loss": 0.7116,
      "step": 35370
    },
    {
      "epoch": 1.3969281794132744,
      "grad_norm": 0.884882390499115,
      "learning_rate": 3.0155565207091245e-05,
      "loss": 0.7053,
      "step": 35380
    },
    {
      "epoch": 1.3973230149642673,
      "grad_norm": 0.6742331385612488,
      "learning_rate": 3.01358234295416e-05,
      "loss": 0.7165,
      "step": 35390
    },
    {
      "epoch": 1.3977178505152603,
      "grad_norm": 0.6802666783332825,
      "learning_rate": 3.0116081651991945e-05,
      "loss": 0.6809,
      "step": 35400
    },
    {
      "epoch": 1.3981126860662534,
      "grad_norm": 0.8630008101463318,
      "learning_rate": 3.00963398744423e-05,
      "loss": 0.7217,
      "step": 35410
    },
    {
      "epoch": 1.3985075216172465,
      "grad_norm": 0.6967977285385132,
      "learning_rate": 3.0076598096892645e-05,
      "loss": 0.7089,
      "step": 35420
    },
    {
      "epoch": 1.3989023571682395,
      "grad_norm": 0.6103845238685608,
      "learning_rate": 3.0056856319342995e-05,
      "loss": 0.737,
      "step": 35430
    },
    {
      "epoch": 1.3992971927192324,
      "grad_norm": 0.7318221926689148,
      "learning_rate": 3.0037114541793345e-05,
      "loss": 0.7123,
      "step": 35440
    },
    {
      "epoch": 1.3996920282702254,
      "grad_norm": 0.6810833811759949,
      "learning_rate": 3.0017372764243695e-05,
      "loss": 0.7011,
      "step": 35450
    },
    {
      "epoch": 1.4000868638212185,
      "grad_norm": 0.7762901782989502,
      "learning_rate": 2.999763098669404e-05,
      "loss": 0.7038,
      "step": 35460
    },
    {
      "epoch": 1.4004816993722113,
      "grad_norm": 0.7657638192176819,
      "learning_rate": 2.9977889209144394e-05,
      "loss": 0.7192,
      "step": 35470
    },
    {
      "epoch": 1.4008765349232044,
      "grad_norm": 0.8724566698074341,
      "learning_rate": 2.995814743159474e-05,
      "loss": 0.7167,
      "step": 35480
    },
    {
      "epoch": 1.4012713704741975,
      "grad_norm": 0.7720882296562195,
      "learning_rate": 2.9938405654045094e-05,
      "loss": 0.7346,
      "step": 35490
    },
    {
      "epoch": 1.4016662060251905,
      "grad_norm": 0.7734428644180298,
      "learning_rate": 2.991866387649544e-05,
      "loss": 0.7733,
      "step": 35500
    },
    {
      "epoch": 1.4020610415761836,
      "grad_norm": 0.6746132373809814,
      "learning_rate": 2.989892209894579e-05,
      "loss": 0.6717,
      "step": 35510
    },
    {
      "epoch": 1.4024558771271765,
      "grad_norm": 0.7139909267425537,
      "learning_rate": 2.9879180321396137e-05,
      "loss": 0.6893,
      "step": 35520
    },
    {
      "epoch": 1.4028507126781695,
      "grad_norm": 0.7972297668457031,
      "learning_rate": 2.985943854384649e-05,
      "loss": 0.6923,
      "step": 35530
    },
    {
      "epoch": 1.4032455482291626,
      "grad_norm": 0.711167573928833,
      "learning_rate": 2.9839696766296837e-05,
      "loss": 0.7204,
      "step": 35540
    },
    {
      "epoch": 1.4036403837801554,
      "grad_norm": 0.8476189374923706,
      "learning_rate": 2.981995498874719e-05,
      "loss": 0.662,
      "step": 35550
    },
    {
      "epoch": 1.4040352193311485,
      "grad_norm": 1.003077745437622,
      "learning_rate": 2.9800213211197537e-05,
      "loss": 0.7463,
      "step": 35560
    },
    {
      "epoch": 1.4044300548821416,
      "grad_norm": 0.6815871000289917,
      "learning_rate": 2.978047143364789e-05,
      "loss": 0.6439,
      "step": 35570
    },
    {
      "epoch": 1.4048248904331346,
      "grad_norm": 0.733941376209259,
      "learning_rate": 2.9760729656098237e-05,
      "loss": 0.6949,
      "step": 35580
    },
    {
      "epoch": 1.4052197259841277,
      "grad_norm": 0.7979305386543274,
      "learning_rate": 2.9740987878548587e-05,
      "loss": 0.6918,
      "step": 35590
    },
    {
      "epoch": 1.4056145615351205,
      "grad_norm": 0.6778528094291687,
      "learning_rate": 2.9721246100998933e-05,
      "loss": 0.6277,
      "step": 35600
    },
    {
      "epoch": 1.4060093970861136,
      "grad_norm": 0.7698192000389099,
      "learning_rate": 2.9701504323449286e-05,
      "loss": 0.7115,
      "step": 35610
    },
    {
      "epoch": 1.4064042326371067,
      "grad_norm": 0.8064047694206238,
      "learning_rate": 2.9681762545899633e-05,
      "loss": 0.7192,
      "step": 35620
    },
    {
      "epoch": 1.4067990681880995,
      "grad_norm": 0.8508288264274597,
      "learning_rate": 2.9662020768349986e-05,
      "loss": 0.6945,
      "step": 35630
    },
    {
      "epoch": 1.4071939037390926,
      "grad_norm": 1.022142767906189,
      "learning_rate": 2.9642278990800333e-05,
      "loss": 0.7009,
      "step": 35640
    },
    {
      "epoch": 1.4075887392900857,
      "grad_norm": 0.9165693521499634,
      "learning_rate": 2.9622537213250683e-05,
      "loss": 0.7452,
      "step": 35650
    },
    {
      "epoch": 1.4079835748410787,
      "grad_norm": 0.7915076613426208,
      "learning_rate": 2.960279543570103e-05,
      "loss": 0.7161,
      "step": 35660
    },
    {
      "epoch": 1.4083784103920718,
      "grad_norm": 0.7652508616447449,
      "learning_rate": 2.9583053658151383e-05,
      "loss": 0.7277,
      "step": 35670
    },
    {
      "epoch": 1.4087732459430646,
      "grad_norm": 1.0566368103027344,
      "learning_rate": 2.956331188060173e-05,
      "loss": 0.7029,
      "step": 35680
    },
    {
      "epoch": 1.4091680814940577,
      "grad_norm": 0.7973082661628723,
      "learning_rate": 2.9543570103052082e-05,
      "loss": 0.6893,
      "step": 35690
    },
    {
      "epoch": 1.4095629170450508,
      "grad_norm": 0.8429720401763916,
      "learning_rate": 2.952382832550243e-05,
      "loss": 0.7155,
      "step": 35700
    },
    {
      "epoch": 1.4099577525960436,
      "grad_norm": 0.8468986749649048,
      "learning_rate": 2.9504086547952782e-05,
      "loss": 0.7013,
      "step": 35710
    },
    {
      "epoch": 1.4103525881470367,
      "grad_norm": 0.738578200340271,
      "learning_rate": 2.948434477040313e-05,
      "loss": 0.7121,
      "step": 35720
    },
    {
      "epoch": 1.4107474236980297,
      "grad_norm": 1.060502529144287,
      "learning_rate": 2.946460299285348e-05,
      "loss": 0.6473,
      "step": 35730
    },
    {
      "epoch": 1.4111422592490228,
      "grad_norm": 0.9052024483680725,
      "learning_rate": 2.9444861215303825e-05,
      "loss": 0.7271,
      "step": 35740
    },
    {
      "epoch": 1.4115370948000159,
      "grad_norm": 0.795340895652771,
      "learning_rate": 2.942511943775418e-05,
      "loss": 0.7219,
      "step": 35750
    },
    {
      "epoch": 1.4119319303510087,
      "grad_norm": 1.0642808675765991,
      "learning_rate": 2.9405377660204525e-05,
      "loss": 0.6592,
      "step": 35760
    },
    {
      "epoch": 1.4123267659020018,
      "grad_norm": 0.7810069918632507,
      "learning_rate": 2.9385635882654878e-05,
      "loss": 0.7253,
      "step": 35770
    },
    {
      "epoch": 1.4127216014529949,
      "grad_norm": 0.732778012752533,
      "learning_rate": 2.9365894105105225e-05,
      "loss": 0.699,
      "step": 35780
    },
    {
      "epoch": 1.4131164370039877,
      "grad_norm": 0.7994028925895691,
      "learning_rate": 2.9346152327555575e-05,
      "loss": 0.7062,
      "step": 35790
    },
    {
      "epoch": 1.4135112725549808,
      "grad_norm": 0.71192467212677,
      "learning_rate": 2.932641055000592e-05,
      "loss": 0.7109,
      "step": 35800
    },
    {
      "epoch": 1.4139061081059738,
      "grad_norm": 0.7680704593658447,
      "learning_rate": 2.9306668772456275e-05,
      "loss": 0.701,
      "step": 35810
    },
    {
      "epoch": 1.414300943656967,
      "grad_norm": 0.8409878611564636,
      "learning_rate": 2.928692699490662e-05,
      "loss": 0.7195,
      "step": 35820
    },
    {
      "epoch": 1.41469577920796,
      "grad_norm": 0.7320556640625,
      "learning_rate": 2.9267185217356974e-05,
      "loss": 0.7036,
      "step": 35830
    },
    {
      "epoch": 1.4150906147589528,
      "grad_norm": 0.767324686050415,
      "learning_rate": 2.924744343980732e-05,
      "loss": 0.7036,
      "step": 35840
    },
    {
      "epoch": 1.4154854503099459,
      "grad_norm": 0.8524002432823181,
      "learning_rate": 2.9227701662257674e-05,
      "loss": 0.7124,
      "step": 35850
    },
    {
      "epoch": 1.415880285860939,
      "grad_norm": 1.0576715469360352,
      "learning_rate": 2.920795988470802e-05,
      "loss": 0.7167,
      "step": 35860
    },
    {
      "epoch": 1.4162751214119318,
      "grad_norm": 0.6837464570999146,
      "learning_rate": 2.918821810715837e-05,
      "loss": 0.7015,
      "step": 35870
    },
    {
      "epoch": 1.4166699569629249,
      "grad_norm": 0.7024692893028259,
      "learning_rate": 2.9168476329608717e-05,
      "loss": 0.7356,
      "step": 35880
    },
    {
      "epoch": 1.417064792513918,
      "grad_norm": 0.858310878276825,
      "learning_rate": 2.914873455205907e-05,
      "loss": 0.7551,
      "step": 35890
    },
    {
      "epoch": 1.417459628064911,
      "grad_norm": 0.7616181373596191,
      "learning_rate": 2.9128992774509417e-05,
      "loss": 0.6961,
      "step": 35900
    },
    {
      "epoch": 1.417854463615904,
      "grad_norm": 0.8201426863670349,
      "learning_rate": 2.910925099695977e-05,
      "loss": 0.7003,
      "step": 35910
    },
    {
      "epoch": 1.418249299166897,
      "grad_norm": 0.6737470030784607,
      "learning_rate": 2.9089509219410117e-05,
      "loss": 0.7083,
      "step": 35920
    },
    {
      "epoch": 1.41864413471789,
      "grad_norm": 0.8204128742218018,
      "learning_rate": 2.9069767441860467e-05,
      "loss": 0.735,
      "step": 35930
    },
    {
      "epoch": 1.419038970268883,
      "grad_norm": 0.8180732131004333,
      "learning_rate": 2.9050025664310813e-05,
      "loss": 0.7107,
      "step": 35940
    },
    {
      "epoch": 1.419433805819876,
      "grad_norm": 0.7911181449890137,
      "learning_rate": 2.9030283886761167e-05,
      "loss": 0.7255,
      "step": 35950
    },
    {
      "epoch": 1.4198286413708692,
      "grad_norm": 0.7241342067718506,
      "learning_rate": 2.9010542109211513e-05,
      "loss": 0.7176,
      "step": 35960
    },
    {
      "epoch": 1.420223476921862,
      "grad_norm": 0.5638464093208313,
      "learning_rate": 2.8990800331661866e-05,
      "loss": 0.6999,
      "step": 35970
    },
    {
      "epoch": 1.420618312472855,
      "grad_norm": 0.950814425945282,
      "learning_rate": 2.8971058554112213e-05,
      "loss": 0.7046,
      "step": 35980
    },
    {
      "epoch": 1.4210131480238481,
      "grad_norm": 0.6903448104858398,
      "learning_rate": 2.8951316776562566e-05,
      "loss": 0.7566,
      "step": 35990
    },
    {
      "epoch": 1.421407983574841,
      "grad_norm": 0.8911612629890442,
      "learning_rate": 2.8931574999012913e-05,
      "loss": 0.7356,
      "step": 36000
    },
    {
      "epoch": 1.421407983574841,
      "eval_loss": 0.7644989490509033,
      "eval_runtime": 1219.4593,
      "eval_samples_per_second": 9.231,
      "eval_steps_per_second": 9.231,
      "step": 36000
    },
    {
      "epoch": 1.421802819125834,
      "grad_norm": 0.7981581091880798,
      "learning_rate": 2.8911833221463263e-05,
      "loss": 0.7231,
      "step": 36010
    },
    {
      "epoch": 1.4221976546768271,
      "grad_norm": 0.6718114018440247,
      "learning_rate": 2.889209144391361e-05,
      "loss": 0.7252,
      "step": 36020
    },
    {
      "epoch": 1.4225924902278202,
      "grad_norm": 0.9071598052978516,
      "learning_rate": 2.8872349666363962e-05,
      "loss": 0.7138,
      "step": 36030
    },
    {
      "epoch": 1.4229873257788133,
      "grad_norm": 0.8206009864807129,
      "learning_rate": 2.885260788881431e-05,
      "loss": 0.7514,
      "step": 36040
    },
    {
      "epoch": 1.423382161329806,
      "grad_norm": 0.8335214853286743,
      "learning_rate": 2.8832866111264662e-05,
      "loss": 0.7227,
      "step": 36050
    },
    {
      "epoch": 1.4237769968807992,
      "grad_norm": 0.9703687429428101,
      "learning_rate": 2.881312433371501e-05,
      "loss": 0.7132,
      "step": 36060
    },
    {
      "epoch": 1.4241718324317922,
      "grad_norm": 0.9170871376991272,
      "learning_rate": 2.879338255616536e-05,
      "loss": 0.7503,
      "step": 36070
    },
    {
      "epoch": 1.424566667982785,
      "grad_norm": 0.9735654592514038,
      "learning_rate": 2.8773640778615705e-05,
      "loss": 0.6967,
      "step": 36080
    },
    {
      "epoch": 1.4249615035337782,
      "grad_norm": 0.7283927202224731,
      "learning_rate": 2.875389900106606e-05,
      "loss": 0.7111,
      "step": 36090
    },
    {
      "epoch": 1.4253563390847712,
      "grad_norm": 0.7621373534202576,
      "learning_rate": 2.8734157223516405e-05,
      "loss": 0.688,
      "step": 36100
    },
    {
      "epoch": 1.4257511746357643,
      "grad_norm": 0.7644574642181396,
      "learning_rate": 2.871441544596676e-05,
      "loss": 0.6616,
      "step": 36110
    },
    {
      "epoch": 1.4261460101867574,
      "grad_norm": 0.5059332847595215,
      "learning_rate": 2.8694673668417105e-05,
      "loss": 0.715,
      "step": 36120
    },
    {
      "epoch": 1.4265408457377502,
      "grad_norm": 0.6510090827941895,
      "learning_rate": 2.8674931890867458e-05,
      "loss": 0.7577,
      "step": 36130
    },
    {
      "epoch": 1.4269356812887433,
      "grad_norm": 0.7217729091644287,
      "learning_rate": 2.8655190113317805e-05,
      "loss": 0.7099,
      "step": 36140
    },
    {
      "epoch": 1.4273305168397363,
      "grad_norm": 0.8747050762176514,
      "learning_rate": 2.8635448335768155e-05,
      "loss": 0.6921,
      "step": 36150
    },
    {
      "epoch": 1.4277253523907292,
      "grad_norm": 0.7204887866973877,
      "learning_rate": 2.86157065582185e-05,
      "loss": 0.7286,
      "step": 36160
    },
    {
      "epoch": 1.4281201879417222,
      "grad_norm": 0.8990779519081116,
      "learning_rate": 2.8595964780668855e-05,
      "loss": 0.7104,
      "step": 36170
    },
    {
      "epoch": 1.4285150234927153,
      "grad_norm": 0.7472249865531921,
      "learning_rate": 2.85762230031192e-05,
      "loss": 0.7143,
      "step": 36180
    },
    {
      "epoch": 1.4289098590437084,
      "grad_norm": 0.9451598525047302,
      "learning_rate": 2.8556481225569554e-05,
      "loss": 0.7268,
      "step": 36190
    },
    {
      "epoch": 1.4293046945947014,
      "grad_norm": 0.8923260569572449,
      "learning_rate": 2.85367394480199e-05,
      "loss": 0.7356,
      "step": 36200
    },
    {
      "epoch": 1.4296995301456943,
      "grad_norm": 0.8436161279678345,
      "learning_rate": 2.851699767047025e-05,
      "loss": 0.711,
      "step": 36210
    },
    {
      "epoch": 1.4300943656966874,
      "grad_norm": 0.8402449488639832,
      "learning_rate": 2.84972558929206e-05,
      "loss": 0.6817,
      "step": 36220
    },
    {
      "epoch": 1.4304892012476804,
      "grad_norm": 0.8332924842834473,
      "learning_rate": 2.847751411537095e-05,
      "loss": 0.6973,
      "step": 36230
    },
    {
      "epoch": 1.4308840367986733,
      "grad_norm": 0.8112673759460449,
      "learning_rate": 2.8457772337821297e-05,
      "loss": 0.696,
      "step": 36240
    },
    {
      "epoch": 1.4312788723496663,
      "grad_norm": 0.6593031287193298,
      "learning_rate": 2.843803056027165e-05,
      "loss": 0.6977,
      "step": 36250
    },
    {
      "epoch": 1.4316737079006594,
      "grad_norm": 1.0436934232711792,
      "learning_rate": 2.8418288782721997e-05,
      "loss": 0.7285,
      "step": 36260
    },
    {
      "epoch": 1.4320685434516525,
      "grad_norm": 0.7728669047355652,
      "learning_rate": 2.839854700517235e-05,
      "loss": 0.7529,
      "step": 36270
    },
    {
      "epoch": 1.4324633790026455,
      "grad_norm": 0.7438809871673584,
      "learning_rate": 2.8378805227622697e-05,
      "loss": 0.736,
      "step": 36280
    },
    {
      "epoch": 1.4328582145536384,
      "grad_norm": 0.9594649076461792,
      "learning_rate": 2.8359063450073047e-05,
      "loss": 0.6881,
      "step": 36290
    },
    {
      "epoch": 1.4332530501046314,
      "grad_norm": 0.8887708783149719,
      "learning_rate": 2.8339321672523393e-05,
      "loss": 0.7151,
      "step": 36300
    },
    {
      "epoch": 1.4336478856556245,
      "grad_norm": 0.8536302447319031,
      "learning_rate": 2.8319579894973747e-05,
      "loss": 0.7488,
      "step": 36310
    },
    {
      "epoch": 1.4340427212066174,
      "grad_norm": 0.7369965314865112,
      "learning_rate": 2.8299838117424093e-05,
      "loss": 0.6811,
      "step": 36320
    },
    {
      "epoch": 1.4344375567576104,
      "grad_norm": 0.760147750377655,
      "learning_rate": 2.8280096339874446e-05,
      "loss": 0.6525,
      "step": 36330
    },
    {
      "epoch": 1.4348323923086035,
      "grad_norm": 0.6683061718940735,
      "learning_rate": 2.8260354562324793e-05,
      "loss": 0.7201,
      "step": 36340
    },
    {
      "epoch": 1.4352272278595966,
      "grad_norm": 0.8198908567428589,
      "learning_rate": 2.8240612784775146e-05,
      "loss": 0.7391,
      "step": 36350
    },
    {
      "epoch": 1.4356220634105896,
      "grad_norm": 0.6355031728744507,
      "learning_rate": 2.8220871007225493e-05,
      "loss": 0.6888,
      "step": 36360
    },
    {
      "epoch": 1.4360168989615825,
      "grad_norm": 0.8611272573471069,
      "learning_rate": 2.8201129229675843e-05,
      "loss": 0.7185,
      "step": 36370
    },
    {
      "epoch": 1.4364117345125755,
      "grad_norm": 0.8926335573196411,
      "learning_rate": 2.818138745212619e-05,
      "loss": 0.7554,
      "step": 36380
    },
    {
      "epoch": 1.4368065700635686,
      "grad_norm": 1.22632896900177,
      "learning_rate": 2.8161645674576542e-05,
      "loss": 0.7387,
      "step": 36390
    },
    {
      "epoch": 1.4372014056145614,
      "grad_norm": 0.8032490611076355,
      "learning_rate": 2.814190389702689e-05,
      "loss": 0.6887,
      "step": 36400
    },
    {
      "epoch": 1.4375962411655545,
      "grad_norm": 0.7015951871871948,
      "learning_rate": 2.8122162119477242e-05,
      "loss": 0.6894,
      "step": 36410
    },
    {
      "epoch": 1.4379910767165476,
      "grad_norm": 0.7789873480796814,
      "learning_rate": 2.810242034192759e-05,
      "loss": 0.6676,
      "step": 36420
    },
    {
      "epoch": 1.4383859122675406,
      "grad_norm": 0.7915650606155396,
      "learning_rate": 2.808267856437794e-05,
      "loss": 0.6911,
      "step": 36430
    },
    {
      "epoch": 1.4387807478185337,
      "grad_norm": 0.7437384128570557,
      "learning_rate": 2.8062936786828285e-05,
      "loss": 0.692,
      "step": 36440
    },
    {
      "epoch": 1.4391755833695266,
      "grad_norm": 0.9199157953262329,
      "learning_rate": 2.804319500927864e-05,
      "loss": 0.6738,
      "step": 36450
    },
    {
      "epoch": 1.4395704189205196,
      "grad_norm": 0.7735220789909363,
      "learning_rate": 2.8023453231728985e-05,
      "loss": 0.6676,
      "step": 36460
    },
    {
      "epoch": 1.4399652544715127,
      "grad_norm": 0.9998920559883118,
      "learning_rate": 2.800371145417934e-05,
      "loss": 0.7237,
      "step": 36470
    },
    {
      "epoch": 1.4403600900225055,
      "grad_norm": 0.7186641097068787,
      "learning_rate": 2.7983969676629685e-05,
      "loss": 0.6846,
      "step": 36480
    },
    {
      "epoch": 1.4407549255734986,
      "grad_norm": 0.8564705848693848,
      "learning_rate": 2.7964227899080038e-05,
      "loss": 0.7202,
      "step": 36490
    },
    {
      "epoch": 1.4411497611244917,
      "grad_norm": 0.7851531505584717,
      "learning_rate": 2.7944486121530385e-05,
      "loss": 0.6863,
      "step": 36500
    },
    {
      "epoch": 1.4415445966754847,
      "grad_norm": 0.7036353349685669,
      "learning_rate": 2.7924744343980735e-05,
      "loss": 0.7084,
      "step": 36510
    },
    {
      "epoch": 1.4419394322264778,
      "grad_norm": 0.9994798302650452,
      "learning_rate": 2.790500256643108e-05,
      "loss": 0.7354,
      "step": 36520
    },
    {
      "epoch": 1.4423342677774706,
      "grad_norm": 0.9664022326469421,
      "learning_rate": 2.7885260788881434e-05,
      "loss": 0.6867,
      "step": 36530
    },
    {
      "epoch": 1.4427291033284637,
      "grad_norm": 0.8070870041847229,
      "learning_rate": 2.786551901133178e-05,
      "loss": 0.7102,
      "step": 36540
    },
    {
      "epoch": 1.4431239388794568,
      "grad_norm": 0.9297584295272827,
      "learning_rate": 2.7845777233782134e-05,
      "loss": 0.7385,
      "step": 36550
    },
    {
      "epoch": 1.4435187744304496,
      "grad_norm": 0.7246711254119873,
      "learning_rate": 2.782603545623248e-05,
      "loss": 0.7505,
      "step": 36560
    },
    {
      "epoch": 1.4439136099814427,
      "grad_norm": 0.6155511736869812,
      "learning_rate": 2.780629367868283e-05,
      "loss": 0.6973,
      "step": 36570
    },
    {
      "epoch": 1.4443084455324358,
      "grad_norm": 0.75121009349823,
      "learning_rate": 2.7786551901133177e-05,
      "loss": 0.7216,
      "step": 36580
    },
    {
      "epoch": 1.4447032810834288,
      "grad_norm": 0.7595940232276917,
      "learning_rate": 2.776681012358353e-05,
      "loss": 0.7214,
      "step": 36590
    },
    {
      "epoch": 1.445098116634422,
      "grad_norm": 0.9228445291519165,
      "learning_rate": 2.7747068346033877e-05,
      "loss": 0.7357,
      "step": 36600
    },
    {
      "epoch": 1.4454929521854147,
      "grad_norm": 0.6609699726104736,
      "learning_rate": 2.772732656848423e-05,
      "loss": 0.7172,
      "step": 36610
    },
    {
      "epoch": 1.4458877877364078,
      "grad_norm": 0.8238226771354675,
      "learning_rate": 2.7707584790934577e-05,
      "loss": 0.659,
      "step": 36620
    },
    {
      "epoch": 1.4462826232874009,
      "grad_norm": 0.7459299564361572,
      "learning_rate": 2.768784301338493e-05,
      "loss": 0.7219,
      "step": 36630
    },
    {
      "epoch": 1.4466774588383937,
      "grad_norm": 0.846617579460144,
      "learning_rate": 2.7668101235835277e-05,
      "loss": 0.723,
      "step": 36640
    },
    {
      "epoch": 1.4470722943893868,
      "grad_norm": 1.0091378688812256,
      "learning_rate": 2.7648359458285627e-05,
      "loss": 0.7271,
      "step": 36650
    },
    {
      "epoch": 1.4474671299403798,
      "grad_norm": 0.7934256792068481,
      "learning_rate": 2.7628617680735973e-05,
      "loss": 0.6826,
      "step": 36660
    },
    {
      "epoch": 1.447861965491373,
      "grad_norm": 0.8484697937965393,
      "learning_rate": 2.7608875903186326e-05,
      "loss": 0.703,
      "step": 36670
    },
    {
      "epoch": 1.448256801042366,
      "grad_norm": 0.6938184499740601,
      "learning_rate": 2.7589134125636673e-05,
      "loss": 0.6763,
      "step": 36680
    },
    {
      "epoch": 1.4486516365933588,
      "grad_norm": 0.7610655426979065,
      "learning_rate": 2.7569392348087026e-05,
      "loss": 0.708,
      "step": 36690
    },
    {
      "epoch": 1.449046472144352,
      "grad_norm": 0.7874404788017273,
      "learning_rate": 2.7549650570537373e-05,
      "loss": 0.72,
      "step": 36700
    },
    {
      "epoch": 1.449441307695345,
      "grad_norm": 0.8482788801193237,
      "learning_rate": 2.7529908792987723e-05,
      "loss": 0.7365,
      "step": 36710
    },
    {
      "epoch": 1.4498361432463378,
      "grad_norm": 0.6775288581848145,
      "learning_rate": 2.751016701543807e-05,
      "loss": 0.6915,
      "step": 36720
    },
    {
      "epoch": 1.4502309787973309,
      "grad_norm": 0.8417578935623169,
      "learning_rate": 2.7490425237888423e-05,
      "loss": 0.7229,
      "step": 36730
    },
    {
      "epoch": 1.450625814348324,
      "grad_norm": 0.6812112331390381,
      "learning_rate": 2.747068346033877e-05,
      "loss": 0.7406,
      "step": 36740
    },
    {
      "epoch": 1.451020649899317,
      "grad_norm": 0.6436556577682495,
      "learning_rate": 2.7450941682789122e-05,
      "loss": 0.7097,
      "step": 36750
    },
    {
      "epoch": 1.45141548545031,
      "grad_norm": 0.8222054839134216,
      "learning_rate": 2.743119990523947e-05,
      "loss": 0.7134,
      "step": 36760
    },
    {
      "epoch": 1.451810321001303,
      "grad_norm": 0.7462788224220276,
      "learning_rate": 2.7411458127689822e-05,
      "loss": 0.6992,
      "step": 36770
    },
    {
      "epoch": 1.452205156552296,
      "grad_norm": 0.8029965162277222,
      "learning_rate": 2.739171635014017e-05,
      "loss": 0.6597,
      "step": 36780
    },
    {
      "epoch": 1.452599992103289,
      "grad_norm": 0.6565721035003662,
      "learning_rate": 2.737197457259052e-05,
      "loss": 0.7258,
      "step": 36790
    },
    {
      "epoch": 1.452994827654282,
      "grad_norm": 0.8689293265342712,
      "learning_rate": 2.7352232795040865e-05,
      "loss": 0.7243,
      "step": 36800
    },
    {
      "epoch": 1.453389663205275,
      "grad_norm": 0.6868607401847839,
      "learning_rate": 2.733249101749122e-05,
      "loss": 0.6974,
      "step": 36810
    },
    {
      "epoch": 1.453784498756268,
      "grad_norm": 0.5866960287094116,
      "learning_rate": 2.7312749239941565e-05,
      "loss": 0.7329,
      "step": 36820
    },
    {
      "epoch": 1.454179334307261,
      "grad_norm": 0.746660590171814,
      "learning_rate": 2.729300746239192e-05,
      "loss": 0.6434,
      "step": 36830
    },
    {
      "epoch": 1.4545741698582542,
      "grad_norm": 0.7863873839378357,
      "learning_rate": 2.7273265684842265e-05,
      "loss": 0.709,
      "step": 36840
    },
    {
      "epoch": 1.454969005409247,
      "grad_norm": 0.6828038096427917,
      "learning_rate": 2.7253523907292615e-05,
      "loss": 0.7046,
      "step": 36850
    },
    {
      "epoch": 1.45536384096024,
      "grad_norm": 0.7108086347579956,
      "learning_rate": 2.723378212974296e-05,
      "loss": 0.6737,
      "step": 36860
    },
    {
      "epoch": 1.4557586765112331,
      "grad_norm": 0.7143807411193848,
      "learning_rate": 2.7214040352193315e-05,
      "loss": 0.6885,
      "step": 36870
    },
    {
      "epoch": 1.456153512062226,
      "grad_norm": 0.699016273021698,
      "learning_rate": 2.719429857464366e-05,
      "loss": 0.7065,
      "step": 36880
    },
    {
      "epoch": 1.456548347613219,
      "grad_norm": 0.8383671641349792,
      "learning_rate": 2.7174556797094014e-05,
      "loss": 0.7146,
      "step": 36890
    },
    {
      "epoch": 1.4569431831642121,
      "grad_norm": 0.7211551070213318,
      "learning_rate": 2.715481501954436e-05,
      "loss": 0.7409,
      "step": 36900
    },
    {
      "epoch": 1.4573380187152052,
      "grad_norm": 0.8612214922904968,
      "learning_rate": 2.7135073241994714e-05,
      "loss": 0.7114,
      "step": 36910
    },
    {
      "epoch": 1.4577328542661983,
      "grad_norm": 0.7769003510475159,
      "learning_rate": 2.711533146444506e-05,
      "loss": 0.6916,
      "step": 36920
    },
    {
      "epoch": 1.458127689817191,
      "grad_norm": 0.8640367388725281,
      "learning_rate": 2.709558968689541e-05,
      "loss": 0.739,
      "step": 36930
    },
    {
      "epoch": 1.4585225253681842,
      "grad_norm": 0.7851678133010864,
      "learning_rate": 2.7075847909345757e-05,
      "loss": 0.6808,
      "step": 36940
    },
    {
      "epoch": 1.4589173609191772,
      "grad_norm": 0.7135913372039795,
      "learning_rate": 2.705610613179611e-05,
      "loss": 0.7462,
      "step": 36950
    },
    {
      "epoch": 1.45931219647017,
      "grad_norm": 0.7927341461181641,
      "learning_rate": 2.7036364354246457e-05,
      "loss": 0.7078,
      "step": 36960
    },
    {
      "epoch": 1.4597070320211631,
      "grad_norm": 0.8634494543075562,
      "learning_rate": 2.701662257669681e-05,
      "loss": 0.6986,
      "step": 36970
    },
    {
      "epoch": 1.4601018675721562,
      "grad_norm": 0.8977263569831848,
      "learning_rate": 2.6996880799147157e-05,
      "loss": 0.6939,
      "step": 36980
    },
    {
      "epoch": 1.4604967031231493,
      "grad_norm": 0.9001408815383911,
      "learning_rate": 2.6977139021597507e-05,
      "loss": 0.7105,
      "step": 36990
    },
    {
      "epoch": 1.4608915386741423,
      "grad_norm": 0.7582162618637085,
      "learning_rate": 2.6957397244047857e-05,
      "loss": 0.7162,
      "step": 37000
    },
    {
      "epoch": 1.4608915386741423,
      "eval_loss": 0.7635016441345215,
      "eval_runtime": 1216.6718,
      "eval_samples_per_second": 9.252,
      "eval_steps_per_second": 9.252,
      "step": 37000
    },
    {
      "epoch": 1.4612863742251352,
      "grad_norm": 0.9396231770515442,
      "learning_rate": 2.6937655466498207e-05,
      "loss": 0.6981,
      "step": 37010
    },
    {
      "epoch": 1.4616812097761283,
      "grad_norm": 0.7883409261703491,
      "learning_rate": 2.6917913688948553e-05,
      "loss": 0.7042,
      "step": 37020
    },
    {
      "epoch": 1.4620760453271213,
      "grad_norm": 0.7728544473648071,
      "learning_rate": 2.6898171911398906e-05,
      "loss": 0.7091,
      "step": 37030
    },
    {
      "epoch": 1.4624708808781142,
      "grad_norm": 1.0589427947998047,
      "learning_rate": 2.6878430133849253e-05,
      "loss": 0.7004,
      "step": 37040
    },
    {
      "epoch": 1.4628657164291072,
      "grad_norm": 0.7605564594268799,
      "learning_rate": 2.6858688356299606e-05,
      "loss": 0.7251,
      "step": 37050
    },
    {
      "epoch": 1.4632605519801003,
      "grad_norm": 0.8028787970542908,
      "learning_rate": 2.6838946578749953e-05,
      "loss": 0.7134,
      "step": 37060
    },
    {
      "epoch": 1.4636553875310934,
      "grad_norm": 0.9951236248016357,
      "learning_rate": 2.6819204801200303e-05,
      "loss": 0.7059,
      "step": 37070
    },
    {
      "epoch": 1.4640502230820864,
      "grad_norm": 0.7122153043746948,
      "learning_rate": 2.679946302365065e-05,
      "loss": 0.6995,
      "step": 37080
    },
    {
      "epoch": 1.4644450586330793,
      "grad_norm": 1.0107358694076538,
      "learning_rate": 2.6779721246101003e-05,
      "loss": 0.7105,
      "step": 37090
    },
    {
      "epoch": 1.4648398941840723,
      "grad_norm": 0.768812358379364,
      "learning_rate": 2.675997946855135e-05,
      "loss": 0.6965,
      "step": 37100
    },
    {
      "epoch": 1.4652347297350654,
      "grad_norm": 0.891431987285614,
      "learning_rate": 2.6740237691001702e-05,
      "loss": 0.7303,
      "step": 37110
    },
    {
      "epoch": 1.4656295652860583,
      "grad_norm": 1.0496618747711182,
      "learning_rate": 2.672049591345205e-05,
      "loss": 0.6637,
      "step": 37120
    },
    {
      "epoch": 1.4660244008370513,
      "grad_norm": 0.7957670092582703,
      "learning_rate": 2.67007541359024e-05,
      "loss": 0.7019,
      "step": 37130
    },
    {
      "epoch": 1.4664192363880444,
      "grad_norm": 0.766451895236969,
      "learning_rate": 2.668101235835275e-05,
      "loss": 0.692,
      "step": 37140
    },
    {
      "epoch": 1.4668140719390375,
      "grad_norm": 0.7571955919265747,
      "learning_rate": 2.66612705808031e-05,
      "loss": 0.7465,
      "step": 37150
    },
    {
      "epoch": 1.4672089074900305,
      "grad_norm": 0.6951565146446228,
      "learning_rate": 2.6641528803253445e-05,
      "loss": 0.6908,
      "step": 37160
    },
    {
      "epoch": 1.4676037430410234,
      "grad_norm": 0.6413501501083374,
      "learning_rate": 2.66217870257038e-05,
      "loss": 0.7247,
      "step": 37170
    },
    {
      "epoch": 1.4679985785920164,
      "grad_norm": 0.7649120688438416,
      "learning_rate": 2.6602045248154145e-05,
      "loss": 0.6827,
      "step": 37180
    },
    {
      "epoch": 1.4683934141430095,
      "grad_norm": 0.8782073259353638,
      "learning_rate": 2.6582303470604498e-05,
      "loss": 0.7102,
      "step": 37190
    },
    {
      "epoch": 1.4687882496940023,
      "grad_norm": 0.7087144255638123,
      "learning_rate": 2.6562561693054845e-05,
      "loss": 0.688,
      "step": 37200
    },
    {
      "epoch": 1.4691830852449954,
      "grad_norm": 0.7982509732246399,
      "learning_rate": 2.6542819915505195e-05,
      "loss": 0.679,
      "step": 37210
    },
    {
      "epoch": 1.4695779207959885,
      "grad_norm": 0.7586053013801575,
      "learning_rate": 2.652307813795554e-05,
      "loss": 0.7204,
      "step": 37220
    },
    {
      "epoch": 1.4699727563469815,
      "grad_norm": 0.9441350102424622,
      "learning_rate": 2.6503336360405895e-05,
      "loss": 0.7071,
      "step": 37230
    },
    {
      "epoch": 1.4703675918979746,
      "grad_norm": 0.8835339546203613,
      "learning_rate": 2.648359458285624e-05,
      "loss": 0.7093,
      "step": 37240
    },
    {
      "epoch": 1.4707624274489675,
      "grad_norm": 0.6140122413635254,
      "learning_rate": 2.6463852805306594e-05,
      "loss": 0.7306,
      "step": 37250
    },
    {
      "epoch": 1.4711572629999605,
      "grad_norm": 0.8601051568984985,
      "learning_rate": 2.644411102775694e-05,
      "loss": 0.7127,
      "step": 37260
    },
    {
      "epoch": 1.4715520985509536,
      "grad_norm": 0.7686482667922974,
      "learning_rate": 2.6424369250207294e-05,
      "loss": 0.6993,
      "step": 37270
    },
    {
      "epoch": 1.4719469341019464,
      "grad_norm": 0.7764801979064941,
      "learning_rate": 2.640462747265764e-05,
      "loss": 0.6977,
      "step": 37280
    },
    {
      "epoch": 1.4723417696529395,
      "grad_norm": 0.7829206585884094,
      "learning_rate": 2.638488569510799e-05,
      "loss": 0.6823,
      "step": 37290
    },
    {
      "epoch": 1.4727366052039326,
      "grad_norm": 0.7315250635147095,
      "learning_rate": 2.6365143917558337e-05,
      "loss": 0.7119,
      "step": 37300
    },
    {
      "epoch": 1.4731314407549256,
      "grad_norm": 0.675297737121582,
      "learning_rate": 2.634540214000869e-05,
      "loss": 0.7061,
      "step": 37310
    },
    {
      "epoch": 1.4735262763059187,
      "grad_norm": 0.734006404876709,
      "learning_rate": 2.6325660362459037e-05,
      "loss": 0.6845,
      "step": 37320
    },
    {
      "epoch": 1.4739211118569115,
      "grad_norm": 0.8817944526672363,
      "learning_rate": 2.630591858490939e-05,
      "loss": 0.7133,
      "step": 37330
    },
    {
      "epoch": 1.4743159474079046,
      "grad_norm": 0.7685514092445374,
      "learning_rate": 2.6286176807359737e-05,
      "loss": 0.6999,
      "step": 37340
    },
    {
      "epoch": 1.4747107829588977,
      "grad_norm": 0.7801794409751892,
      "learning_rate": 2.6266435029810087e-05,
      "loss": 0.7159,
      "step": 37350
    },
    {
      "epoch": 1.4751056185098905,
      "grad_norm": 0.7753512859344482,
      "learning_rate": 2.6246693252260433e-05,
      "loss": 0.7019,
      "step": 37360
    },
    {
      "epoch": 1.4755004540608836,
      "grad_norm": 0.9854361414909363,
      "learning_rate": 2.6226951474710787e-05,
      "loss": 0.7019,
      "step": 37370
    },
    {
      "epoch": 1.4758952896118767,
      "grad_norm": 0.744770884513855,
      "learning_rate": 2.6207209697161133e-05,
      "loss": 0.7463,
      "step": 37380
    },
    {
      "epoch": 1.4762901251628697,
      "grad_norm": 0.7095930576324463,
      "learning_rate": 2.6187467919611486e-05,
      "loss": 0.7416,
      "step": 37390
    },
    {
      "epoch": 1.4766849607138628,
      "grad_norm": 0.9955053925514221,
      "learning_rate": 2.6167726142061833e-05,
      "loss": 0.7297,
      "step": 37400
    },
    {
      "epoch": 1.4770797962648556,
      "grad_norm": 0.6941370964050293,
      "learning_rate": 2.6147984364512186e-05,
      "loss": 0.6793,
      "step": 37410
    },
    {
      "epoch": 1.4774746318158487,
      "grad_norm": 0.7024986147880554,
      "learning_rate": 2.6128242586962533e-05,
      "loss": 0.7205,
      "step": 37420
    },
    {
      "epoch": 1.4778694673668418,
      "grad_norm": 0.6981072425842285,
      "learning_rate": 2.610850080941288e-05,
      "loss": 0.6984,
      "step": 37430
    },
    {
      "epoch": 1.4782643029178346,
      "grad_norm": 0.6987897157669067,
      "learning_rate": 2.608875903186323e-05,
      "loss": 0.6969,
      "step": 37440
    },
    {
      "epoch": 1.4786591384688277,
      "grad_norm": 0.9467880725860596,
      "learning_rate": 2.6069017254313576e-05,
      "loss": 0.7482,
      "step": 37450
    },
    {
      "epoch": 1.4790539740198207,
      "grad_norm": 0.848935604095459,
      "learning_rate": 2.604927547676393e-05,
      "loss": 0.706,
      "step": 37460
    },
    {
      "epoch": 1.4794488095708138,
      "grad_norm": 0.8833354711532593,
      "learning_rate": 2.6029533699214276e-05,
      "loss": 0.7287,
      "step": 37470
    },
    {
      "epoch": 1.4798436451218069,
      "grad_norm": 0.6010631918907166,
      "learning_rate": 2.600979192166463e-05,
      "loss": 0.6784,
      "step": 37480
    },
    {
      "epoch": 1.4802384806727997,
      "grad_norm": 0.7758013010025024,
      "learning_rate": 2.5990050144114975e-05,
      "loss": 0.7294,
      "step": 37490
    },
    {
      "epoch": 1.4806333162237928,
      "grad_norm": 0.9011154174804688,
      "learning_rate": 2.5970308366565325e-05,
      "loss": 0.7203,
      "step": 37500
    },
    {
      "epoch": 1.4810281517747859,
      "grad_norm": 0.6926453113555908,
      "learning_rate": 2.5950566589015672e-05,
      "loss": 0.6975,
      "step": 37510
    },
    {
      "epoch": 1.4814229873257787,
      "grad_norm": 0.7581200003623962,
      "learning_rate": 2.5930824811466025e-05,
      "loss": 0.7012,
      "step": 37520
    },
    {
      "epoch": 1.4818178228767718,
      "grad_norm": 0.7405800819396973,
      "learning_rate": 2.591108303391637e-05,
      "loss": 0.6962,
      "step": 37530
    },
    {
      "epoch": 1.4822126584277648,
      "grad_norm": 1.0089720487594604,
      "learning_rate": 2.5891341256366725e-05,
      "loss": 0.7147,
      "step": 37540
    },
    {
      "epoch": 1.482607493978758,
      "grad_norm": 1.083894968032837,
      "learning_rate": 2.587159947881707e-05,
      "loss": 0.7249,
      "step": 37550
    },
    {
      "epoch": 1.483002329529751,
      "grad_norm": 0.9432200193405151,
      "learning_rate": 2.5851857701267425e-05,
      "loss": 0.7171,
      "step": 37560
    },
    {
      "epoch": 1.4833971650807438,
      "grad_norm": 0.7600322365760803,
      "learning_rate": 2.583211592371777e-05,
      "loss": 0.6777,
      "step": 37570
    },
    {
      "epoch": 1.4837920006317369,
      "grad_norm": 0.725337028503418,
      "learning_rate": 2.581237414616812e-05,
      "loss": 0.7129,
      "step": 37580
    },
    {
      "epoch": 1.48418683618273,
      "grad_norm": 0.6215822100639343,
      "learning_rate": 2.5792632368618468e-05,
      "loss": 0.7439,
      "step": 37590
    },
    {
      "epoch": 1.4845816717337228,
      "grad_norm": 0.7669187784194946,
      "learning_rate": 2.577289059106882e-05,
      "loss": 0.7092,
      "step": 37600
    },
    {
      "epoch": 1.4849765072847159,
      "grad_norm": 0.6850118041038513,
      "learning_rate": 2.5753148813519168e-05,
      "loss": 0.7388,
      "step": 37610
    },
    {
      "epoch": 1.485371342835709,
      "grad_norm": 0.6710284352302551,
      "learning_rate": 2.573340703596952e-05,
      "loss": 0.7108,
      "step": 37620
    },
    {
      "epoch": 1.485766178386702,
      "grad_norm": 0.7558144927024841,
      "learning_rate": 2.5713665258419867e-05,
      "loss": 0.7355,
      "step": 37630
    },
    {
      "epoch": 1.486161013937695,
      "grad_norm": 0.8161376118659973,
      "learning_rate": 2.5693923480870217e-05,
      "loss": 0.7086,
      "step": 37640
    },
    {
      "epoch": 1.486555849488688,
      "grad_norm": 0.7956558465957642,
      "learning_rate": 2.5674181703320567e-05,
      "loss": 0.6896,
      "step": 37650
    },
    {
      "epoch": 1.486950685039681,
      "grad_norm": 0.6578824520111084,
      "learning_rate": 2.5654439925770917e-05,
      "loss": 0.6848,
      "step": 37660
    },
    {
      "epoch": 1.487345520590674,
      "grad_norm": 0.845786988735199,
      "learning_rate": 2.5634698148221264e-05,
      "loss": 0.7059,
      "step": 37670
    },
    {
      "epoch": 1.4877403561416669,
      "grad_norm": 0.6754435300827026,
      "learning_rate": 2.5614956370671617e-05,
      "loss": 0.703,
      "step": 37680
    },
    {
      "epoch": 1.48813519169266,
      "grad_norm": 0.7971127033233643,
      "learning_rate": 2.5595214593121963e-05,
      "loss": 0.6859,
      "step": 37690
    },
    {
      "epoch": 1.488530027243653,
      "grad_norm": 0.7574929594993591,
      "learning_rate": 2.5575472815572317e-05,
      "loss": 0.7572,
      "step": 37700
    },
    {
      "epoch": 1.488924862794646,
      "grad_norm": 0.8313413858413696,
      "learning_rate": 2.5555731038022663e-05,
      "loss": 0.7213,
      "step": 37710
    },
    {
      "epoch": 1.4893196983456392,
      "grad_norm": 0.7860494256019592,
      "learning_rate": 2.5535989260473013e-05,
      "loss": 0.697,
      "step": 37720
    },
    {
      "epoch": 1.489714533896632,
      "grad_norm": 0.672663688659668,
      "learning_rate": 2.551624748292336e-05,
      "loss": 0.7059,
      "step": 37730
    },
    {
      "epoch": 1.490109369447625,
      "grad_norm": 0.9783653616905212,
      "learning_rate": 2.5496505705373713e-05,
      "loss": 0.715,
      "step": 37740
    },
    {
      "epoch": 1.4905042049986181,
      "grad_norm": 0.7627424597740173,
      "learning_rate": 2.547676392782406e-05,
      "loss": 0.7083,
      "step": 37750
    },
    {
      "epoch": 1.490899040549611,
      "grad_norm": 0.6132835149765015,
      "learning_rate": 2.5457022150274413e-05,
      "loss": 0.7205,
      "step": 37760
    },
    {
      "epoch": 1.491293876100604,
      "grad_norm": 0.6694920063018799,
      "learning_rate": 2.543728037272476e-05,
      "loss": 0.6902,
      "step": 37770
    },
    {
      "epoch": 1.491688711651597,
      "grad_norm": 0.7386063933372498,
      "learning_rate": 2.5417538595175113e-05,
      "loss": 0.6993,
      "step": 37780
    },
    {
      "epoch": 1.4920835472025902,
      "grad_norm": 0.7063804864883423,
      "learning_rate": 2.539779681762546e-05,
      "loss": 0.7087,
      "step": 37790
    },
    {
      "epoch": 1.4924783827535832,
      "grad_norm": 0.9812520146369934,
      "learning_rate": 2.537805504007581e-05,
      "loss": 0.6862,
      "step": 37800
    },
    {
      "epoch": 1.492873218304576,
      "grad_norm": 0.9726914763450623,
      "learning_rate": 2.5358313262526156e-05,
      "loss": 0.7082,
      "step": 37810
    },
    {
      "epoch": 1.4932680538555692,
      "grad_norm": 0.7082109451293945,
      "learning_rate": 2.533857148497651e-05,
      "loss": 0.7132,
      "step": 37820
    },
    {
      "epoch": 1.4936628894065622,
      "grad_norm": 0.7395367622375488,
      "learning_rate": 2.5318829707426856e-05,
      "loss": 0.7335,
      "step": 37830
    },
    {
      "epoch": 1.494057724957555,
      "grad_norm": 0.8686916828155518,
      "learning_rate": 2.529908792987721e-05,
      "loss": 0.7173,
      "step": 37840
    },
    {
      "epoch": 1.4944525605085481,
      "grad_norm": 0.9304060935974121,
      "learning_rate": 2.5279346152327555e-05,
      "loss": 0.6932,
      "step": 37850
    },
    {
      "epoch": 1.4948473960595412,
      "grad_norm": 0.866910994052887,
      "learning_rate": 2.5259604374777905e-05,
      "loss": 0.6868,
      "step": 37860
    },
    {
      "epoch": 1.4952422316105343,
      "grad_norm": 0.8415698409080505,
      "learning_rate": 2.5239862597228252e-05,
      "loss": 0.7108,
      "step": 37870
    },
    {
      "epoch": 1.4956370671615273,
      "grad_norm": 0.7572310566902161,
      "learning_rate": 2.5220120819678605e-05,
      "loss": 0.729,
      "step": 37880
    },
    {
      "epoch": 1.4960319027125202,
      "grad_norm": 0.9092600345611572,
      "learning_rate": 2.520037904212895e-05,
      "loss": 0.683,
      "step": 37890
    },
    {
      "epoch": 1.4964267382635132,
      "grad_norm": 0.8477237224578857,
      "learning_rate": 2.5180637264579305e-05,
      "loss": 0.7313,
      "step": 37900
    },
    {
      "epoch": 1.4968215738145063,
      "grad_norm": 1.167327880859375,
      "learning_rate": 2.516089548702965e-05,
      "loss": 0.704,
      "step": 37910
    },
    {
      "epoch": 1.4972164093654992,
      "grad_norm": 0.8443375825881958,
      "learning_rate": 2.5141153709480005e-05,
      "loss": 0.7308,
      "step": 37920
    },
    {
      "epoch": 1.4976112449164922,
      "grad_norm": 1.0266610383987427,
      "learning_rate": 2.512141193193035e-05,
      "loss": 0.748,
      "step": 37930
    },
    {
      "epoch": 1.4980060804674853,
      "grad_norm": 0.7967482209205627,
      "learning_rate": 2.51016701543807e-05,
      "loss": 0.7032,
      "step": 37940
    },
    {
      "epoch": 1.4984009160184784,
      "grad_norm": 1.0432571172714233,
      "learning_rate": 2.5081928376831048e-05,
      "loss": 0.7263,
      "step": 37950
    },
    {
      "epoch": 1.4987957515694714,
      "grad_norm": 0.7278279066085815,
      "learning_rate": 2.50621865992814e-05,
      "loss": 0.7267,
      "step": 37960
    },
    {
      "epoch": 1.4991905871204643,
      "grad_norm": 0.6230884194374084,
      "learning_rate": 2.5042444821731748e-05,
      "loss": 0.7197,
      "step": 37970
    },
    {
      "epoch": 1.4995854226714573,
      "grad_norm": 0.687886655330658,
      "learning_rate": 2.50227030441821e-05,
      "loss": 0.7382,
      "step": 37980
    },
    {
      "epoch": 1.4999802582224504,
      "grad_norm": 0.9008182883262634,
      "learning_rate": 2.5002961266632447e-05,
      "loss": 0.6877,
      "step": 37990
    },
    {
      "epoch": 1.5003750937734432,
      "grad_norm": 0.9848582148551941,
      "learning_rate": 2.4983219489082797e-05,
      "loss": 0.7423,
      "step": 38000
    },
    {
      "epoch": 1.5003750937734432,
      "eval_loss": 0.7624733448028564,
      "eval_runtime": 1219.6492,
      "eval_samples_per_second": 9.23,
      "eval_steps_per_second": 9.23,
      "step": 38000
    },
    {
      "epoch": 1.5007699293244365,
      "grad_norm": 0.7547109127044678,
      "learning_rate": 2.4963477711533147e-05,
      "loss": 0.7144,
      "step": 38010
    },
    {
      "epoch": 1.5011647648754294,
      "grad_norm": 0.7613022923469543,
      "learning_rate": 2.4943735933983497e-05,
      "loss": 0.7084,
      "step": 38020
    },
    {
      "epoch": 1.5015596004264224,
      "grad_norm": 0.6711260676383972,
      "learning_rate": 2.4923994156433847e-05,
      "loss": 0.6886,
      "step": 38030
    },
    {
      "epoch": 1.5019544359774155,
      "grad_norm": 0.9098730087280273,
      "learning_rate": 2.4904252378884197e-05,
      "loss": 0.7027,
      "step": 38040
    },
    {
      "epoch": 1.5023492715284084,
      "grad_norm": 0.7397809624671936,
      "learning_rate": 2.4884510601334547e-05,
      "loss": 0.7196,
      "step": 38050
    },
    {
      "epoch": 1.5027441070794014,
      "grad_norm": 0.8065410852432251,
      "learning_rate": 2.4864768823784897e-05,
      "loss": 0.7293,
      "step": 38060
    },
    {
      "epoch": 1.5031389426303945,
      "grad_norm": 0.6217390298843384,
      "learning_rate": 2.4845027046235243e-05,
      "loss": 0.7377,
      "step": 38070
    },
    {
      "epoch": 1.5035337781813873,
      "grad_norm": 0.762300431728363,
      "learning_rate": 2.4825285268685593e-05,
      "loss": 0.7387,
      "step": 38080
    },
    {
      "epoch": 1.5039286137323806,
      "grad_norm": 0.9234234690666199,
      "learning_rate": 2.4805543491135943e-05,
      "loss": 0.7015,
      "step": 38090
    },
    {
      "epoch": 1.5043234492833735,
      "grad_norm": 0.8510581254959106,
      "learning_rate": 2.4785801713586293e-05,
      "loss": 0.7468,
      "step": 38100
    },
    {
      "epoch": 1.5047182848343665,
      "grad_norm": 0.9886549115180969,
      "learning_rate": 2.4766059936036643e-05,
      "loss": 0.7204,
      "step": 38110
    },
    {
      "epoch": 1.5051131203853596,
      "grad_norm": 0.6966473460197449,
      "learning_rate": 2.4746318158486993e-05,
      "loss": 0.7157,
      "step": 38120
    },
    {
      "epoch": 1.5055079559363524,
      "grad_norm": 0.7232251167297363,
      "learning_rate": 2.4726576380937343e-05,
      "loss": 0.7086,
      "step": 38130
    },
    {
      "epoch": 1.5059027914873455,
      "grad_norm": 0.7556312084197998,
      "learning_rate": 2.470683460338769e-05,
      "loss": 0.7078,
      "step": 38140
    },
    {
      "epoch": 1.5062976270383386,
      "grad_norm": 0.7824733257293701,
      "learning_rate": 2.468709282583804e-05,
      "loss": 0.6826,
      "step": 38150
    },
    {
      "epoch": 1.5066924625893314,
      "grad_norm": 0.7253828644752502,
      "learning_rate": 2.466735104828839e-05,
      "loss": 0.7166,
      "step": 38160
    },
    {
      "epoch": 1.5070872981403247,
      "grad_norm": 0.7709974050521851,
      "learning_rate": 2.464760927073874e-05,
      "loss": 0.7152,
      "step": 38170
    },
    {
      "epoch": 1.5074821336913176,
      "grad_norm": 0.7475894689559937,
      "learning_rate": 2.462786749318909e-05,
      "loss": 0.7155,
      "step": 38180
    },
    {
      "epoch": 1.5078769692423106,
      "grad_norm": 0.8023422956466675,
      "learning_rate": 2.460812571563944e-05,
      "loss": 0.6893,
      "step": 38190
    },
    {
      "epoch": 1.5082718047933037,
      "grad_norm": 0.8187656998634338,
      "learning_rate": 2.458838393808979e-05,
      "loss": 0.7337,
      "step": 38200
    },
    {
      "epoch": 1.5086666403442965,
      "grad_norm": 0.8412353992462158,
      "learning_rate": 2.4568642160540135e-05,
      "loss": 0.7152,
      "step": 38210
    },
    {
      "epoch": 1.5090614758952896,
      "grad_norm": 0.7842525839805603,
      "learning_rate": 2.4548900382990485e-05,
      "loss": 0.7201,
      "step": 38220
    },
    {
      "epoch": 1.5094563114462827,
      "grad_norm": 0.5961682200431824,
      "learning_rate": 2.4529158605440835e-05,
      "loss": 0.6875,
      "step": 38230
    },
    {
      "epoch": 1.5098511469972755,
      "grad_norm": 0.8642664551734924,
      "learning_rate": 2.4509416827891185e-05,
      "loss": 0.6987,
      "step": 38240
    },
    {
      "epoch": 1.5102459825482688,
      "grad_norm": 0.627432644367218,
      "learning_rate": 2.4489675050341535e-05,
      "loss": 0.7558,
      "step": 38250
    },
    {
      "epoch": 1.5106408180992617,
      "grad_norm": 0.6697466373443604,
      "learning_rate": 2.4469933272791885e-05,
      "loss": 0.7359,
      "step": 38260
    },
    {
      "epoch": 1.5110356536502547,
      "grad_norm": 1.1883374452590942,
      "learning_rate": 2.4450191495242235e-05,
      "loss": 0.7068,
      "step": 38270
    },
    {
      "epoch": 1.5114304892012478,
      "grad_norm": 0.6696888208389282,
      "learning_rate": 2.443044971769258e-05,
      "loss": 0.6874,
      "step": 38280
    },
    {
      "epoch": 1.5118253247522406,
      "grad_norm": 0.9162256121635437,
      "learning_rate": 2.441070794014293e-05,
      "loss": 0.6904,
      "step": 38290
    },
    {
      "epoch": 1.5122201603032337,
      "grad_norm": 1.0457818508148193,
      "learning_rate": 2.439096616259328e-05,
      "loss": 0.6951,
      "step": 38300
    },
    {
      "epoch": 1.5126149958542268,
      "grad_norm": 0.90330970287323,
      "learning_rate": 2.437122438504363e-05,
      "loss": 0.7075,
      "step": 38310
    },
    {
      "epoch": 1.5130098314052196,
      "grad_norm": 0.6128904223442078,
      "learning_rate": 2.435148260749398e-05,
      "loss": 0.6835,
      "step": 38320
    },
    {
      "epoch": 1.513404666956213,
      "grad_norm": 0.6555445194244385,
      "learning_rate": 2.433174082994433e-05,
      "loss": 0.6967,
      "step": 38330
    },
    {
      "epoch": 1.5137995025072057,
      "grad_norm": 0.7639603018760681,
      "learning_rate": 2.431199905239468e-05,
      "loss": 0.6863,
      "step": 38340
    },
    {
      "epoch": 1.5141943380581988,
      "grad_norm": 0.9877809286117554,
      "learning_rate": 2.4292257274845027e-05,
      "loss": 0.678,
      "step": 38350
    },
    {
      "epoch": 1.5145891736091919,
      "grad_norm": 0.8925045132637024,
      "learning_rate": 2.4272515497295377e-05,
      "loss": 0.702,
      "step": 38360
    },
    {
      "epoch": 1.5149840091601847,
      "grad_norm": 0.775131106376648,
      "learning_rate": 2.4252773719745727e-05,
      "loss": 0.7193,
      "step": 38370
    },
    {
      "epoch": 1.5153788447111778,
      "grad_norm": 0.6778219938278198,
      "learning_rate": 2.4233031942196077e-05,
      "loss": 0.6916,
      "step": 38380
    },
    {
      "epoch": 1.5157736802621709,
      "grad_norm": 0.8458194136619568,
      "learning_rate": 2.4213290164646427e-05,
      "loss": 0.6915,
      "step": 38390
    },
    {
      "epoch": 1.5161685158131637,
      "grad_norm": 0.7031954526901245,
      "learning_rate": 2.4193548387096777e-05,
      "loss": 0.725,
      "step": 38400
    },
    {
      "epoch": 1.516563351364157,
      "grad_norm": 0.683885931968689,
      "learning_rate": 2.4173806609547127e-05,
      "loss": 0.6732,
      "step": 38410
    },
    {
      "epoch": 1.5169581869151498,
      "grad_norm": 0.8612027764320374,
      "learning_rate": 2.4154064831997473e-05,
      "loss": 0.6967,
      "step": 38420
    },
    {
      "epoch": 1.517353022466143,
      "grad_norm": 0.8881217837333679,
      "learning_rate": 2.4134323054447823e-05,
      "loss": 0.7426,
      "step": 38430
    },
    {
      "epoch": 1.517747858017136,
      "grad_norm": 0.9303311109542847,
      "learning_rate": 2.4114581276898173e-05,
      "loss": 0.684,
      "step": 38440
    },
    {
      "epoch": 1.5181426935681288,
      "grad_norm": 0.9224776029586792,
      "learning_rate": 2.4094839499348523e-05,
      "loss": 0.6822,
      "step": 38450
    },
    {
      "epoch": 1.5185375291191219,
      "grad_norm": 0.7853529453277588,
      "learning_rate": 2.4075097721798873e-05,
      "loss": 0.6998,
      "step": 38460
    },
    {
      "epoch": 1.518932364670115,
      "grad_norm": 0.672864556312561,
      "learning_rate": 2.4055355944249223e-05,
      "loss": 0.694,
      "step": 38470
    },
    {
      "epoch": 1.5193272002211078,
      "grad_norm": 0.7907010912895203,
      "learning_rate": 2.4035614166699573e-05,
      "loss": 0.6763,
      "step": 38480
    },
    {
      "epoch": 1.519722035772101,
      "grad_norm": 0.7517145872116089,
      "learning_rate": 2.401587238914992e-05,
      "loss": 0.6988,
      "step": 38490
    },
    {
      "epoch": 1.520116871323094,
      "grad_norm": 0.8333122730255127,
      "learning_rate": 2.399613061160027e-05,
      "loss": 0.7263,
      "step": 38500
    },
    {
      "epoch": 1.520511706874087,
      "grad_norm": 0.818014919757843,
      "learning_rate": 2.397638883405062e-05,
      "loss": 0.7008,
      "step": 38510
    },
    {
      "epoch": 1.52090654242508,
      "grad_norm": 0.8710513710975647,
      "learning_rate": 2.395664705650097e-05,
      "loss": 0.7325,
      "step": 38520
    },
    {
      "epoch": 1.521301377976073,
      "grad_norm": 0.8699640035629272,
      "learning_rate": 2.393690527895132e-05,
      "loss": 0.6798,
      "step": 38530
    },
    {
      "epoch": 1.521696213527066,
      "grad_norm": 0.5314207077026367,
      "learning_rate": 2.391716350140167e-05,
      "loss": 0.6955,
      "step": 38540
    },
    {
      "epoch": 1.522091049078059,
      "grad_norm": 0.7707491517066956,
      "learning_rate": 2.389742172385202e-05,
      "loss": 0.7175,
      "step": 38550
    },
    {
      "epoch": 1.5224858846290519,
      "grad_norm": 0.5961522459983826,
      "learning_rate": 2.387767994630237e-05,
      "loss": 0.6772,
      "step": 38560
    },
    {
      "epoch": 1.5228807201800452,
      "grad_norm": 1.0027220249176025,
      "learning_rate": 2.3857938168752715e-05,
      "loss": 0.7352,
      "step": 38570
    },
    {
      "epoch": 1.523275555731038,
      "grad_norm": 0.8770104050636292,
      "learning_rate": 2.3838196391203065e-05,
      "loss": 0.6812,
      "step": 38580
    },
    {
      "epoch": 1.523670391282031,
      "grad_norm": 0.8882968425750732,
      "learning_rate": 2.3818454613653415e-05,
      "loss": 0.6973,
      "step": 38590
    },
    {
      "epoch": 1.5240652268330241,
      "grad_norm": 1.083569049835205,
      "learning_rate": 2.3798712836103765e-05,
      "loss": 0.6759,
      "step": 38600
    },
    {
      "epoch": 1.524460062384017,
      "grad_norm": 0.7244042158126831,
      "learning_rate": 2.3778971058554115e-05,
      "loss": 0.7345,
      "step": 38610
    },
    {
      "epoch": 1.52485489793501,
      "grad_norm": 0.6221156716346741,
      "learning_rate": 2.3759229281004465e-05,
      "loss": 0.7276,
      "step": 38620
    },
    {
      "epoch": 1.5252497334860031,
      "grad_norm": 0.7931166887283325,
      "learning_rate": 2.3739487503454815e-05,
      "loss": 0.7023,
      "step": 38630
    },
    {
      "epoch": 1.525644569036996,
      "grad_norm": 0.7294391393661499,
      "learning_rate": 2.371974572590516e-05,
      "loss": 0.7186,
      "step": 38640
    },
    {
      "epoch": 1.5260394045879893,
      "grad_norm": 0.833161473274231,
      "learning_rate": 2.370000394835551e-05,
      "loss": 0.7075,
      "step": 38650
    },
    {
      "epoch": 1.526434240138982,
      "grad_norm": 0.7957247495651245,
      "learning_rate": 2.368026217080586e-05,
      "loss": 0.7022,
      "step": 38660
    },
    {
      "epoch": 1.5268290756899752,
      "grad_norm": 0.8437362313270569,
      "learning_rate": 2.366052039325621e-05,
      "loss": 0.715,
      "step": 38670
    },
    {
      "epoch": 1.5272239112409682,
      "grad_norm": 0.824325680732727,
      "learning_rate": 2.364077861570656e-05,
      "loss": 0.6861,
      "step": 38680
    },
    {
      "epoch": 1.527618746791961,
      "grad_norm": 0.8456340432167053,
      "learning_rate": 2.362103683815691e-05,
      "loss": 0.7096,
      "step": 38690
    },
    {
      "epoch": 1.5280135823429541,
      "grad_norm": 0.8994261026382446,
      "learning_rate": 2.360129506060726e-05,
      "loss": 0.6691,
      "step": 38700
    },
    {
      "epoch": 1.5284084178939472,
      "grad_norm": 0.8586122989654541,
      "learning_rate": 2.3581553283057607e-05,
      "loss": 0.6641,
      "step": 38710
    },
    {
      "epoch": 1.52880325344494,
      "grad_norm": 0.7085229158401489,
      "learning_rate": 2.3561811505507957e-05,
      "loss": 0.7153,
      "step": 38720
    },
    {
      "epoch": 1.5291980889959333,
      "grad_norm": 0.76316237449646,
      "learning_rate": 2.3542069727958307e-05,
      "loss": 0.7217,
      "step": 38730
    },
    {
      "epoch": 1.5295929245469262,
      "grad_norm": 0.820563554763794,
      "learning_rate": 2.3522327950408657e-05,
      "loss": 0.7129,
      "step": 38740
    },
    {
      "epoch": 1.5299877600979193,
      "grad_norm": 0.8281111121177673,
      "learning_rate": 2.3502586172859007e-05,
      "loss": 0.7032,
      "step": 38750
    },
    {
      "epoch": 1.5303825956489123,
      "grad_norm": 0.7402462363243103,
      "learning_rate": 2.3482844395309357e-05,
      "loss": 0.6716,
      "step": 38760
    },
    {
      "epoch": 1.5307774311999052,
      "grad_norm": 0.6931487321853638,
      "learning_rate": 2.3463102617759707e-05,
      "loss": 0.667,
      "step": 38770
    },
    {
      "epoch": 1.5311722667508982,
      "grad_norm": 0.7977966666221619,
      "learning_rate": 2.3443360840210053e-05,
      "loss": 0.7008,
      "step": 38780
    },
    {
      "epoch": 1.5315671023018913,
      "grad_norm": 0.7441149353981018,
      "learning_rate": 2.3423619062660403e-05,
      "loss": 0.6984,
      "step": 38790
    },
    {
      "epoch": 1.5319619378528841,
      "grad_norm": 0.8133353590965271,
      "learning_rate": 2.3403877285110753e-05,
      "loss": 0.6573,
      "step": 38800
    },
    {
      "epoch": 1.5323567734038774,
      "grad_norm": 0.8137113451957703,
      "learning_rate": 2.3384135507561103e-05,
      "loss": 0.6608,
      "step": 38810
    },
    {
      "epoch": 1.5327516089548703,
      "grad_norm": 0.852729320526123,
      "learning_rate": 2.3364393730011453e-05,
      "loss": 0.69,
      "step": 38820
    },
    {
      "epoch": 1.5331464445058633,
      "grad_norm": 0.8712787628173828,
      "learning_rate": 2.3344651952461803e-05,
      "loss": 0.7356,
      "step": 38830
    },
    {
      "epoch": 1.5335412800568564,
      "grad_norm": 0.9985079169273376,
      "learning_rate": 2.3324910174912153e-05,
      "loss": 0.7082,
      "step": 38840
    },
    {
      "epoch": 1.5339361156078493,
      "grad_norm": 0.7684686779975891,
      "learning_rate": 2.33051683973625e-05,
      "loss": 0.7011,
      "step": 38850
    },
    {
      "epoch": 1.5343309511588423,
      "grad_norm": 0.9255910515785217,
      "learning_rate": 2.328542661981285e-05,
      "loss": 0.7269,
      "step": 38860
    },
    {
      "epoch": 1.5347257867098354,
      "grad_norm": 0.9330103993415833,
      "learning_rate": 2.32656848422632e-05,
      "loss": 0.7145,
      "step": 38870
    },
    {
      "epoch": 1.5351206222608282,
      "grad_norm": 0.7489504218101501,
      "learning_rate": 2.324594306471355e-05,
      "loss": 0.6961,
      "step": 38880
    },
    {
      "epoch": 1.5355154578118215,
      "grad_norm": 0.6896730065345764,
      "learning_rate": 2.32262012871639e-05,
      "loss": 0.7133,
      "step": 38890
    },
    {
      "epoch": 1.5359102933628144,
      "grad_norm": 0.6974741816520691,
      "learning_rate": 2.320645950961425e-05,
      "loss": 0.7377,
      "step": 38900
    },
    {
      "epoch": 1.5363051289138074,
      "grad_norm": 0.6313426494598389,
      "learning_rate": 2.31867177320646e-05,
      "loss": 0.7299,
      "step": 38910
    },
    {
      "epoch": 1.5366999644648005,
      "grad_norm": 0.612135112285614,
      "learning_rate": 2.3166975954514945e-05,
      "loss": 0.6838,
      "step": 38920
    },
    {
      "epoch": 1.5370948000157933,
      "grad_norm": 0.6417556405067444,
      "learning_rate": 2.3147234176965295e-05,
      "loss": 0.7091,
      "step": 38930
    },
    {
      "epoch": 1.5374896355667864,
      "grad_norm": 0.7658911347389221,
      "learning_rate": 2.3127492399415645e-05,
      "loss": 0.7071,
      "step": 38940
    },
    {
      "epoch": 1.5378844711177795,
      "grad_norm": 0.774509608745575,
      "learning_rate": 2.3107750621865995e-05,
      "loss": 0.7165,
      "step": 38950
    },
    {
      "epoch": 1.5382793066687723,
      "grad_norm": 0.794501543045044,
      "learning_rate": 2.3088008844316345e-05,
      "loss": 0.7347,
      "step": 38960
    },
    {
      "epoch": 1.5386741422197656,
      "grad_norm": 0.6675713062286377,
      "learning_rate": 2.3068267066766695e-05,
      "loss": 0.679,
      "step": 38970
    },
    {
      "epoch": 1.5390689777707585,
      "grad_norm": 0.8687155246734619,
      "learning_rate": 2.3048525289217045e-05,
      "loss": 0.7186,
      "step": 38980
    },
    {
      "epoch": 1.5394638133217515,
      "grad_norm": 0.7561178207397461,
      "learning_rate": 2.302878351166739e-05,
      "loss": 0.6571,
      "step": 38990
    },
    {
      "epoch": 1.5398586488727446,
      "grad_norm": 0.8806009888648987,
      "learning_rate": 2.300904173411774e-05,
      "loss": 0.7268,
      "step": 39000
    },
    {
      "epoch": 1.5398586488727446,
      "eval_loss": 0.7623107433319092,
      "eval_runtime": 1219.1131,
      "eval_samples_per_second": 9.234,
      "eval_steps_per_second": 9.234,
      "step": 39000
    },
    {
      "epoch": 1.5402534844237374,
      "grad_norm": 0.9398055076599121,
      "learning_rate": 2.298929995656809e-05,
      "loss": 0.7029,
      "step": 39010
    },
    {
      "epoch": 1.5406483199747305,
      "grad_norm": 0.8580857515335083,
      "learning_rate": 2.296955817901844e-05,
      "loss": 0.7616,
      "step": 39020
    },
    {
      "epoch": 1.5410431555257236,
      "grad_norm": 0.6705293655395508,
      "learning_rate": 2.294981640146879e-05,
      "loss": 0.6448,
      "step": 39030
    },
    {
      "epoch": 1.5414379910767164,
      "grad_norm": 0.8145930767059326,
      "learning_rate": 2.293007462391914e-05,
      "loss": 0.7115,
      "step": 39040
    },
    {
      "epoch": 1.5418328266277097,
      "grad_norm": 0.678013265132904,
      "learning_rate": 2.291033284636949e-05,
      "loss": 0.6799,
      "step": 39050
    },
    {
      "epoch": 1.5422276621787026,
      "grad_norm": 0.8444539904594421,
      "learning_rate": 2.2890591068819837e-05,
      "loss": 0.6978,
      "step": 39060
    },
    {
      "epoch": 1.5426224977296956,
      "grad_norm": 1.0958878993988037,
      "learning_rate": 2.2870849291270187e-05,
      "loss": 0.719,
      "step": 39070
    },
    {
      "epoch": 1.5430173332806887,
      "grad_norm": 0.8782224655151367,
      "learning_rate": 2.2851107513720537e-05,
      "loss": 0.6705,
      "step": 39080
    },
    {
      "epoch": 1.5434121688316815,
      "grad_norm": 0.8007495403289795,
      "learning_rate": 2.2831365736170887e-05,
      "loss": 0.7401,
      "step": 39090
    },
    {
      "epoch": 1.5438070043826746,
      "grad_norm": 0.707761287689209,
      "learning_rate": 2.2811623958621237e-05,
      "loss": 0.6961,
      "step": 39100
    },
    {
      "epoch": 1.5442018399336677,
      "grad_norm": 0.7332175970077515,
      "learning_rate": 2.2791882181071587e-05,
      "loss": 0.7,
      "step": 39110
    },
    {
      "epoch": 1.5445966754846605,
      "grad_norm": 0.8984394669532776,
      "learning_rate": 2.2772140403521937e-05,
      "loss": 0.7262,
      "step": 39120
    },
    {
      "epoch": 1.5449915110356538,
      "grad_norm": 0.7696622610092163,
      "learning_rate": 2.2752398625972283e-05,
      "loss": 0.6954,
      "step": 39130
    },
    {
      "epoch": 1.5453863465866466,
      "grad_norm": 0.8648125529289246,
      "learning_rate": 2.2732656848422633e-05,
      "loss": 0.7273,
      "step": 39140
    },
    {
      "epoch": 1.5457811821376397,
      "grad_norm": 0.9800359010696411,
      "learning_rate": 2.2712915070872983e-05,
      "loss": 0.7112,
      "step": 39150
    },
    {
      "epoch": 1.5461760176886328,
      "grad_norm": 0.8853136897087097,
      "learning_rate": 2.2693173293323333e-05,
      "loss": 0.6951,
      "step": 39160
    },
    {
      "epoch": 1.5465708532396256,
      "grad_norm": 0.7787624597549438,
      "learning_rate": 2.2673431515773683e-05,
      "loss": 0.7272,
      "step": 39170
    },
    {
      "epoch": 1.5469656887906187,
      "grad_norm": 0.92380690574646,
      "learning_rate": 2.2653689738224033e-05,
      "loss": 0.7297,
      "step": 39180
    },
    {
      "epoch": 1.5473605243416118,
      "grad_norm": 0.7322981953620911,
      "learning_rate": 2.2633947960674383e-05,
      "loss": 0.6947,
      "step": 39190
    },
    {
      "epoch": 1.5477553598926046,
      "grad_norm": 0.6357238292694092,
      "learning_rate": 2.261420618312473e-05,
      "loss": 0.7044,
      "step": 39200
    },
    {
      "epoch": 1.5481501954435979,
      "grad_norm": 0.6742597818374634,
      "learning_rate": 2.259446440557508e-05,
      "loss": 0.7288,
      "step": 39210
    },
    {
      "epoch": 1.5485450309945907,
      "grad_norm": 1.0660909414291382,
      "learning_rate": 2.257472262802543e-05,
      "loss": 0.6681,
      "step": 39220
    },
    {
      "epoch": 1.5489398665455838,
      "grad_norm": 0.7721738219261169,
      "learning_rate": 2.255498085047578e-05,
      "loss": 0.7017,
      "step": 39230
    },
    {
      "epoch": 1.5493347020965769,
      "grad_norm": 0.7048145532608032,
      "learning_rate": 2.253523907292613e-05,
      "loss": 0.6352,
      "step": 39240
    },
    {
      "epoch": 1.5497295376475697,
      "grad_norm": 0.9554762840270996,
      "learning_rate": 2.251549729537648e-05,
      "loss": 0.7155,
      "step": 39250
    },
    {
      "epoch": 1.5501243731985628,
      "grad_norm": 0.6218653321266174,
      "learning_rate": 2.249575551782683e-05,
      "loss": 0.7195,
      "step": 39260
    },
    {
      "epoch": 1.5505192087495558,
      "grad_norm": 0.6840336918830872,
      "learning_rate": 2.2476013740277175e-05,
      "loss": 0.6932,
      "step": 39270
    },
    {
      "epoch": 1.5509140443005487,
      "grad_norm": 0.8854249715805054,
      "learning_rate": 2.2456271962727525e-05,
      "loss": 0.7145,
      "step": 39280
    },
    {
      "epoch": 1.551308879851542,
      "grad_norm": 0.8844672441482544,
      "learning_rate": 2.2436530185177875e-05,
      "loss": 0.7077,
      "step": 39290
    },
    {
      "epoch": 1.5517037154025348,
      "grad_norm": 0.8462405204772949,
      "learning_rate": 2.2416788407628225e-05,
      "loss": 0.6787,
      "step": 39300
    },
    {
      "epoch": 1.552098550953528,
      "grad_norm": 0.7034744024276733,
      "learning_rate": 2.2397046630078575e-05,
      "loss": 0.6927,
      "step": 39310
    },
    {
      "epoch": 1.552493386504521,
      "grad_norm": 0.7777203917503357,
      "learning_rate": 2.237730485252892e-05,
      "loss": 0.6655,
      "step": 39320
    },
    {
      "epoch": 1.5528882220555138,
      "grad_norm": 0.7878585457801819,
      "learning_rate": 2.235756307497927e-05,
      "loss": 0.6728,
      "step": 39330
    },
    {
      "epoch": 1.5532830576065069,
      "grad_norm": 0.7458804845809937,
      "learning_rate": 2.233782129742962e-05,
      "loss": 0.7046,
      "step": 39340
    },
    {
      "epoch": 1.5536778931575,
      "grad_norm": 0.7990315556526184,
      "learning_rate": 2.231807951987997e-05,
      "loss": 0.7142,
      "step": 39350
    },
    {
      "epoch": 1.5540727287084928,
      "grad_norm": 0.6095502972602844,
      "learning_rate": 2.2298337742330318e-05,
      "loss": 0.7076,
      "step": 39360
    },
    {
      "epoch": 1.554467564259486,
      "grad_norm": 0.7646414041519165,
      "learning_rate": 2.2278595964780668e-05,
      "loss": 0.6843,
      "step": 39370
    },
    {
      "epoch": 1.554862399810479,
      "grad_norm": 0.9765002131462097,
      "learning_rate": 2.2258854187231018e-05,
      "loss": 0.7491,
      "step": 39380
    },
    {
      "epoch": 1.555257235361472,
      "grad_norm": 0.7292956709861755,
      "learning_rate": 2.2239112409681368e-05,
      "loss": 0.7136,
      "step": 39390
    },
    {
      "epoch": 1.555652070912465,
      "grad_norm": 0.8339702486991882,
      "learning_rate": 2.2219370632131717e-05,
      "loss": 0.7457,
      "step": 39400
    },
    {
      "epoch": 1.556046906463458,
      "grad_norm": 0.7938748598098755,
      "learning_rate": 2.2199628854582067e-05,
      "loss": 0.7895,
      "step": 39410
    },
    {
      "epoch": 1.556441742014451,
      "grad_norm": 0.7986287474632263,
      "learning_rate": 2.2179887077032417e-05,
      "loss": 0.6937,
      "step": 39420
    },
    {
      "epoch": 1.556836577565444,
      "grad_norm": 0.9249372482299805,
      "learning_rate": 2.2160145299482764e-05,
      "loss": 0.7196,
      "step": 39430
    },
    {
      "epoch": 1.5572314131164369,
      "grad_norm": 0.9074551463127136,
      "learning_rate": 2.2140403521933114e-05,
      "loss": 0.7368,
      "step": 39440
    },
    {
      "epoch": 1.5576262486674302,
      "grad_norm": 0.783728837966919,
      "learning_rate": 2.2120661744383464e-05,
      "loss": 0.6965,
      "step": 39450
    },
    {
      "epoch": 1.558021084218423,
      "grad_norm": 0.8184762597084045,
      "learning_rate": 2.2100919966833814e-05,
      "loss": 0.6954,
      "step": 39460
    },
    {
      "epoch": 1.558415919769416,
      "grad_norm": 0.889423668384552,
      "learning_rate": 2.2081178189284163e-05,
      "loss": 0.6859,
      "step": 39470
    },
    {
      "epoch": 1.5588107553204091,
      "grad_norm": 0.7116861343383789,
      "learning_rate": 2.2061436411734513e-05,
      "loss": 0.7173,
      "step": 39480
    },
    {
      "epoch": 1.559205590871402,
      "grad_norm": 0.5877129435539246,
      "learning_rate": 2.2041694634184863e-05,
      "loss": 0.6962,
      "step": 39490
    },
    {
      "epoch": 1.559600426422395,
      "grad_norm": 0.7680791616439819,
      "learning_rate": 2.202195285663521e-05,
      "loss": 0.7745,
      "step": 39500
    },
    {
      "epoch": 1.5599952619733881,
      "grad_norm": 0.7692148685455322,
      "learning_rate": 2.200221107908556e-05,
      "loss": 0.6976,
      "step": 39510
    },
    {
      "epoch": 1.560390097524381,
      "grad_norm": 0.8778876662254333,
      "learning_rate": 2.198246930153591e-05,
      "loss": 0.6936,
      "step": 39520
    },
    {
      "epoch": 1.5607849330753742,
      "grad_norm": 0.690268874168396,
      "learning_rate": 2.196272752398626e-05,
      "loss": 0.6666,
      "step": 39530
    },
    {
      "epoch": 1.561179768626367,
      "grad_norm": 0.611207127571106,
      "learning_rate": 2.194298574643661e-05,
      "loss": 0.6637,
      "step": 39540
    },
    {
      "epoch": 1.5615746041773602,
      "grad_norm": 0.7634499669075012,
      "learning_rate": 2.192324396888696e-05,
      "loss": 0.7216,
      "step": 39550
    },
    {
      "epoch": 1.5619694397283532,
      "grad_norm": 0.6774807572364807,
      "learning_rate": 2.190350219133731e-05,
      "loss": 0.7235,
      "step": 39560
    },
    {
      "epoch": 1.562364275279346,
      "grad_norm": 0.6654950380325317,
      "learning_rate": 2.1883760413787656e-05,
      "loss": 0.7198,
      "step": 39570
    },
    {
      "epoch": 1.5627591108303391,
      "grad_norm": 0.8154381513595581,
      "learning_rate": 2.1864018636238006e-05,
      "loss": 0.7161,
      "step": 39580
    },
    {
      "epoch": 1.5631539463813322,
      "grad_norm": 0.8148225545883179,
      "learning_rate": 2.1844276858688356e-05,
      "loss": 0.7138,
      "step": 39590
    },
    {
      "epoch": 1.563548781932325,
      "grad_norm": 0.8809996247291565,
      "learning_rate": 2.1824535081138706e-05,
      "loss": 0.6888,
      "step": 39600
    },
    {
      "epoch": 1.5639436174833183,
      "grad_norm": 0.9844194054603577,
      "learning_rate": 2.1804793303589055e-05,
      "loss": 0.7088,
      "step": 39610
    },
    {
      "epoch": 1.5643384530343112,
      "grad_norm": 0.7766733169555664,
      "learning_rate": 2.1785051526039405e-05,
      "loss": 0.6665,
      "step": 39620
    },
    {
      "epoch": 1.5647332885853042,
      "grad_norm": 0.7234345078468323,
      "learning_rate": 2.1765309748489755e-05,
      "loss": 0.7216,
      "step": 39630
    },
    {
      "epoch": 1.5651281241362973,
      "grad_norm": 0.8831411600112915,
      "learning_rate": 2.1745567970940102e-05,
      "loss": 0.7272,
      "step": 39640
    },
    {
      "epoch": 1.5655229596872902,
      "grad_norm": 0.8256193995475769,
      "learning_rate": 2.1725826193390452e-05,
      "loss": 0.722,
      "step": 39650
    },
    {
      "epoch": 1.5659177952382832,
      "grad_norm": 0.7873157858848572,
      "learning_rate": 2.17060844158408e-05,
      "loss": 0.7079,
      "step": 39660
    },
    {
      "epoch": 1.5663126307892763,
      "grad_norm": 0.7314792275428772,
      "learning_rate": 2.168634263829115e-05,
      "loss": 0.7067,
      "step": 39670
    },
    {
      "epoch": 1.5667074663402691,
      "grad_norm": 0.9923030734062195,
      "learning_rate": 2.16666008607415e-05,
      "loss": 0.7504,
      "step": 39680
    },
    {
      "epoch": 1.5671023018912624,
      "grad_norm": 0.8309398889541626,
      "learning_rate": 2.164685908319185e-05,
      "loss": 0.6826,
      "step": 39690
    },
    {
      "epoch": 1.5674971374422553,
      "grad_norm": 0.9768442511558533,
      "learning_rate": 2.16271173056422e-05,
      "loss": 0.6967,
      "step": 39700
    },
    {
      "epoch": 1.5678919729932483,
      "grad_norm": 0.8235207796096802,
      "learning_rate": 2.1607375528092548e-05,
      "loss": 0.7082,
      "step": 39710
    },
    {
      "epoch": 1.5682868085442414,
      "grad_norm": 0.9095545411109924,
      "learning_rate": 2.1587633750542898e-05,
      "loss": 0.7266,
      "step": 39720
    },
    {
      "epoch": 1.5686816440952343,
      "grad_norm": 0.7099409699440002,
      "learning_rate": 2.1567891972993248e-05,
      "loss": 0.6988,
      "step": 39730
    },
    {
      "epoch": 1.5690764796462273,
      "grad_norm": 0.8407516479492188,
      "learning_rate": 2.1548150195443598e-05,
      "loss": 0.7463,
      "step": 39740
    },
    {
      "epoch": 1.5694713151972204,
      "grad_norm": 0.8253749012947083,
      "learning_rate": 2.1528408417893947e-05,
      "loss": 0.724,
      "step": 39750
    },
    {
      "epoch": 1.5698661507482132,
      "grad_norm": 0.7067914009094238,
      "learning_rate": 2.1508666640344297e-05,
      "loss": 0.6698,
      "step": 39760
    },
    {
      "epoch": 1.5702609862992065,
      "grad_norm": 0.8895052075386047,
      "learning_rate": 2.1488924862794647e-05,
      "loss": 0.6754,
      "step": 39770
    },
    {
      "epoch": 1.5706558218501994,
      "grad_norm": 0.7815210819244385,
      "learning_rate": 2.1469183085244994e-05,
      "loss": 0.7085,
      "step": 39780
    },
    {
      "epoch": 1.5710506574011924,
      "grad_norm": 0.7706052660942078,
      "learning_rate": 2.1449441307695344e-05,
      "loss": 0.7083,
      "step": 39790
    },
    {
      "epoch": 1.5714454929521855,
      "grad_norm": 1.143593668937683,
      "learning_rate": 2.1429699530145694e-05,
      "loss": 0.7217,
      "step": 39800
    },
    {
      "epoch": 1.5718403285031783,
      "grad_norm": 0.8299785852432251,
      "learning_rate": 2.1409957752596044e-05,
      "loss": 0.6806,
      "step": 39810
    },
    {
      "epoch": 1.5722351640541714,
      "grad_norm": 0.7390324473381042,
      "learning_rate": 2.1390215975046393e-05,
      "loss": 0.7256,
      "step": 39820
    },
    {
      "epoch": 1.5726299996051645,
      "grad_norm": 0.8879485726356506,
      "learning_rate": 2.1370474197496743e-05,
      "loss": 0.7177,
      "step": 39830
    },
    {
      "epoch": 1.5730248351561573,
      "grad_norm": 0.8436059355735779,
      "learning_rate": 2.1350732419947093e-05,
      "loss": 0.7159,
      "step": 39840
    },
    {
      "epoch": 1.5734196707071506,
      "grad_norm": 0.8039036393165588,
      "learning_rate": 2.133099064239744e-05,
      "loss": 0.685,
      "step": 39850
    },
    {
      "epoch": 1.5738145062581435,
      "grad_norm": 0.6885622143745422,
      "learning_rate": 2.131124886484779e-05,
      "loss": 0.7354,
      "step": 39860
    },
    {
      "epoch": 1.5742093418091365,
      "grad_norm": 0.6720532178878784,
      "learning_rate": 2.129150708729814e-05,
      "loss": 0.6665,
      "step": 39870
    },
    {
      "epoch": 1.5746041773601296,
      "grad_norm": 0.6251872777938843,
      "learning_rate": 2.127176530974849e-05,
      "loss": 0.6536,
      "step": 39880
    },
    {
      "epoch": 1.5749990129111224,
      "grad_norm": 0.8292146325111389,
      "learning_rate": 2.125202353219884e-05,
      "loss": 0.7001,
      "step": 39890
    },
    {
      "epoch": 1.5753938484621155,
      "grad_norm": 0.6735407114028931,
      "learning_rate": 2.123228175464919e-05,
      "loss": 0.7111,
      "step": 39900
    },
    {
      "epoch": 1.5757886840131086,
      "grad_norm": 0.7968688011169434,
      "learning_rate": 2.121253997709954e-05,
      "loss": 0.6556,
      "step": 39910
    },
    {
      "epoch": 1.5761835195641014,
      "grad_norm": 0.8223379850387573,
      "learning_rate": 2.1192798199549886e-05,
      "loss": 0.7148,
      "step": 39920
    },
    {
      "epoch": 1.5765783551150947,
      "grad_norm": 0.7268731594085693,
      "learning_rate": 2.1173056422000236e-05,
      "loss": 0.7027,
      "step": 39930
    },
    {
      "epoch": 1.5769731906660875,
      "grad_norm": 1.0494627952575684,
      "learning_rate": 2.1153314644450586e-05,
      "loss": 0.696,
      "step": 39940
    },
    {
      "epoch": 1.5773680262170806,
      "grad_norm": 0.7004697918891907,
      "learning_rate": 2.1133572866900936e-05,
      "loss": 0.7238,
      "step": 39950
    },
    {
      "epoch": 1.5777628617680737,
      "grad_norm": 0.858604907989502,
      "learning_rate": 2.1113831089351286e-05,
      "loss": 0.7062,
      "step": 39960
    },
    {
      "epoch": 1.5781576973190665,
      "grad_norm": 0.7167415618896484,
      "learning_rate": 2.1094089311801635e-05,
      "loss": 0.6804,
      "step": 39970
    },
    {
      "epoch": 1.5785525328700596,
      "grad_norm": 0.8623978495597839,
      "learning_rate": 2.1074347534251985e-05,
      "loss": 0.7151,
      "step": 39980
    },
    {
      "epoch": 1.5789473684210527,
      "grad_norm": 0.8262240886688232,
      "learning_rate": 2.1054605756702335e-05,
      "loss": 0.717,
      "step": 39990
    },
    {
      "epoch": 1.5793422039720455,
      "grad_norm": 0.7643224000930786,
      "learning_rate": 2.1034863979152682e-05,
      "loss": 0.733,
      "step": 40000
    },
    {
      "epoch": 1.5793422039720455,
      "eval_loss": 0.7604591846466064,
      "eval_runtime": 1219.0722,
      "eval_samples_per_second": 9.234,
      "eval_steps_per_second": 9.234,
      "step": 40000
    },
    {
      "epoch": 1.5797370395230388,
      "grad_norm": 0.7893639802932739,
      "learning_rate": 2.1015122201603032e-05,
      "loss": 0.7436,
      "step": 40010
    },
    {
      "epoch": 1.5801318750740316,
      "grad_norm": 0.8461567759513855,
      "learning_rate": 2.099538042405338e-05,
      "loss": 0.7286,
      "step": 40020
    },
    {
      "epoch": 1.5805267106250247,
      "grad_norm": 0.7361785769462585,
      "learning_rate": 2.097563864650373e-05,
      "loss": 0.7597,
      "step": 40030
    },
    {
      "epoch": 1.5809215461760178,
      "grad_norm": 0.7187495827674866,
      "learning_rate": 2.095589686895408e-05,
      "loss": 0.715,
      "step": 40040
    },
    {
      "epoch": 1.5813163817270106,
      "grad_norm": 0.755544900894165,
      "learning_rate": 2.093615509140443e-05,
      "loss": 0.7024,
      "step": 40050
    },
    {
      "epoch": 1.5817112172780037,
      "grad_norm": 0.8012928366661072,
      "learning_rate": 2.091641331385478e-05,
      "loss": 0.6909,
      "step": 40060
    },
    {
      "epoch": 1.5821060528289967,
      "grad_norm": 0.6671237349510193,
      "learning_rate": 2.0896671536305128e-05,
      "loss": 0.7091,
      "step": 40070
    },
    {
      "epoch": 1.5825008883799896,
      "grad_norm": 0.7112669348716736,
      "learning_rate": 2.0876929758755478e-05,
      "loss": 0.6998,
      "step": 40080
    },
    {
      "epoch": 1.5828957239309829,
      "grad_norm": 1.0775208473205566,
      "learning_rate": 2.0857187981205828e-05,
      "loss": 0.7178,
      "step": 40090
    },
    {
      "epoch": 1.5832905594819757,
      "grad_norm": 0.8583369255065918,
      "learning_rate": 2.0837446203656178e-05,
      "loss": 0.7185,
      "step": 40100
    },
    {
      "epoch": 1.5836853950329688,
      "grad_norm": 1.2979280948638916,
      "learning_rate": 2.0817704426106527e-05,
      "loss": 0.667,
      "step": 40110
    },
    {
      "epoch": 1.5840802305839619,
      "grad_norm": 0.7660717964172363,
      "learning_rate": 2.0797962648556877e-05,
      "loss": 0.7029,
      "step": 40120
    },
    {
      "epoch": 1.5844750661349547,
      "grad_norm": 0.7950818538665771,
      "learning_rate": 2.0778220871007227e-05,
      "loss": 0.7101,
      "step": 40130
    },
    {
      "epoch": 1.5848699016859478,
      "grad_norm": 0.5680896043777466,
      "learning_rate": 2.0758479093457574e-05,
      "loss": 0.7147,
      "step": 40140
    },
    {
      "epoch": 1.5852647372369408,
      "grad_norm": 0.7093222141265869,
      "learning_rate": 2.0738737315907924e-05,
      "loss": 0.6896,
      "step": 40150
    },
    {
      "epoch": 1.5856595727879337,
      "grad_norm": 0.9545114040374756,
      "learning_rate": 2.0718995538358274e-05,
      "loss": 0.6622,
      "step": 40160
    },
    {
      "epoch": 1.586054408338927,
      "grad_norm": 0.864794909954071,
      "learning_rate": 2.0699253760808624e-05,
      "loss": 0.7595,
      "step": 40170
    },
    {
      "epoch": 1.5864492438899198,
      "grad_norm": 0.9760574698448181,
      "learning_rate": 2.0679511983258973e-05,
      "loss": 0.7239,
      "step": 40180
    },
    {
      "epoch": 1.5868440794409129,
      "grad_norm": 0.6937304735183716,
      "learning_rate": 2.0659770205709323e-05,
      "loss": 0.6972,
      "step": 40190
    },
    {
      "epoch": 1.587238914991906,
      "grad_norm": 0.8559474349021912,
      "learning_rate": 2.0640028428159673e-05,
      "loss": 0.7032,
      "step": 40200
    },
    {
      "epoch": 1.5876337505428988,
      "grad_norm": 0.7952092289924622,
      "learning_rate": 2.062028665061002e-05,
      "loss": 0.7157,
      "step": 40210
    },
    {
      "epoch": 1.5880285860938919,
      "grad_norm": 1.0012800693511963,
      "learning_rate": 2.060054487306037e-05,
      "loss": 0.718,
      "step": 40220
    },
    {
      "epoch": 1.588423421644885,
      "grad_norm": 0.8303987383842468,
      "learning_rate": 2.058080309551072e-05,
      "loss": 0.7196,
      "step": 40230
    },
    {
      "epoch": 1.5888182571958778,
      "grad_norm": 0.7711241841316223,
      "learning_rate": 2.056106131796107e-05,
      "loss": 0.7079,
      "step": 40240
    },
    {
      "epoch": 1.589213092746871,
      "grad_norm": 0.7545072436332703,
      "learning_rate": 2.054131954041142e-05,
      "loss": 0.6868,
      "step": 40250
    },
    {
      "epoch": 1.589607928297864,
      "grad_norm": 0.7207604646682739,
      "learning_rate": 2.052157776286177e-05,
      "loss": 0.7246,
      "step": 40260
    },
    {
      "epoch": 1.590002763848857,
      "grad_norm": 0.6305363178253174,
      "learning_rate": 2.050183598531212e-05,
      "loss": 0.7617,
      "step": 40270
    },
    {
      "epoch": 1.59039759939985,
      "grad_norm": 0.7856823801994324,
      "learning_rate": 2.0482094207762466e-05,
      "loss": 0.7285,
      "step": 40280
    },
    {
      "epoch": 1.5907924349508429,
      "grad_norm": 0.6568973064422607,
      "learning_rate": 2.0462352430212816e-05,
      "loss": 0.6688,
      "step": 40290
    },
    {
      "epoch": 1.591187270501836,
      "grad_norm": 0.8036534786224365,
      "learning_rate": 2.0442610652663166e-05,
      "loss": 0.7223,
      "step": 40300
    },
    {
      "epoch": 1.591582106052829,
      "grad_norm": 0.7946886420249939,
      "learning_rate": 2.0422868875113516e-05,
      "loss": 0.6937,
      "step": 40310
    },
    {
      "epoch": 1.5919769416038219,
      "grad_norm": 0.9200554490089417,
      "learning_rate": 2.0403127097563865e-05,
      "loss": 0.7166,
      "step": 40320
    },
    {
      "epoch": 1.5923717771548151,
      "grad_norm": 0.8167934417724609,
      "learning_rate": 2.0383385320014215e-05,
      "loss": 0.6874,
      "step": 40330
    },
    {
      "epoch": 1.592766612705808,
      "grad_norm": 0.750036895275116,
      "learning_rate": 2.0363643542464565e-05,
      "loss": 0.6722,
      "step": 40340
    },
    {
      "epoch": 1.593161448256801,
      "grad_norm": 0.8721019625663757,
      "learning_rate": 2.0343901764914912e-05,
      "loss": 0.7122,
      "step": 40350
    },
    {
      "epoch": 1.5935562838077941,
      "grad_norm": 0.7192047834396362,
      "learning_rate": 2.0324159987365262e-05,
      "loss": 0.6811,
      "step": 40360
    },
    {
      "epoch": 1.593951119358787,
      "grad_norm": 0.6976072788238525,
      "learning_rate": 2.030441820981561e-05,
      "loss": 0.7004,
      "step": 40370
    },
    {
      "epoch": 1.59434595490978,
      "grad_norm": 0.8007376790046692,
      "learning_rate": 2.028467643226596e-05,
      "loss": 0.6551,
      "step": 40380
    },
    {
      "epoch": 1.594740790460773,
      "grad_norm": 1.0089919567108154,
      "learning_rate": 2.026493465471631e-05,
      "loss": 0.7104,
      "step": 40390
    },
    {
      "epoch": 1.595135626011766,
      "grad_norm": 0.7705692052841187,
      "learning_rate": 2.024519287716666e-05,
      "loss": 0.767,
      "step": 40400
    },
    {
      "epoch": 1.5955304615627592,
      "grad_norm": 0.7486013174057007,
      "learning_rate": 2.022545109961701e-05,
      "loss": 0.7394,
      "step": 40410
    },
    {
      "epoch": 1.595925297113752,
      "grad_norm": 0.8319032788276672,
      "learning_rate": 2.0205709322067358e-05,
      "loss": 0.6892,
      "step": 40420
    },
    {
      "epoch": 1.5963201326647452,
      "grad_norm": 0.7936161160469055,
      "learning_rate": 2.0185967544517708e-05,
      "loss": 0.7185,
      "step": 40430
    },
    {
      "epoch": 1.5967149682157382,
      "grad_norm": 0.7890838980674744,
      "learning_rate": 2.0166225766968058e-05,
      "loss": 0.7623,
      "step": 40440
    },
    {
      "epoch": 1.597109803766731,
      "grad_norm": 0.7853350043296814,
      "learning_rate": 2.0146483989418408e-05,
      "loss": 0.7236,
      "step": 40450
    },
    {
      "epoch": 1.5975046393177241,
      "grad_norm": 0.8117653131484985,
      "learning_rate": 2.0126742211868757e-05,
      "loss": 0.68,
      "step": 40460
    },
    {
      "epoch": 1.5978994748687172,
      "grad_norm": 0.6382284164428711,
      "learning_rate": 2.0107000434319107e-05,
      "loss": 0.6942,
      "step": 40470
    },
    {
      "epoch": 1.59829431041971,
      "grad_norm": 0.6284233331680298,
      "learning_rate": 2.0087258656769457e-05,
      "loss": 0.7255,
      "step": 40480
    },
    {
      "epoch": 1.5986891459707033,
      "grad_norm": 0.7209469079971313,
      "learning_rate": 2.0067516879219804e-05,
      "loss": 0.7037,
      "step": 40490
    },
    {
      "epoch": 1.5990839815216962,
      "grad_norm": 0.8062772154808044,
      "learning_rate": 2.0047775101670154e-05,
      "loss": 0.7061,
      "step": 40500
    },
    {
      "epoch": 1.5994788170726892,
      "grad_norm": 0.9562636017799377,
      "learning_rate": 2.0028033324120504e-05,
      "loss": 0.7172,
      "step": 40510
    },
    {
      "epoch": 1.5998736526236823,
      "grad_norm": 0.7484160661697388,
      "learning_rate": 2.0008291546570854e-05,
      "loss": 0.7572,
      "step": 40520
    },
    {
      "epoch": 1.6002684881746752,
      "grad_norm": 0.8795589208602905,
      "learning_rate": 1.9988549769021204e-05,
      "loss": 0.7096,
      "step": 40530
    },
    {
      "epoch": 1.6006633237256682,
      "grad_norm": 0.8939523696899414,
      "learning_rate": 1.9968807991471553e-05,
      "loss": 0.7527,
      "step": 40540
    },
    {
      "epoch": 1.6010581592766613,
      "grad_norm": 0.7300298810005188,
      "learning_rate": 1.9949066213921903e-05,
      "loss": 0.713,
      "step": 40550
    },
    {
      "epoch": 1.6014529948276541,
      "grad_norm": 0.8508470058441162,
      "learning_rate": 1.992932443637225e-05,
      "loss": 0.7,
      "step": 40560
    },
    {
      "epoch": 1.6018478303786474,
      "grad_norm": 0.7692750096321106,
      "learning_rate": 1.99095826588226e-05,
      "loss": 0.6899,
      "step": 40570
    },
    {
      "epoch": 1.6022426659296403,
      "grad_norm": 0.8864414095878601,
      "learning_rate": 1.988984088127295e-05,
      "loss": 0.7027,
      "step": 40580
    },
    {
      "epoch": 1.6026375014806333,
      "grad_norm": 0.8635615110397339,
      "learning_rate": 1.98700991037233e-05,
      "loss": 0.674,
      "step": 40590
    },
    {
      "epoch": 1.6030323370316264,
      "grad_norm": 0.9614040851593018,
      "learning_rate": 1.985035732617365e-05,
      "loss": 0.7056,
      "step": 40600
    },
    {
      "epoch": 1.6034271725826192,
      "grad_norm": 1.3228867053985596,
      "learning_rate": 1.9830615548624e-05,
      "loss": 0.7005,
      "step": 40610
    },
    {
      "epoch": 1.6038220081336123,
      "grad_norm": 0.7968546748161316,
      "learning_rate": 1.981087377107435e-05,
      "loss": 0.7085,
      "step": 40620
    },
    {
      "epoch": 1.6042168436846054,
      "grad_norm": 0.7866812944412231,
      "learning_rate": 1.9791131993524696e-05,
      "loss": 0.705,
      "step": 40630
    },
    {
      "epoch": 1.6046116792355982,
      "grad_norm": 0.8719800710678101,
      "learning_rate": 1.9771390215975046e-05,
      "loss": 0.706,
      "step": 40640
    },
    {
      "epoch": 1.6050065147865915,
      "grad_norm": 0.8027932047843933,
      "learning_rate": 1.9751648438425396e-05,
      "loss": 0.6835,
      "step": 40650
    },
    {
      "epoch": 1.6054013503375844,
      "grad_norm": 0.7378674745559692,
      "learning_rate": 1.9731906660875746e-05,
      "loss": 0.6811,
      "step": 40660
    },
    {
      "epoch": 1.6057961858885774,
      "grad_norm": 0.700718104839325,
      "learning_rate": 1.9712164883326096e-05,
      "loss": 0.6916,
      "step": 40670
    },
    {
      "epoch": 1.6061910214395705,
      "grad_norm": 0.7149726748466492,
      "learning_rate": 1.9692423105776445e-05,
      "loss": 0.6857,
      "step": 40680
    },
    {
      "epoch": 1.6065858569905633,
      "grad_norm": 0.8957074284553528,
      "learning_rate": 1.9672681328226795e-05,
      "loss": 0.6896,
      "step": 40690
    },
    {
      "epoch": 1.6069806925415564,
      "grad_norm": 0.8422836661338806,
      "learning_rate": 1.9652939550677142e-05,
      "loss": 0.6706,
      "step": 40700
    },
    {
      "epoch": 1.6073755280925495,
      "grad_norm": 0.8132429122924805,
      "learning_rate": 1.9633197773127492e-05,
      "loss": 0.7376,
      "step": 40710
    },
    {
      "epoch": 1.6077703636435423,
      "grad_norm": 0.7508012652397156,
      "learning_rate": 1.9613455995577842e-05,
      "loss": 0.7061,
      "step": 40720
    },
    {
      "epoch": 1.6081651991945356,
      "grad_norm": 0.9902104139328003,
      "learning_rate": 1.959371421802819e-05,
      "loss": 0.7223,
      "step": 40730
    },
    {
      "epoch": 1.6085600347455284,
      "grad_norm": 0.7086647748947144,
      "learning_rate": 1.957397244047854e-05,
      "loss": 0.6836,
      "step": 40740
    },
    {
      "epoch": 1.6089548702965215,
      "grad_norm": 0.8220637440681458,
      "learning_rate": 1.955423066292889e-05,
      "loss": 0.7046,
      "step": 40750
    },
    {
      "epoch": 1.6093497058475146,
      "grad_norm": 0.7332461476325989,
      "learning_rate": 1.953448888537924e-05,
      "loss": 0.7215,
      "step": 40760
    },
    {
      "epoch": 1.6097445413985074,
      "grad_norm": 0.7757176756858826,
      "learning_rate": 1.951474710782959e-05,
      "loss": 0.7007,
      "step": 40770
    },
    {
      "epoch": 1.6101393769495005,
      "grad_norm": 0.7174513936042786,
      "learning_rate": 1.9495005330279938e-05,
      "loss": 0.7082,
      "step": 40780
    },
    {
      "epoch": 1.6105342125004936,
      "grad_norm": 0.757878303527832,
      "learning_rate": 1.9475263552730288e-05,
      "loss": 0.6528,
      "step": 40790
    },
    {
      "epoch": 1.6109290480514864,
      "grad_norm": 0.7559181451797485,
      "learning_rate": 1.9455521775180638e-05,
      "loss": 0.6908,
      "step": 40800
    },
    {
      "epoch": 1.6113238836024797,
      "grad_norm": 0.8602634072303772,
      "learning_rate": 1.9435779997630988e-05,
      "loss": 0.6974,
      "step": 40810
    },
    {
      "epoch": 1.6117187191534725,
      "grad_norm": 0.6376652717590332,
      "learning_rate": 1.9416038220081337e-05,
      "loss": 0.7911,
      "step": 40820
    },
    {
      "epoch": 1.6121135547044656,
      "grad_norm": 0.6524843573570251,
      "learning_rate": 1.9396296442531687e-05,
      "loss": 0.7293,
      "step": 40830
    },
    {
      "epoch": 1.6125083902554587,
      "grad_norm": 0.7146030068397522,
      "learning_rate": 1.9376554664982037e-05,
      "loss": 0.6925,
      "step": 40840
    },
    {
      "epoch": 1.6129032258064515,
      "grad_norm": 0.8602392673492432,
      "learning_rate": 1.9356812887432384e-05,
      "loss": 0.7057,
      "step": 40850
    },
    {
      "epoch": 1.6132980613574446,
      "grad_norm": 0.7770994305610657,
      "learning_rate": 1.9337071109882734e-05,
      "loss": 0.6614,
      "step": 40860
    },
    {
      "epoch": 1.6136928969084376,
      "grad_norm": 0.7478178143501282,
      "learning_rate": 1.9317329332333084e-05,
      "loss": 0.6999,
      "step": 40870
    },
    {
      "epoch": 1.6140877324594305,
      "grad_norm": 0.9720804691314697,
      "learning_rate": 1.9297587554783434e-05,
      "loss": 0.7114,
      "step": 40880
    },
    {
      "epoch": 1.6144825680104238,
      "grad_norm": 0.6144976615905762,
      "learning_rate": 1.9277845777233783e-05,
      "loss": 0.6952,
      "step": 40890
    },
    {
      "epoch": 1.6148774035614166,
      "grad_norm": 0.792010486125946,
      "learning_rate": 1.9258103999684133e-05,
      "loss": 0.7316,
      "step": 40900
    },
    {
      "epoch": 1.6152722391124097,
      "grad_norm": 0.8623785376548767,
      "learning_rate": 1.9238362222134483e-05,
      "loss": 0.6854,
      "step": 40910
    },
    {
      "epoch": 1.6156670746634028,
      "grad_norm": 0.8757302165031433,
      "learning_rate": 1.921862044458483e-05,
      "loss": 0.6836,
      "step": 40920
    },
    {
      "epoch": 1.6160619102143956,
      "grad_norm": 0.8871709108352661,
      "learning_rate": 1.919887866703518e-05,
      "loss": 0.7037,
      "step": 40930
    },
    {
      "epoch": 1.6164567457653887,
      "grad_norm": 0.8727890849113464,
      "learning_rate": 1.917913688948553e-05,
      "loss": 0.6982,
      "step": 40940
    },
    {
      "epoch": 1.6168515813163817,
      "grad_norm": 0.7766683101654053,
      "learning_rate": 1.915939511193588e-05,
      "loss": 0.7286,
      "step": 40950
    },
    {
      "epoch": 1.6172464168673746,
      "grad_norm": 0.6697719097137451,
      "learning_rate": 1.913965333438623e-05,
      "loss": 0.6984,
      "step": 40960
    },
    {
      "epoch": 1.6176412524183679,
      "grad_norm": 0.7538975477218628,
      "learning_rate": 1.911991155683658e-05,
      "loss": 0.7496,
      "step": 40970
    },
    {
      "epoch": 1.6180360879693607,
      "grad_norm": 0.8763318061828613,
      "learning_rate": 1.910016977928693e-05,
      "loss": 0.6821,
      "step": 40980
    },
    {
      "epoch": 1.6184309235203538,
      "grad_norm": 0.7487843632698059,
      "learning_rate": 1.9080428001737276e-05,
      "loss": 0.7077,
      "step": 40990
    },
    {
      "epoch": 1.6188257590713468,
      "grad_norm": 0.8989836573600769,
      "learning_rate": 1.9060686224187626e-05,
      "loss": 0.6935,
      "step": 41000
    },
    {
      "epoch": 1.6188257590713468,
      "eval_loss": 0.7594522833824158,
      "eval_runtime": 1218.7534,
      "eval_samples_per_second": 9.236,
      "eval_steps_per_second": 9.236,
      "step": 41000
    },
    {
      "epoch": 1.6192205946223397,
      "grad_norm": 0.9215932488441467,
      "learning_rate": 1.9040944446637976e-05,
      "loss": 0.6864,
      "step": 41010
    },
    {
      "epoch": 1.6196154301733328,
      "grad_norm": 0.7779975533485413,
      "learning_rate": 1.9021202669088326e-05,
      "loss": 0.7297,
      "step": 41020
    },
    {
      "epoch": 1.6200102657243258,
      "grad_norm": 0.9068019390106201,
      "learning_rate": 1.9001460891538675e-05,
      "loss": 0.6612,
      "step": 41030
    },
    {
      "epoch": 1.6204051012753187,
      "grad_norm": 0.6250146627426147,
      "learning_rate": 1.8981719113989025e-05,
      "loss": 0.6854,
      "step": 41040
    },
    {
      "epoch": 1.620799936826312,
      "grad_norm": 0.7554544806480408,
      "learning_rate": 1.8961977336439375e-05,
      "loss": 0.7519,
      "step": 41050
    },
    {
      "epoch": 1.6211947723773048,
      "grad_norm": 0.5946546792984009,
      "learning_rate": 1.8942235558889722e-05,
      "loss": 0.6937,
      "step": 41060
    },
    {
      "epoch": 1.6215896079282979,
      "grad_norm": 0.8117381930351257,
      "learning_rate": 1.8922493781340072e-05,
      "loss": 0.7307,
      "step": 41070
    },
    {
      "epoch": 1.621984443479291,
      "grad_norm": 1.0471032857894897,
      "learning_rate": 1.890275200379042e-05,
      "loss": 0.7359,
      "step": 41080
    },
    {
      "epoch": 1.6223792790302838,
      "grad_norm": 0.6807131171226501,
      "learning_rate": 1.888301022624077e-05,
      "loss": 0.6955,
      "step": 41090
    },
    {
      "epoch": 1.6227741145812768,
      "grad_norm": 0.6351495385169983,
      "learning_rate": 1.886326844869112e-05,
      "loss": 0.7123,
      "step": 41100
    },
    {
      "epoch": 1.62316895013227,
      "grad_norm": 0.8289141058921814,
      "learning_rate": 1.884352667114147e-05,
      "loss": 0.7339,
      "step": 41110
    },
    {
      "epoch": 1.6235637856832628,
      "grad_norm": 0.7800928950309753,
      "learning_rate": 1.882378489359182e-05,
      "loss": 0.6717,
      "step": 41120
    },
    {
      "epoch": 1.623958621234256,
      "grad_norm": 0.6996013522148132,
      "learning_rate": 1.8804043116042168e-05,
      "loss": 0.6755,
      "step": 41130
    },
    {
      "epoch": 1.624353456785249,
      "grad_norm": 0.8942441940307617,
      "learning_rate": 1.8784301338492518e-05,
      "loss": 0.7544,
      "step": 41140
    },
    {
      "epoch": 1.624748292336242,
      "grad_norm": 0.7386051416397095,
      "learning_rate": 1.8764559560942868e-05,
      "loss": 0.6557,
      "step": 41150
    },
    {
      "epoch": 1.625143127887235,
      "grad_norm": 0.8510711193084717,
      "learning_rate": 1.8744817783393218e-05,
      "loss": 0.6551,
      "step": 41160
    },
    {
      "epoch": 1.6255379634382279,
      "grad_norm": 0.9614827632904053,
      "learning_rate": 1.8725076005843567e-05,
      "loss": 0.7229,
      "step": 41170
    },
    {
      "epoch": 1.625932798989221,
      "grad_norm": 0.8701969981193542,
      "learning_rate": 1.8705334228293917e-05,
      "loss": 0.7132,
      "step": 41180
    },
    {
      "epoch": 1.626327634540214,
      "grad_norm": 0.8761422634124756,
      "learning_rate": 1.8685592450744267e-05,
      "loss": 0.7237,
      "step": 41190
    },
    {
      "epoch": 1.6267224700912069,
      "grad_norm": 0.8742901086807251,
      "learning_rate": 1.8665850673194614e-05,
      "loss": 0.7075,
      "step": 41200
    },
    {
      "epoch": 1.6271173056422001,
      "grad_norm": 0.8141238689422607,
      "learning_rate": 1.8646108895644964e-05,
      "loss": 0.7095,
      "step": 41210
    },
    {
      "epoch": 1.627512141193193,
      "grad_norm": 0.8251432776451111,
      "learning_rate": 1.8626367118095314e-05,
      "loss": 0.6997,
      "step": 41220
    },
    {
      "epoch": 1.627906976744186,
      "grad_norm": 0.6404747366905212,
      "learning_rate": 1.8606625340545664e-05,
      "loss": 0.7188,
      "step": 41230
    },
    {
      "epoch": 1.6283018122951791,
      "grad_norm": 1.1274032592773438,
      "learning_rate": 1.8586883562996014e-05,
      "loss": 0.7448,
      "step": 41240
    },
    {
      "epoch": 1.628696647846172,
      "grad_norm": 0.7972239255905151,
      "learning_rate": 1.8567141785446363e-05,
      "loss": 0.6881,
      "step": 41250
    },
    {
      "epoch": 1.629091483397165,
      "grad_norm": 0.7797562479972839,
      "learning_rate": 1.8547400007896713e-05,
      "loss": 0.748,
      "step": 41260
    },
    {
      "epoch": 1.629486318948158,
      "grad_norm": 0.6556195616722107,
      "learning_rate": 1.852765823034706e-05,
      "loss": 0.7027,
      "step": 41270
    },
    {
      "epoch": 1.629881154499151,
      "grad_norm": 0.6916125416755676,
      "learning_rate": 1.850791645279741e-05,
      "loss": 0.6772,
      "step": 41280
    },
    {
      "epoch": 1.6302759900501442,
      "grad_norm": 1.0059616565704346,
      "learning_rate": 1.848817467524776e-05,
      "loss": 0.6775,
      "step": 41290
    },
    {
      "epoch": 1.630670825601137,
      "grad_norm": 0.9231522679328918,
      "learning_rate": 1.846843289769811e-05,
      "loss": 0.7605,
      "step": 41300
    },
    {
      "epoch": 1.6310656611521301,
      "grad_norm": 0.8630616068840027,
      "learning_rate": 1.844869112014846e-05,
      "loss": 0.7039,
      "step": 41310
    },
    {
      "epoch": 1.6314604967031232,
      "grad_norm": 0.8261721134185791,
      "learning_rate": 1.842894934259881e-05,
      "loss": 0.6776,
      "step": 41320
    },
    {
      "epoch": 1.631855332254116,
      "grad_norm": 0.7765255570411682,
      "learning_rate": 1.840920756504916e-05,
      "loss": 0.6688,
      "step": 41330
    },
    {
      "epoch": 1.6322501678051091,
      "grad_norm": 0.7200167775154114,
      "learning_rate": 1.8389465787499506e-05,
      "loss": 0.6748,
      "step": 41340
    },
    {
      "epoch": 1.6326450033561022,
      "grad_norm": 0.7424996495246887,
      "learning_rate": 1.8369724009949856e-05,
      "loss": 0.6886,
      "step": 41350
    },
    {
      "epoch": 1.633039838907095,
      "grad_norm": 0.8039376735687256,
      "learning_rate": 1.8349982232400206e-05,
      "loss": 0.677,
      "step": 41360
    },
    {
      "epoch": 1.6334346744580883,
      "grad_norm": 0.8340467810630798,
      "learning_rate": 1.8330240454850556e-05,
      "loss": 0.745,
      "step": 41370
    },
    {
      "epoch": 1.6338295100090812,
      "grad_norm": 0.6408172845840454,
      "learning_rate": 1.8310498677300906e-05,
      "loss": 0.6642,
      "step": 41380
    },
    {
      "epoch": 1.6342243455600742,
      "grad_norm": 0.7823782563209534,
      "learning_rate": 1.8290756899751255e-05,
      "loss": 0.7286,
      "step": 41390
    },
    {
      "epoch": 1.6346191811110673,
      "grad_norm": 0.7815371751785278,
      "learning_rate": 1.8271015122201605e-05,
      "loss": 0.6883,
      "step": 41400
    },
    {
      "epoch": 1.6350140166620601,
      "grad_norm": 0.7933323383331299,
      "learning_rate": 1.8251273344651952e-05,
      "loss": 0.7344,
      "step": 41410
    },
    {
      "epoch": 1.6354088522130532,
      "grad_norm": 0.8229715824127197,
      "learning_rate": 1.8231531567102302e-05,
      "loss": 0.6972,
      "step": 41420
    },
    {
      "epoch": 1.6358036877640463,
      "grad_norm": 0.7380692958831787,
      "learning_rate": 1.8211789789552652e-05,
      "loss": 0.6901,
      "step": 41430
    },
    {
      "epoch": 1.6361985233150391,
      "grad_norm": 0.929901123046875,
      "learning_rate": 1.8192048012003e-05,
      "loss": 0.7062,
      "step": 41440
    },
    {
      "epoch": 1.6365933588660324,
      "grad_norm": 0.892085611820221,
      "learning_rate": 1.817230623445335e-05,
      "loss": 0.6881,
      "step": 41450
    },
    {
      "epoch": 1.6369881944170253,
      "grad_norm": 0.7435389757156372,
      "learning_rate": 1.81525644569037e-05,
      "loss": 0.7232,
      "step": 41460
    },
    {
      "epoch": 1.6373830299680183,
      "grad_norm": 0.7956851124763489,
      "learning_rate": 1.813282267935405e-05,
      "loss": 0.7234,
      "step": 41470
    },
    {
      "epoch": 1.6377778655190114,
      "grad_norm": 0.6501065492630005,
      "learning_rate": 1.8113080901804398e-05,
      "loss": 0.7129,
      "step": 41480
    },
    {
      "epoch": 1.6381727010700042,
      "grad_norm": 0.7663562893867493,
      "learning_rate": 1.8093339124254748e-05,
      "loss": 0.7177,
      "step": 41490
    },
    {
      "epoch": 1.6385675366209973,
      "grad_norm": 0.7574123740196228,
      "learning_rate": 1.8073597346705098e-05,
      "loss": 0.6903,
      "step": 41500
    },
    {
      "epoch": 1.6389623721719904,
      "grad_norm": 0.7769872546195984,
      "learning_rate": 1.8053855569155448e-05,
      "loss": 0.7082,
      "step": 41510
    },
    {
      "epoch": 1.6393572077229832,
      "grad_norm": 0.8986386656761169,
      "learning_rate": 1.8034113791605798e-05,
      "loss": 0.7078,
      "step": 41520
    },
    {
      "epoch": 1.6397520432739765,
      "grad_norm": 0.9743478298187256,
      "learning_rate": 1.8014372014056147e-05,
      "loss": 0.7013,
      "step": 41530
    },
    {
      "epoch": 1.6401468788249693,
      "grad_norm": 0.8209816217422485,
      "learning_rate": 1.7994630236506497e-05,
      "loss": 0.6792,
      "step": 41540
    },
    {
      "epoch": 1.6405417143759624,
      "grad_norm": 0.5979938507080078,
      "learning_rate": 1.7974888458956847e-05,
      "loss": 0.6793,
      "step": 41550
    },
    {
      "epoch": 1.6409365499269555,
      "grad_norm": 0.6548500657081604,
      "learning_rate": 1.7955146681407194e-05,
      "loss": 0.6984,
      "step": 41560
    },
    {
      "epoch": 1.6413313854779483,
      "grad_norm": 1.1528031826019287,
      "learning_rate": 1.7935404903857544e-05,
      "loss": 0.6894,
      "step": 41570
    },
    {
      "epoch": 1.6417262210289414,
      "grad_norm": 0.8365693688392639,
      "learning_rate": 1.7915663126307894e-05,
      "loss": 0.6414,
      "step": 41580
    },
    {
      "epoch": 1.6421210565799345,
      "grad_norm": 0.9104278683662415,
      "learning_rate": 1.7895921348758244e-05,
      "loss": 0.7017,
      "step": 41590
    },
    {
      "epoch": 1.6425158921309275,
      "grad_norm": 0.705462634563446,
      "learning_rate": 1.7876179571208593e-05,
      "loss": 0.7384,
      "step": 41600
    },
    {
      "epoch": 1.6429107276819206,
      "grad_norm": 0.8685086369514465,
      "learning_rate": 1.7856437793658943e-05,
      "loss": 0.6605,
      "step": 41610
    },
    {
      "epoch": 1.6433055632329134,
      "grad_norm": 1.0278418064117432,
      "learning_rate": 1.7836696016109293e-05,
      "loss": 0.6954,
      "step": 41620
    },
    {
      "epoch": 1.6437003987839065,
      "grad_norm": 0.8579517006874084,
      "learning_rate": 1.781695423855964e-05,
      "loss": 0.6851,
      "step": 41630
    },
    {
      "epoch": 1.6440952343348996,
      "grad_norm": 1.0192888975143433,
      "learning_rate": 1.779721246100999e-05,
      "loss": 0.7022,
      "step": 41640
    },
    {
      "epoch": 1.6444900698858924,
      "grad_norm": 0.8162305355072021,
      "learning_rate": 1.777747068346034e-05,
      "loss": 0.7216,
      "step": 41650
    },
    {
      "epoch": 1.6448849054368857,
      "grad_norm": 0.6141889095306396,
      "learning_rate": 1.775772890591069e-05,
      "loss": 0.6948,
      "step": 41660
    },
    {
      "epoch": 1.6452797409878785,
      "grad_norm": 0.7558560371398926,
      "learning_rate": 1.773798712836104e-05,
      "loss": 0.6523,
      "step": 41670
    },
    {
      "epoch": 1.6456745765388716,
      "grad_norm": 0.9680705070495605,
      "learning_rate": 1.771824535081139e-05,
      "loss": 0.7229,
      "step": 41680
    },
    {
      "epoch": 1.6460694120898647,
      "grad_norm": 0.7212762236595154,
      "learning_rate": 1.769850357326174e-05,
      "loss": 0.7284,
      "step": 41690
    },
    {
      "epoch": 1.6464642476408575,
      "grad_norm": 0.6774821877479553,
      "learning_rate": 1.7678761795712086e-05,
      "loss": 0.6974,
      "step": 41700
    },
    {
      "epoch": 1.6468590831918506,
      "grad_norm": 0.9769638776779175,
      "learning_rate": 1.7659020018162436e-05,
      "loss": 0.6975,
      "step": 41710
    },
    {
      "epoch": 1.6472539187428437,
      "grad_norm": 0.675614595413208,
      "learning_rate": 1.7639278240612786e-05,
      "loss": 0.7162,
      "step": 41720
    },
    {
      "epoch": 1.6476487542938365,
      "grad_norm": 0.7060747742652893,
      "learning_rate": 1.7619536463063136e-05,
      "loss": 0.6653,
      "step": 41730
    },
    {
      "epoch": 1.6480435898448298,
      "grad_norm": 0.862693190574646,
      "learning_rate": 1.7599794685513485e-05,
      "loss": 0.7021,
      "step": 41740
    },
    {
      "epoch": 1.6484384253958226,
      "grad_norm": 0.9573610424995422,
      "learning_rate": 1.7580052907963835e-05,
      "loss": 0.7313,
      "step": 41750
    },
    {
      "epoch": 1.6488332609468157,
      "grad_norm": 0.7967763543128967,
      "learning_rate": 1.7560311130414185e-05,
      "loss": 0.7135,
      "step": 41760
    },
    {
      "epoch": 1.6492280964978088,
      "grad_norm": 0.8793359398841858,
      "learning_rate": 1.7540569352864532e-05,
      "loss": 0.7357,
      "step": 41770
    },
    {
      "epoch": 1.6496229320488016,
      "grad_norm": 0.9039514064788818,
      "learning_rate": 1.7520827575314882e-05,
      "loss": 0.7041,
      "step": 41780
    },
    {
      "epoch": 1.6500177675997947,
      "grad_norm": 0.8608463406562805,
      "learning_rate": 1.750108579776523e-05,
      "loss": 0.7264,
      "step": 41790
    },
    {
      "epoch": 1.6504126031507877,
      "grad_norm": 0.869765043258667,
      "learning_rate": 1.748134402021558e-05,
      "loss": 0.6671,
      "step": 41800
    },
    {
      "epoch": 1.6508074387017806,
      "grad_norm": 0.7354884743690491,
      "learning_rate": 1.746160224266593e-05,
      "loss": 0.7254,
      "step": 41810
    },
    {
      "epoch": 1.6512022742527739,
      "grad_norm": 0.8075681924819946,
      "learning_rate": 1.744186046511628e-05,
      "loss": 0.7304,
      "step": 41820
    },
    {
      "epoch": 1.6515971098037667,
      "grad_norm": 0.7440814971923828,
      "learning_rate": 1.742211868756663e-05,
      "loss": 0.7169,
      "step": 41830
    },
    {
      "epoch": 1.6519919453547598,
      "grad_norm": 0.7589097619056702,
      "learning_rate": 1.7402376910016978e-05,
      "loss": 0.7147,
      "step": 41840
    },
    {
      "epoch": 1.6523867809057529,
      "grad_norm": 0.8487566709518433,
      "learning_rate": 1.7382635132467328e-05,
      "loss": 0.7625,
      "step": 41850
    },
    {
      "epoch": 1.6527816164567457,
      "grad_norm": 0.99856036901474,
      "learning_rate": 1.7362893354917678e-05,
      "loss": 0.7063,
      "step": 41860
    },
    {
      "epoch": 1.6531764520077388,
      "grad_norm": 0.7477579116821289,
      "learning_rate": 1.7343151577368028e-05,
      "loss": 0.7081,
      "step": 41870
    },
    {
      "epoch": 1.6535712875587318,
      "grad_norm": 0.8233370184898376,
      "learning_rate": 1.7323409799818377e-05,
      "loss": 0.7267,
      "step": 41880
    },
    {
      "epoch": 1.6539661231097247,
      "grad_norm": 0.8837265968322754,
      "learning_rate": 1.7303668022268727e-05,
      "loss": 0.6967,
      "step": 41890
    },
    {
      "epoch": 1.654360958660718,
      "grad_norm": 0.8164513111114502,
      "learning_rate": 1.7283926244719077e-05,
      "loss": 0.7234,
      "step": 41900
    },
    {
      "epoch": 1.6547557942117108,
      "grad_norm": 0.920096218585968,
      "learning_rate": 1.7264184467169424e-05,
      "loss": 0.693,
      "step": 41910
    },
    {
      "epoch": 1.6551506297627039,
      "grad_norm": 0.7632832527160645,
      "learning_rate": 1.7244442689619774e-05,
      "loss": 0.6622,
      "step": 41920
    },
    {
      "epoch": 1.655545465313697,
      "grad_norm": 0.707508385181427,
      "learning_rate": 1.7224700912070124e-05,
      "loss": 0.7017,
      "step": 41930
    },
    {
      "epoch": 1.6559403008646898,
      "grad_norm": 0.7475948929786682,
      "learning_rate": 1.7204959134520474e-05,
      "loss": 0.6935,
      "step": 41940
    },
    {
      "epoch": 1.6563351364156829,
      "grad_norm": 0.7224045395851135,
      "learning_rate": 1.7185217356970824e-05,
      "loss": 0.7101,
      "step": 41950
    },
    {
      "epoch": 1.656729971966676,
      "grad_norm": 0.9786837697029114,
      "learning_rate": 1.7165475579421173e-05,
      "loss": 0.7182,
      "step": 41960
    },
    {
      "epoch": 1.6571248075176688,
      "grad_norm": 0.8264985084533691,
      "learning_rate": 1.7145733801871523e-05,
      "loss": 0.7004,
      "step": 41970
    },
    {
      "epoch": 1.657519643068662,
      "grad_norm": 0.9918757677078247,
      "learning_rate": 1.712599202432187e-05,
      "loss": 0.7397,
      "step": 41980
    },
    {
      "epoch": 1.657914478619655,
      "grad_norm": 0.7099060416221619,
      "learning_rate": 1.710625024677222e-05,
      "loss": 0.6901,
      "step": 41990
    },
    {
      "epoch": 1.658309314170648,
      "grad_norm": 0.8521968126296997,
      "learning_rate": 1.708650846922257e-05,
      "loss": 0.6887,
      "step": 42000
    },
    {
      "epoch": 1.658309314170648,
      "eval_loss": 0.7584850192070007,
      "eval_runtime": 1218.6874,
      "eval_samples_per_second": 9.237,
      "eval_steps_per_second": 9.237,
      "step": 42000
    },
    {
      "epoch": 1.658704149721641,
      "grad_norm": 0.6537789106369019,
      "learning_rate": 1.706676669167292e-05,
      "loss": 0.6719,
      "step": 42010
    },
    {
      "epoch": 1.6590989852726339,
      "grad_norm": 0.7500476837158203,
      "learning_rate": 1.704702491412327e-05,
      "loss": 0.7159,
      "step": 42020
    },
    {
      "epoch": 1.659493820823627,
      "grad_norm": 0.714409589767456,
      "learning_rate": 1.702728313657362e-05,
      "loss": 0.7021,
      "step": 42030
    },
    {
      "epoch": 1.65988865637462,
      "grad_norm": 0.7975358963012695,
      "learning_rate": 1.700754135902397e-05,
      "loss": 0.6744,
      "step": 42040
    },
    {
      "epoch": 1.6602834919256129,
      "grad_norm": 0.778865396976471,
      "learning_rate": 1.6987799581474316e-05,
      "loss": 0.7343,
      "step": 42050
    },
    {
      "epoch": 1.6606783274766062,
      "grad_norm": 0.9171885848045349,
      "learning_rate": 1.6968057803924666e-05,
      "loss": 0.6656,
      "step": 42060
    },
    {
      "epoch": 1.661073163027599,
      "grad_norm": 0.7448065280914307,
      "learning_rate": 1.6948316026375016e-05,
      "loss": 0.7353,
      "step": 42070
    },
    {
      "epoch": 1.661467998578592,
      "grad_norm": 0.8004050254821777,
      "learning_rate": 1.6928574248825366e-05,
      "loss": 0.6908,
      "step": 42080
    },
    {
      "epoch": 1.6618628341295851,
      "grad_norm": 0.8686687350273132,
      "learning_rate": 1.6908832471275716e-05,
      "loss": 0.7145,
      "step": 42090
    },
    {
      "epoch": 1.662257669680578,
      "grad_norm": 0.6984241008758545,
      "learning_rate": 1.6889090693726065e-05,
      "loss": 0.6834,
      "step": 42100
    },
    {
      "epoch": 1.662652505231571,
      "grad_norm": 0.8941907286643982,
      "learning_rate": 1.6869348916176415e-05,
      "loss": 0.7299,
      "step": 42110
    },
    {
      "epoch": 1.663047340782564,
      "grad_norm": 0.7390344738960266,
      "learning_rate": 1.6849607138626762e-05,
      "loss": 0.6955,
      "step": 42120
    },
    {
      "epoch": 1.663442176333557,
      "grad_norm": 0.9382339715957642,
      "learning_rate": 1.6829865361077112e-05,
      "loss": 0.6899,
      "step": 42130
    },
    {
      "epoch": 1.6638370118845502,
      "grad_norm": 0.6765228509902954,
      "learning_rate": 1.6810123583527462e-05,
      "loss": 0.6972,
      "step": 42140
    },
    {
      "epoch": 1.664231847435543,
      "grad_norm": 0.7570146322250366,
      "learning_rate": 1.679038180597781e-05,
      "loss": 0.7262,
      "step": 42150
    },
    {
      "epoch": 1.6646266829865362,
      "grad_norm": 0.9351701140403748,
      "learning_rate": 1.677064002842816e-05,
      "loss": 0.6883,
      "step": 42160
    },
    {
      "epoch": 1.6650215185375292,
      "grad_norm": 0.7592816352844238,
      "learning_rate": 1.675089825087851e-05,
      "loss": 0.7541,
      "step": 42170
    },
    {
      "epoch": 1.665416354088522,
      "grad_norm": 0.7260425090789795,
      "learning_rate": 1.673115647332886e-05,
      "loss": 0.7074,
      "step": 42180
    },
    {
      "epoch": 1.6658111896395151,
      "grad_norm": 0.8270539045333862,
      "learning_rate": 1.6711414695779208e-05,
      "loss": 0.7074,
      "step": 42190
    },
    {
      "epoch": 1.6662060251905082,
      "grad_norm": 0.9323627352714539,
      "learning_rate": 1.6691672918229558e-05,
      "loss": 0.6969,
      "step": 42200
    },
    {
      "epoch": 1.666600860741501,
      "grad_norm": 0.7242361903190613,
      "learning_rate": 1.6671931140679908e-05,
      "loss": 0.7164,
      "step": 42210
    },
    {
      "epoch": 1.6669956962924943,
      "grad_norm": 0.675524890422821,
      "learning_rate": 1.6652189363130258e-05,
      "loss": 0.718,
      "step": 42220
    },
    {
      "epoch": 1.6673905318434872,
      "grad_norm": 0.7557585835456848,
      "learning_rate": 1.6632447585580608e-05,
      "loss": 0.7067,
      "step": 42230
    },
    {
      "epoch": 1.6677853673944802,
      "grad_norm": 0.9052377939224243,
      "learning_rate": 1.6612705808030957e-05,
      "loss": 0.6693,
      "step": 42240
    },
    {
      "epoch": 1.6681802029454733,
      "grad_norm": 1.0062885284423828,
      "learning_rate": 1.6592964030481307e-05,
      "loss": 0.7424,
      "step": 42250
    },
    {
      "epoch": 1.6685750384964662,
      "grad_norm": 0.7777234315872192,
      "learning_rate": 1.6573222252931654e-05,
      "loss": 0.696,
      "step": 42260
    },
    {
      "epoch": 1.6689698740474592,
      "grad_norm": 0.8328141570091248,
      "learning_rate": 1.6553480475382004e-05,
      "loss": 0.6709,
      "step": 42270
    },
    {
      "epoch": 1.6693647095984523,
      "grad_norm": 0.8021861910820007,
      "learning_rate": 1.6533738697832354e-05,
      "loss": 0.7026,
      "step": 42280
    },
    {
      "epoch": 1.6697595451494451,
      "grad_norm": 0.7118732929229736,
      "learning_rate": 1.6513996920282704e-05,
      "loss": 0.6976,
      "step": 42290
    },
    {
      "epoch": 1.6701543807004384,
      "grad_norm": 0.6726446151733398,
      "learning_rate": 1.6494255142733054e-05,
      "loss": 0.6664,
      "step": 42300
    },
    {
      "epoch": 1.6705492162514313,
      "grad_norm": 0.609832763671875,
      "learning_rate": 1.6474513365183403e-05,
      "loss": 0.6618,
      "step": 42310
    },
    {
      "epoch": 1.6709440518024243,
      "grad_norm": 0.7692704200744629,
      "learning_rate": 1.6454771587633753e-05,
      "loss": 0.7215,
      "step": 42320
    },
    {
      "epoch": 1.6713388873534174,
      "grad_norm": 0.8102081418037415,
      "learning_rate": 1.64350298100841e-05,
      "loss": 0.7288,
      "step": 42330
    },
    {
      "epoch": 1.6717337229044102,
      "grad_norm": 0.7354607582092285,
      "learning_rate": 1.641528803253445e-05,
      "loss": 0.7077,
      "step": 42340
    },
    {
      "epoch": 1.6721285584554033,
      "grad_norm": 0.9734202027320862,
      "learning_rate": 1.63955462549848e-05,
      "loss": 0.756,
      "step": 42350
    },
    {
      "epoch": 1.6725233940063964,
      "grad_norm": 0.7795824408531189,
      "learning_rate": 1.637580447743515e-05,
      "loss": 0.7223,
      "step": 42360
    },
    {
      "epoch": 1.6729182295573892,
      "grad_norm": 0.7224775552749634,
      "learning_rate": 1.63560626998855e-05,
      "loss": 0.7213,
      "step": 42370
    },
    {
      "epoch": 1.6733130651083825,
      "grad_norm": 0.843899667263031,
      "learning_rate": 1.633632092233585e-05,
      "loss": 0.7384,
      "step": 42380
    },
    {
      "epoch": 1.6737079006593754,
      "grad_norm": 0.6416022181510925,
      "learning_rate": 1.63165791447862e-05,
      "loss": 0.7253,
      "step": 42390
    },
    {
      "epoch": 1.6741027362103684,
      "grad_norm": 0.9522533416748047,
      "learning_rate": 1.629683736723655e-05,
      "loss": 0.6892,
      "step": 42400
    },
    {
      "epoch": 1.6744975717613615,
      "grad_norm": 0.8060064911842346,
      "learning_rate": 1.6277095589686896e-05,
      "loss": 0.6949,
      "step": 42410
    },
    {
      "epoch": 1.6748924073123543,
      "grad_norm": 0.6071974635124207,
      "learning_rate": 1.6257353812137246e-05,
      "loss": 0.7124,
      "step": 42420
    },
    {
      "epoch": 1.6752872428633474,
      "grad_norm": 0.6996212601661682,
      "learning_rate": 1.6237612034587596e-05,
      "loss": 0.7204,
      "step": 42430
    },
    {
      "epoch": 1.6756820784143405,
      "grad_norm": 0.6804178357124329,
      "learning_rate": 1.6217870257037946e-05,
      "loss": 0.7375,
      "step": 42440
    },
    {
      "epoch": 1.6760769139653333,
      "grad_norm": 0.8131441473960876,
      "learning_rate": 1.6198128479488295e-05,
      "loss": 0.6986,
      "step": 42450
    },
    {
      "epoch": 1.6764717495163266,
      "grad_norm": 0.7880460619926453,
      "learning_rate": 1.6178386701938645e-05,
      "loss": 0.6704,
      "step": 42460
    },
    {
      "epoch": 1.6768665850673194,
      "grad_norm": 0.7736607789993286,
      "learning_rate": 1.6158644924388995e-05,
      "loss": 0.7188,
      "step": 42470
    },
    {
      "epoch": 1.6772614206183125,
      "grad_norm": 0.606935441493988,
      "learning_rate": 1.6138903146839342e-05,
      "loss": 0.6543,
      "step": 42480
    },
    {
      "epoch": 1.6776562561693056,
      "grad_norm": 0.6608468890190125,
      "learning_rate": 1.6119161369289692e-05,
      "loss": 0.6936,
      "step": 42490
    },
    {
      "epoch": 1.6780510917202984,
      "grad_norm": 0.862800121307373,
      "learning_rate": 1.609941959174004e-05,
      "loss": 0.7107,
      "step": 42500
    },
    {
      "epoch": 1.6784459272712915,
      "grad_norm": 0.8205899000167847,
      "learning_rate": 1.607967781419039e-05,
      "loss": 0.7026,
      "step": 42510
    },
    {
      "epoch": 1.6788407628222846,
      "grad_norm": 0.8506867289543152,
      "learning_rate": 1.605993603664074e-05,
      "loss": 0.7421,
      "step": 42520
    },
    {
      "epoch": 1.6792355983732774,
      "grad_norm": 0.667389988899231,
      "learning_rate": 1.604019425909109e-05,
      "loss": 0.6786,
      "step": 42530
    },
    {
      "epoch": 1.6796304339242707,
      "grad_norm": 0.6535105109214783,
      "learning_rate": 1.602045248154144e-05,
      "loss": 0.7128,
      "step": 42540
    },
    {
      "epoch": 1.6800252694752635,
      "grad_norm": 0.7601902484893799,
      "learning_rate": 1.6000710703991788e-05,
      "loss": 0.6773,
      "step": 42550
    },
    {
      "epoch": 1.6804201050262566,
      "grad_norm": 0.6853342652320862,
      "learning_rate": 1.5980968926442138e-05,
      "loss": 0.6881,
      "step": 42560
    },
    {
      "epoch": 1.6808149405772497,
      "grad_norm": 0.8494068384170532,
      "learning_rate": 1.5961227148892488e-05,
      "loss": 0.7212,
      "step": 42570
    },
    {
      "epoch": 1.6812097761282425,
      "grad_norm": 0.8362112045288086,
      "learning_rate": 1.5941485371342838e-05,
      "loss": 0.7065,
      "step": 42580
    },
    {
      "epoch": 1.6816046116792356,
      "grad_norm": 0.860127329826355,
      "learning_rate": 1.5921743593793188e-05,
      "loss": 0.7452,
      "step": 42590
    },
    {
      "epoch": 1.6819994472302287,
      "grad_norm": 0.6382377743721008,
      "learning_rate": 1.5902001816243537e-05,
      "loss": 0.6909,
      "step": 42600
    },
    {
      "epoch": 1.6823942827812215,
      "grad_norm": 0.8216230273246765,
      "learning_rate": 1.5882260038693887e-05,
      "loss": 0.7616,
      "step": 42610
    },
    {
      "epoch": 1.6827891183322148,
      "grad_norm": 0.8454383015632629,
      "learning_rate": 1.5862518261144234e-05,
      "loss": 0.7592,
      "step": 42620
    },
    {
      "epoch": 1.6831839538832076,
      "grad_norm": 0.74754798412323,
      "learning_rate": 1.5842776483594584e-05,
      "loss": 0.6658,
      "step": 42630
    },
    {
      "epoch": 1.6835787894342007,
      "grad_norm": 0.73845374584198,
      "learning_rate": 1.5823034706044934e-05,
      "loss": 0.6601,
      "step": 42640
    },
    {
      "epoch": 1.6839736249851938,
      "grad_norm": 0.7913966774940491,
      "learning_rate": 1.5803292928495284e-05,
      "loss": 0.6706,
      "step": 42650
    },
    {
      "epoch": 1.6843684605361866,
      "grad_norm": 0.7834094166755676,
      "learning_rate": 1.5783551150945634e-05,
      "loss": 0.6936,
      "step": 42660
    },
    {
      "epoch": 1.6847632960871797,
      "grad_norm": 0.6767818331718445,
      "learning_rate": 1.5763809373395983e-05,
      "loss": 0.6812,
      "step": 42670
    },
    {
      "epoch": 1.6851581316381727,
      "grad_norm": 0.8461318016052246,
      "learning_rate": 1.5744067595846333e-05,
      "loss": 0.688,
      "step": 42680
    },
    {
      "epoch": 1.6855529671891656,
      "grad_norm": 0.7988944053649902,
      "learning_rate": 1.572432581829668e-05,
      "loss": 0.721,
      "step": 42690
    },
    {
      "epoch": 1.6859478027401589,
      "grad_norm": 0.7509215474128723,
      "learning_rate": 1.570458404074703e-05,
      "loss": 0.6877,
      "step": 42700
    },
    {
      "epoch": 1.6863426382911517,
      "grad_norm": 1.1324342489242554,
      "learning_rate": 1.568484226319738e-05,
      "loss": 0.6893,
      "step": 42710
    },
    {
      "epoch": 1.6867374738421448,
      "grad_norm": 0.8639966249465942,
      "learning_rate": 1.566510048564773e-05,
      "loss": 0.6878,
      "step": 42720
    },
    {
      "epoch": 1.6871323093931379,
      "grad_norm": 0.8485347628593445,
      "learning_rate": 1.564535870809808e-05,
      "loss": 0.7503,
      "step": 42730
    },
    {
      "epoch": 1.6875271449441307,
      "grad_norm": 0.6726369261741638,
      "learning_rate": 1.562561693054843e-05,
      "loss": 0.6836,
      "step": 42740
    },
    {
      "epoch": 1.6879219804951238,
      "grad_norm": 0.7737798094749451,
      "learning_rate": 1.560587515299878e-05,
      "loss": 0.7088,
      "step": 42750
    },
    {
      "epoch": 1.6883168160461168,
      "grad_norm": 0.7591453194618225,
      "learning_rate": 1.5586133375449126e-05,
      "loss": 0.733,
      "step": 42760
    },
    {
      "epoch": 1.6887116515971097,
      "grad_norm": 0.649039626121521,
      "learning_rate": 1.5566391597899476e-05,
      "loss": 0.6792,
      "step": 42770
    },
    {
      "epoch": 1.689106487148103,
      "grad_norm": 0.7396018505096436,
      "learning_rate": 1.5546649820349826e-05,
      "loss": 0.6967,
      "step": 42780
    },
    {
      "epoch": 1.6895013226990958,
      "grad_norm": 0.807721734046936,
      "learning_rate": 1.5526908042800176e-05,
      "loss": 0.7089,
      "step": 42790
    },
    {
      "epoch": 1.6898961582500889,
      "grad_norm": 0.6671311855316162,
      "learning_rate": 1.5507166265250526e-05,
      "loss": 0.7049,
      "step": 42800
    },
    {
      "epoch": 1.690290993801082,
      "grad_norm": 0.9838877320289612,
      "learning_rate": 1.5487424487700875e-05,
      "loss": 0.7098,
      "step": 42810
    },
    {
      "epoch": 1.6906858293520748,
      "grad_norm": 0.7656440734863281,
      "learning_rate": 1.5467682710151225e-05,
      "loss": 0.6944,
      "step": 42820
    },
    {
      "epoch": 1.6910806649030679,
      "grad_norm": 0.9529087543487549,
      "learning_rate": 1.5447940932601572e-05,
      "loss": 0.7013,
      "step": 42830
    },
    {
      "epoch": 1.691475500454061,
      "grad_norm": 0.812799334526062,
      "learning_rate": 1.5428199155051922e-05,
      "loss": 0.7098,
      "step": 42840
    },
    {
      "epoch": 1.6918703360050538,
      "grad_norm": 1.033332347869873,
      "learning_rate": 1.5408457377502272e-05,
      "loss": 0.6677,
      "step": 42850
    },
    {
      "epoch": 1.692265171556047,
      "grad_norm": 1.030572533607483,
      "learning_rate": 1.538871559995262e-05,
      "loss": 0.7151,
      "step": 42860
    },
    {
      "epoch": 1.69266000710704,
      "grad_norm": 0.9270928502082825,
      "learning_rate": 1.536897382240297e-05,
      "loss": 0.685,
      "step": 42870
    },
    {
      "epoch": 1.693054842658033,
      "grad_norm": 0.8859418034553528,
      "learning_rate": 1.534923204485332e-05,
      "loss": 0.7335,
      "step": 42880
    },
    {
      "epoch": 1.693449678209026,
      "grad_norm": 0.8801677823066711,
      "learning_rate": 1.532949026730367e-05,
      "loss": 0.7091,
      "step": 42890
    },
    {
      "epoch": 1.6938445137600189,
      "grad_norm": 0.7713164687156677,
      "learning_rate": 1.5309748489754018e-05,
      "loss": 0.7215,
      "step": 42900
    },
    {
      "epoch": 1.694239349311012,
      "grad_norm": 0.807245671749115,
      "learning_rate": 1.5290006712204368e-05,
      "loss": 0.6749,
      "step": 42910
    },
    {
      "epoch": 1.694634184862005,
      "grad_norm": 0.8078880310058594,
      "learning_rate": 1.5270264934654718e-05,
      "loss": 0.7155,
      "step": 42920
    },
    {
      "epoch": 1.6950290204129979,
      "grad_norm": 0.9241607189178467,
      "learning_rate": 1.5250523157105068e-05,
      "loss": 0.7216,
      "step": 42930
    },
    {
      "epoch": 1.6954238559639911,
      "grad_norm": 0.6617060899734497,
      "learning_rate": 1.5230781379555418e-05,
      "loss": 0.7073,
      "step": 42940
    },
    {
      "epoch": 1.695818691514984,
      "grad_norm": 0.8423313498497009,
      "learning_rate": 1.5211039602005766e-05,
      "loss": 0.7066,
      "step": 42950
    },
    {
      "epoch": 1.696213527065977,
      "grad_norm": 0.9560360312461853,
      "learning_rate": 1.5191297824456116e-05,
      "loss": 0.7017,
      "step": 42960
    },
    {
      "epoch": 1.6966083626169701,
      "grad_norm": 0.6222502589225769,
      "learning_rate": 1.5171556046906466e-05,
      "loss": 0.7036,
      "step": 42970
    },
    {
      "epoch": 1.697003198167963,
      "grad_norm": 0.8287948369979858,
      "learning_rate": 1.5151814269356816e-05,
      "loss": 0.7215,
      "step": 42980
    },
    {
      "epoch": 1.697398033718956,
      "grad_norm": 1.172200322151184,
      "learning_rate": 1.5132072491807164e-05,
      "loss": 0.7046,
      "step": 42990
    },
    {
      "epoch": 1.697792869269949,
      "grad_norm": 0.8759077787399292,
      "learning_rate": 1.5112330714257514e-05,
      "loss": 0.7177,
      "step": 43000
    },
    {
      "epoch": 1.697792869269949,
      "eval_loss": 0.7577422261238098,
      "eval_runtime": 1220.0129,
      "eval_samples_per_second": 9.227,
      "eval_steps_per_second": 9.227,
      "step": 43000
    },
    {
      "epoch": 1.698187704820942,
      "grad_norm": 0.7600517868995667,
      "learning_rate": 1.5092588936707864e-05,
      "loss": 0.6873,
      "step": 43010
    },
    {
      "epoch": 1.6985825403719352,
      "grad_norm": 0.9629853367805481,
      "learning_rate": 1.5072847159158213e-05,
      "loss": 0.7288,
      "step": 43020
    },
    {
      "epoch": 1.698977375922928,
      "grad_norm": 0.9172294735908508,
      "learning_rate": 1.5053105381608562e-05,
      "loss": 0.7371,
      "step": 43030
    },
    {
      "epoch": 1.6993722114739211,
      "grad_norm": 0.7051848769187927,
      "learning_rate": 1.5033363604058912e-05,
      "loss": 0.7116,
      "step": 43040
    },
    {
      "epoch": 1.6997670470249142,
      "grad_norm": 0.9592563509941101,
      "learning_rate": 1.5013621826509262e-05,
      "loss": 0.6717,
      "step": 43050
    },
    {
      "epoch": 1.700161882575907,
      "grad_norm": 0.9641568064689636,
      "learning_rate": 1.499388004895961e-05,
      "loss": 0.7578,
      "step": 43060
    },
    {
      "epoch": 1.7005567181269001,
      "grad_norm": 0.7721495032310486,
      "learning_rate": 1.497413827140996e-05,
      "loss": 0.7637,
      "step": 43070
    },
    {
      "epoch": 1.7009515536778932,
      "grad_norm": 0.9421365261077881,
      "learning_rate": 1.495439649386031e-05,
      "loss": 0.6896,
      "step": 43080
    },
    {
      "epoch": 1.701346389228886,
      "grad_norm": 0.8802646994590759,
      "learning_rate": 1.493465471631066e-05,
      "loss": 0.7222,
      "step": 43090
    },
    {
      "epoch": 1.7017412247798793,
      "grad_norm": 0.8447570204734802,
      "learning_rate": 1.4914912938761006e-05,
      "loss": 0.7306,
      "step": 43100
    },
    {
      "epoch": 1.7021360603308722,
      "grad_norm": 0.9287092685699463,
      "learning_rate": 1.4895171161211354e-05,
      "loss": 0.7265,
      "step": 43110
    },
    {
      "epoch": 1.7025308958818652,
      "grad_norm": 0.8280193209648132,
      "learning_rate": 1.4875429383661704e-05,
      "loss": 0.6953,
      "step": 43120
    },
    {
      "epoch": 1.7029257314328583,
      "grad_norm": 0.7517442107200623,
      "learning_rate": 1.4855687606112054e-05,
      "loss": 0.7037,
      "step": 43130
    },
    {
      "epoch": 1.7033205669838511,
      "grad_norm": 0.6935744285583496,
      "learning_rate": 1.4835945828562402e-05,
      "loss": 0.7209,
      "step": 43140
    },
    {
      "epoch": 1.7037154025348442,
      "grad_norm": 0.8542420864105225,
      "learning_rate": 1.4816204051012752e-05,
      "loss": 0.7509,
      "step": 43150
    },
    {
      "epoch": 1.7041102380858373,
      "grad_norm": 0.7075847387313843,
      "learning_rate": 1.4796462273463102e-05,
      "loss": 0.6702,
      "step": 43160
    },
    {
      "epoch": 1.7045050736368301,
      "grad_norm": 1.0034544467926025,
      "learning_rate": 1.4776720495913452e-05,
      "loss": 0.6928,
      "step": 43170
    },
    {
      "epoch": 1.7048999091878234,
      "grad_norm": 0.749113142490387,
      "learning_rate": 1.47569787183638e-05,
      "loss": 0.7076,
      "step": 43180
    },
    {
      "epoch": 1.7052947447388163,
      "grad_norm": 0.6773145794868469,
      "learning_rate": 1.473723694081415e-05,
      "loss": 0.6898,
      "step": 43190
    },
    {
      "epoch": 1.7056895802898093,
      "grad_norm": 0.7337566614151001,
      "learning_rate": 1.47174951632645e-05,
      "loss": 0.7081,
      "step": 43200
    },
    {
      "epoch": 1.7060844158408024,
      "grad_norm": 0.8385518193244934,
      "learning_rate": 1.469775338571485e-05,
      "loss": 0.6706,
      "step": 43210
    },
    {
      "epoch": 1.7064792513917952,
      "grad_norm": 0.8944440484046936,
      "learning_rate": 1.4678011608165198e-05,
      "loss": 0.6908,
      "step": 43220
    },
    {
      "epoch": 1.7068740869427883,
      "grad_norm": 0.9180130958557129,
      "learning_rate": 1.4658269830615548e-05,
      "loss": 0.7334,
      "step": 43230
    },
    {
      "epoch": 1.7072689224937814,
      "grad_norm": 0.7427576184272766,
      "learning_rate": 1.4638528053065898e-05,
      "loss": 0.7205,
      "step": 43240
    },
    {
      "epoch": 1.7076637580447742,
      "grad_norm": 0.8079261183738708,
      "learning_rate": 1.4618786275516246e-05,
      "loss": 0.704,
      "step": 43250
    },
    {
      "epoch": 1.7080585935957675,
      "grad_norm": 0.9068697690963745,
      "learning_rate": 1.4599044497966596e-05,
      "loss": 0.7184,
      "step": 43260
    },
    {
      "epoch": 1.7084534291467603,
      "grad_norm": 0.7946810126304626,
      "learning_rate": 1.4579302720416946e-05,
      "loss": 0.7266,
      "step": 43270
    },
    {
      "epoch": 1.7088482646977534,
      "grad_norm": 0.697133481502533,
      "learning_rate": 1.4559560942867296e-05,
      "loss": 0.6601,
      "step": 43280
    },
    {
      "epoch": 1.7092431002487465,
      "grad_norm": 1.0737041234970093,
      "learning_rate": 1.4539819165317644e-05,
      "loss": 0.7296,
      "step": 43290
    },
    {
      "epoch": 1.7096379357997393,
      "grad_norm": 0.8075931072235107,
      "learning_rate": 1.4520077387767994e-05,
      "loss": 0.7229,
      "step": 43300
    },
    {
      "epoch": 1.7100327713507324,
      "grad_norm": 0.6792722940444946,
      "learning_rate": 1.4500335610218344e-05,
      "loss": 0.7017,
      "step": 43310
    },
    {
      "epoch": 1.7104276069017255,
      "grad_norm": 0.9382007718086243,
      "learning_rate": 1.4480593832668692e-05,
      "loss": 0.755,
      "step": 43320
    },
    {
      "epoch": 1.7108224424527183,
      "grad_norm": 0.8011277318000793,
      "learning_rate": 1.4460852055119042e-05,
      "loss": 0.7109,
      "step": 43330
    },
    {
      "epoch": 1.7112172780037116,
      "grad_norm": 0.9807682037353516,
      "learning_rate": 1.4441110277569392e-05,
      "loss": 0.7202,
      "step": 43340
    },
    {
      "epoch": 1.7116121135547044,
      "grad_norm": 0.7243692874908447,
      "learning_rate": 1.4421368500019742e-05,
      "loss": 0.7377,
      "step": 43350
    },
    {
      "epoch": 1.7120069491056975,
      "grad_norm": 0.671626091003418,
      "learning_rate": 1.440162672247009e-05,
      "loss": 0.6982,
      "step": 43360
    },
    {
      "epoch": 1.7124017846566906,
      "grad_norm": 0.7484419345855713,
      "learning_rate": 1.438188494492044e-05,
      "loss": 0.6969,
      "step": 43370
    },
    {
      "epoch": 1.7127966202076834,
      "grad_norm": 1.1024301052093506,
      "learning_rate": 1.436214316737079e-05,
      "loss": 0.6784,
      "step": 43380
    },
    {
      "epoch": 1.7131914557586765,
      "grad_norm": 0.8928332328796387,
      "learning_rate": 1.4342401389821138e-05,
      "loss": 0.6921,
      "step": 43390
    },
    {
      "epoch": 1.7135862913096696,
      "grad_norm": 1.1902860403060913,
      "learning_rate": 1.4322659612271488e-05,
      "loss": 0.738,
      "step": 43400
    },
    {
      "epoch": 1.7139811268606624,
      "grad_norm": 0.7587305903434753,
      "learning_rate": 1.4302917834721838e-05,
      "loss": 0.7055,
      "step": 43410
    },
    {
      "epoch": 1.7143759624116557,
      "grad_norm": 0.9822871685028076,
      "learning_rate": 1.4283176057172188e-05,
      "loss": 0.6696,
      "step": 43420
    },
    {
      "epoch": 1.7147707979626485,
      "grad_norm": 1.1591851711273193,
      "learning_rate": 1.4263434279622536e-05,
      "loss": 0.7157,
      "step": 43430
    },
    {
      "epoch": 1.7151656335136416,
      "grad_norm": 0.9174156785011292,
      "learning_rate": 1.4243692502072886e-05,
      "loss": 0.7096,
      "step": 43440
    },
    {
      "epoch": 1.7155604690646347,
      "grad_norm": 0.8873701095581055,
      "learning_rate": 1.4223950724523236e-05,
      "loss": 0.655,
      "step": 43450
    },
    {
      "epoch": 1.7159553046156275,
      "grad_norm": 0.7693236470222473,
      "learning_rate": 1.4204208946973584e-05,
      "loss": 0.6787,
      "step": 43460
    },
    {
      "epoch": 1.7163501401666206,
      "grad_norm": 1.177680253982544,
      "learning_rate": 1.4184467169423934e-05,
      "loss": 0.697,
      "step": 43470
    },
    {
      "epoch": 1.7167449757176136,
      "grad_norm": 0.611075222492218,
      "learning_rate": 1.4164725391874284e-05,
      "loss": 0.7671,
      "step": 43480
    },
    {
      "epoch": 1.7171398112686065,
      "grad_norm": 0.8072578310966492,
      "learning_rate": 1.4144983614324634e-05,
      "loss": 0.6742,
      "step": 43490
    },
    {
      "epoch": 1.7175346468195998,
      "grad_norm": 0.6935592889785767,
      "learning_rate": 1.4125241836774982e-05,
      "loss": 0.6795,
      "step": 43500
    },
    {
      "epoch": 1.7179294823705926,
      "grad_norm": 0.8643595576286316,
      "learning_rate": 1.4105500059225332e-05,
      "loss": 0.6883,
      "step": 43510
    },
    {
      "epoch": 1.7183243179215857,
      "grad_norm": 0.7296835780143738,
      "learning_rate": 1.4085758281675682e-05,
      "loss": 0.6862,
      "step": 43520
    },
    {
      "epoch": 1.7187191534725788,
      "grad_norm": 0.7507984638214111,
      "learning_rate": 1.406601650412603e-05,
      "loss": 0.7075,
      "step": 43530
    },
    {
      "epoch": 1.7191139890235716,
      "grad_norm": 0.7816837430000305,
      "learning_rate": 1.404627472657638e-05,
      "loss": 0.6715,
      "step": 43540
    },
    {
      "epoch": 1.7195088245745647,
      "grad_norm": 0.9784966707229614,
      "learning_rate": 1.402653294902673e-05,
      "loss": 0.6889,
      "step": 43550
    },
    {
      "epoch": 1.7199036601255577,
      "grad_norm": 0.9590336680412292,
      "learning_rate": 1.400679117147708e-05,
      "loss": 0.7419,
      "step": 43560
    },
    {
      "epoch": 1.7202984956765506,
      "grad_norm": 0.657202959060669,
      "learning_rate": 1.3987049393927428e-05,
      "loss": 0.7618,
      "step": 43570
    },
    {
      "epoch": 1.7206933312275439,
      "grad_norm": 0.7600111961364746,
      "learning_rate": 1.3967307616377778e-05,
      "loss": 0.6451,
      "step": 43580
    },
    {
      "epoch": 1.7210881667785367,
      "grad_norm": 0.6471624374389648,
      "learning_rate": 1.3947565838828128e-05,
      "loss": 0.6936,
      "step": 43590
    },
    {
      "epoch": 1.7214830023295298,
      "grad_norm": 0.9136101007461548,
      "learning_rate": 1.3927824061278478e-05,
      "loss": 0.7181,
      "step": 43600
    },
    {
      "epoch": 1.7218778378805228,
      "grad_norm": 0.7396796941757202,
      "learning_rate": 1.3908082283728826e-05,
      "loss": 0.7522,
      "step": 43610
    },
    {
      "epoch": 1.7222726734315157,
      "grad_norm": 0.7804473638534546,
      "learning_rate": 1.3888340506179176e-05,
      "loss": 0.7302,
      "step": 43620
    },
    {
      "epoch": 1.7226675089825088,
      "grad_norm": 0.9284936785697937,
      "learning_rate": 1.3868598728629526e-05,
      "loss": 0.6934,
      "step": 43630
    },
    {
      "epoch": 1.7230623445335018,
      "grad_norm": 0.8977785110473633,
      "learning_rate": 1.3848856951079874e-05,
      "loss": 0.7099,
      "step": 43640
    },
    {
      "epoch": 1.7234571800844947,
      "grad_norm": 1.0613895654678345,
      "learning_rate": 1.3829115173530224e-05,
      "loss": 0.7074,
      "step": 43650
    },
    {
      "epoch": 1.723852015635488,
      "grad_norm": 0.8089472055435181,
      "learning_rate": 1.3809373395980574e-05,
      "loss": 0.7251,
      "step": 43660
    },
    {
      "epoch": 1.7242468511864808,
      "grad_norm": 0.6735473275184631,
      "learning_rate": 1.3789631618430924e-05,
      "loss": 0.7123,
      "step": 43670
    },
    {
      "epoch": 1.7246416867374739,
      "grad_norm": 0.8041039705276489,
      "learning_rate": 1.3769889840881272e-05,
      "loss": 0.7077,
      "step": 43680
    },
    {
      "epoch": 1.725036522288467,
      "grad_norm": 0.8305646777153015,
      "learning_rate": 1.3750148063331622e-05,
      "loss": 0.6651,
      "step": 43690
    },
    {
      "epoch": 1.7254313578394598,
      "grad_norm": 0.9900418519973755,
      "learning_rate": 1.3730406285781972e-05,
      "loss": 0.6667,
      "step": 43700
    },
    {
      "epoch": 1.7258261933904528,
      "grad_norm": 0.929355800151825,
      "learning_rate": 1.371066450823232e-05,
      "loss": 0.7196,
      "step": 43710
    },
    {
      "epoch": 1.726221028941446,
      "grad_norm": 0.7281988263130188,
      "learning_rate": 1.369092273068267e-05,
      "loss": 0.6538,
      "step": 43720
    },
    {
      "epoch": 1.7266158644924388,
      "grad_norm": 0.6270928978919983,
      "learning_rate": 1.367118095313302e-05,
      "loss": 0.6965,
      "step": 43730
    },
    {
      "epoch": 1.727010700043432,
      "grad_norm": 0.8563603758811951,
      "learning_rate": 1.365143917558337e-05,
      "loss": 0.6648,
      "step": 43740
    },
    {
      "epoch": 1.727405535594425,
      "grad_norm": 1.3123292922973633,
      "learning_rate": 1.3631697398033718e-05,
      "loss": 0.7358,
      "step": 43750
    },
    {
      "epoch": 1.727800371145418,
      "grad_norm": 0.8114360570907593,
      "learning_rate": 1.3611955620484068e-05,
      "loss": 0.6548,
      "step": 43760
    },
    {
      "epoch": 1.728195206696411,
      "grad_norm": 0.7212099432945251,
      "learning_rate": 1.3592213842934418e-05,
      "loss": 0.7295,
      "step": 43770
    },
    {
      "epoch": 1.7285900422474039,
      "grad_norm": 0.8107203245162964,
      "learning_rate": 1.3572472065384766e-05,
      "loss": 0.7087,
      "step": 43780
    },
    {
      "epoch": 1.728984877798397,
      "grad_norm": 0.7389864325523376,
      "learning_rate": 1.3552730287835116e-05,
      "loss": 0.6878,
      "step": 43790
    },
    {
      "epoch": 1.72937971334939,
      "grad_norm": 0.8612627387046814,
      "learning_rate": 1.3532988510285466e-05,
      "loss": 0.7029,
      "step": 43800
    },
    {
      "epoch": 1.7297745489003828,
      "grad_norm": 0.6700525879859924,
      "learning_rate": 1.3513246732735816e-05,
      "loss": 0.6769,
      "step": 43810
    },
    {
      "epoch": 1.7301693844513761,
      "grad_norm": 0.8093128204345703,
      "learning_rate": 1.3493504955186164e-05,
      "loss": 0.748,
      "step": 43820
    },
    {
      "epoch": 1.730564220002369,
      "grad_norm": 0.792700469493866,
      "learning_rate": 1.3473763177636514e-05,
      "loss": 0.7061,
      "step": 43830
    },
    {
      "epoch": 1.730959055553362,
      "grad_norm": 0.7246525883674622,
      "learning_rate": 1.3454021400086864e-05,
      "loss": 0.7127,
      "step": 43840
    },
    {
      "epoch": 1.7313538911043551,
      "grad_norm": 0.7209683060646057,
      "learning_rate": 1.3434279622537212e-05,
      "loss": 0.6807,
      "step": 43850
    },
    {
      "epoch": 1.731748726655348,
      "grad_norm": 0.7267451882362366,
      "learning_rate": 1.3414537844987562e-05,
      "loss": 0.7223,
      "step": 43860
    },
    {
      "epoch": 1.732143562206341,
      "grad_norm": 1.0045222043991089,
      "learning_rate": 1.3394796067437912e-05,
      "loss": 0.7244,
      "step": 43870
    },
    {
      "epoch": 1.732538397757334,
      "grad_norm": 0.9521914720535278,
      "learning_rate": 1.3375054289888262e-05,
      "loss": 0.7166,
      "step": 43880
    },
    {
      "epoch": 1.732933233308327,
      "grad_norm": 0.657595157623291,
      "learning_rate": 1.335531251233861e-05,
      "loss": 0.6992,
      "step": 43890
    },
    {
      "epoch": 1.7333280688593202,
      "grad_norm": 0.7646043300628662,
      "learning_rate": 1.333557073478896e-05,
      "loss": 0.7124,
      "step": 43900
    },
    {
      "epoch": 1.733722904410313,
      "grad_norm": 1.0242924690246582,
      "learning_rate": 1.331582895723931e-05,
      "loss": 0.734,
      "step": 43910
    },
    {
      "epoch": 1.7341177399613061,
      "grad_norm": 1.1886509656906128,
      "learning_rate": 1.3296087179689658e-05,
      "loss": 0.6905,
      "step": 43920
    },
    {
      "epoch": 1.7345125755122992,
      "grad_norm": 0.9049448370933533,
      "learning_rate": 1.3276345402140008e-05,
      "loss": 0.7064,
      "step": 43930
    },
    {
      "epoch": 1.734907411063292,
      "grad_norm": 0.8616165518760681,
      "learning_rate": 1.3256603624590358e-05,
      "loss": 0.699,
      "step": 43940
    },
    {
      "epoch": 1.7353022466142851,
      "grad_norm": 0.7026240825653076,
      "learning_rate": 1.3236861847040708e-05,
      "loss": 0.6844,
      "step": 43950
    },
    {
      "epoch": 1.7356970821652782,
      "grad_norm": 0.9074879884719849,
      "learning_rate": 1.3217120069491056e-05,
      "loss": 0.7052,
      "step": 43960
    },
    {
      "epoch": 1.736091917716271,
      "grad_norm": 0.8058646321296692,
      "learning_rate": 1.3197378291941406e-05,
      "loss": 0.7047,
      "step": 43970
    },
    {
      "epoch": 1.7364867532672643,
      "grad_norm": 0.6677791476249695,
      "learning_rate": 1.3177636514391756e-05,
      "loss": 0.6735,
      "step": 43980
    },
    {
      "epoch": 1.7368815888182572,
      "grad_norm": 0.725473165512085,
      "learning_rate": 1.3157894736842106e-05,
      "loss": 0.7096,
      "step": 43990
    },
    {
      "epoch": 1.7372764243692502,
      "grad_norm": 0.6256405711174011,
      "learning_rate": 1.3138152959292454e-05,
      "loss": 0.7157,
      "step": 44000
    },
    {
      "epoch": 1.7372764243692502,
      "eval_loss": 0.7569719552993774,
      "eval_runtime": 1221.2282,
      "eval_samples_per_second": 9.218,
      "eval_steps_per_second": 9.218,
      "step": 44000
    },
    {
      "epoch": 1.7376712599202433,
      "grad_norm": 0.7476789951324463,
      "learning_rate": 1.3118411181742804e-05,
      "loss": 0.6821,
      "step": 44010
    },
    {
      "epoch": 1.7380660954712361,
      "grad_norm": 1.0509880781173706,
      "learning_rate": 1.3098669404193154e-05,
      "loss": 0.7376,
      "step": 44020
    },
    {
      "epoch": 1.7384609310222292,
      "grad_norm": 0.7453132271766663,
      "learning_rate": 1.3078927626643502e-05,
      "loss": 0.701,
      "step": 44030
    },
    {
      "epoch": 1.7388557665732223,
      "grad_norm": 0.8289363384246826,
      "learning_rate": 1.3059185849093852e-05,
      "loss": 0.7124,
      "step": 44040
    },
    {
      "epoch": 1.7392506021242151,
      "grad_norm": 0.8471128344535828,
      "learning_rate": 1.3039444071544202e-05,
      "loss": 0.6639,
      "step": 44050
    },
    {
      "epoch": 1.7396454376752084,
      "grad_norm": 0.8820219039916992,
      "learning_rate": 1.3019702293994552e-05,
      "loss": 0.664,
      "step": 44060
    },
    {
      "epoch": 1.7400402732262013,
      "grad_norm": 0.7667152285575867,
      "learning_rate": 1.29999605164449e-05,
      "loss": 0.7041,
      "step": 44070
    },
    {
      "epoch": 1.7404351087771943,
      "grad_norm": 0.7518653869628906,
      "learning_rate": 1.298021873889525e-05,
      "loss": 0.6482,
      "step": 44080
    },
    {
      "epoch": 1.7408299443281874,
      "grad_norm": 0.8638657927513123,
      "learning_rate": 1.29604769613456e-05,
      "loss": 0.7259,
      "step": 44090
    },
    {
      "epoch": 1.7412247798791802,
      "grad_norm": 0.8862842321395874,
      "learning_rate": 1.2940735183795948e-05,
      "loss": 0.7102,
      "step": 44100
    },
    {
      "epoch": 1.7416196154301733,
      "grad_norm": 0.7392982840538025,
      "learning_rate": 1.2920993406246298e-05,
      "loss": 0.72,
      "step": 44110
    },
    {
      "epoch": 1.7420144509811664,
      "grad_norm": 0.8959053158760071,
      "learning_rate": 1.2901251628696648e-05,
      "loss": 0.7215,
      "step": 44120
    },
    {
      "epoch": 1.7424092865321592,
      "grad_norm": 0.7856422066688538,
      "learning_rate": 1.2881509851146998e-05,
      "loss": 0.7031,
      "step": 44130
    },
    {
      "epoch": 1.7428041220831525,
      "grad_norm": 0.665219247341156,
      "learning_rate": 1.2861768073597346e-05,
      "loss": 0.6786,
      "step": 44140
    },
    {
      "epoch": 1.7431989576341453,
      "grad_norm": 0.8883125185966492,
      "learning_rate": 1.2842026296047696e-05,
      "loss": 0.717,
      "step": 44150
    },
    {
      "epoch": 1.7435937931851384,
      "grad_norm": 0.8767430782318115,
      "learning_rate": 1.2822284518498046e-05,
      "loss": 0.7262,
      "step": 44160
    },
    {
      "epoch": 1.7439886287361315,
      "grad_norm": 0.824447751045227,
      "learning_rate": 1.2802542740948394e-05,
      "loss": 0.6712,
      "step": 44170
    },
    {
      "epoch": 1.7443834642871243,
      "grad_norm": 0.7985783219337463,
      "learning_rate": 1.2782800963398744e-05,
      "loss": 0.6997,
      "step": 44180
    },
    {
      "epoch": 1.7447782998381174,
      "grad_norm": 0.8370209336280823,
      "learning_rate": 1.2763059185849094e-05,
      "loss": 0.7072,
      "step": 44190
    },
    {
      "epoch": 1.7451731353891105,
      "grad_norm": 0.8311502933502197,
      "learning_rate": 1.2743317408299444e-05,
      "loss": 0.7161,
      "step": 44200
    },
    {
      "epoch": 1.7455679709401033,
      "grad_norm": 0.7275866866111755,
      "learning_rate": 1.2723575630749792e-05,
      "loss": 0.7189,
      "step": 44210
    },
    {
      "epoch": 1.7459628064910966,
      "grad_norm": 0.7734725475311279,
      "learning_rate": 1.2703833853200142e-05,
      "loss": 0.7281,
      "step": 44220
    },
    {
      "epoch": 1.7463576420420894,
      "grad_norm": 0.9380906820297241,
      "learning_rate": 1.2684092075650492e-05,
      "loss": 0.6884,
      "step": 44230
    },
    {
      "epoch": 1.7467524775930825,
      "grad_norm": 0.6456406712532043,
      "learning_rate": 1.266435029810084e-05,
      "loss": 0.7042,
      "step": 44240
    },
    {
      "epoch": 1.7471473131440756,
      "grad_norm": 0.736041784286499,
      "learning_rate": 1.264460852055119e-05,
      "loss": 0.6464,
      "step": 44250
    },
    {
      "epoch": 1.7475421486950684,
      "grad_norm": 0.7895816564559937,
      "learning_rate": 1.262486674300154e-05,
      "loss": 0.7262,
      "step": 44260
    },
    {
      "epoch": 1.7479369842460615,
      "grad_norm": 0.9228877425193787,
      "learning_rate": 1.260512496545189e-05,
      "loss": 0.7133,
      "step": 44270
    },
    {
      "epoch": 1.7483318197970545,
      "grad_norm": 0.8625038266181946,
      "learning_rate": 1.2585383187902238e-05,
      "loss": 0.6691,
      "step": 44280
    },
    {
      "epoch": 1.7487266553480474,
      "grad_norm": 0.9102145433425903,
      "learning_rate": 1.2565641410352588e-05,
      "loss": 0.7195,
      "step": 44290
    },
    {
      "epoch": 1.7491214908990407,
      "grad_norm": 0.7361305952072144,
      "learning_rate": 1.2545899632802938e-05,
      "loss": 0.7426,
      "step": 44300
    },
    {
      "epoch": 1.7495163264500335,
      "grad_norm": 0.7946677803993225,
      "learning_rate": 1.2526157855253286e-05,
      "loss": 0.7178,
      "step": 44310
    },
    {
      "epoch": 1.7499111620010266,
      "grad_norm": 0.7682583332061768,
      "learning_rate": 1.2506416077703636e-05,
      "loss": 0.6766,
      "step": 44320
    },
    {
      "epoch": 1.7503059975520197,
      "grad_norm": 0.9618902802467346,
      "learning_rate": 1.2486674300153986e-05,
      "loss": 0.7559,
      "step": 44330
    },
    {
      "epoch": 1.7507008331030125,
      "grad_norm": 0.8995784521102905,
      "learning_rate": 1.2466932522604336e-05,
      "loss": 0.7231,
      "step": 44340
    },
    {
      "epoch": 1.7510956686540056,
      "grad_norm": 0.8028814792633057,
      "learning_rate": 1.2447190745054684e-05,
      "loss": 0.7178,
      "step": 44350
    },
    {
      "epoch": 1.7514905042049986,
      "grad_norm": 0.9410139322280884,
      "learning_rate": 1.2427448967505034e-05,
      "loss": 0.6688,
      "step": 44360
    },
    {
      "epoch": 1.7518853397559915,
      "grad_norm": 0.754957914352417,
      "learning_rate": 1.2407707189955384e-05,
      "loss": 0.72,
      "step": 44370
    },
    {
      "epoch": 1.7522801753069848,
      "grad_norm": 0.8297414779663086,
      "learning_rate": 1.2387965412405734e-05,
      "loss": 0.7287,
      "step": 44380
    },
    {
      "epoch": 1.7526750108579776,
      "grad_norm": 0.7857192754745483,
      "learning_rate": 1.2368223634856082e-05,
      "loss": 0.7051,
      "step": 44390
    },
    {
      "epoch": 1.7530698464089707,
      "grad_norm": 0.6635466814041138,
      "learning_rate": 1.2348481857306432e-05,
      "loss": 0.6373,
      "step": 44400
    },
    {
      "epoch": 1.7534646819599637,
      "grad_norm": 0.8257811069488525,
      "learning_rate": 1.2328740079756782e-05,
      "loss": 0.7071,
      "step": 44410
    },
    {
      "epoch": 1.7538595175109566,
      "grad_norm": 0.7958698868751526,
      "learning_rate": 1.230899830220713e-05,
      "loss": 0.6669,
      "step": 44420
    },
    {
      "epoch": 1.7542543530619497,
      "grad_norm": 0.6680187582969666,
      "learning_rate": 1.228925652465748e-05,
      "loss": 0.7058,
      "step": 44430
    },
    {
      "epoch": 1.7546491886129427,
      "grad_norm": 0.7628823518753052,
      "learning_rate": 1.226951474710783e-05,
      "loss": 0.6889,
      "step": 44440
    },
    {
      "epoch": 1.7550440241639356,
      "grad_norm": 0.9750545620918274,
      "learning_rate": 1.224977296955818e-05,
      "loss": 0.731,
      "step": 44450
    },
    {
      "epoch": 1.7554388597149289,
      "grad_norm": 0.8386533260345459,
      "learning_rate": 1.2230031192008528e-05,
      "loss": 0.7124,
      "step": 44460
    },
    {
      "epoch": 1.7558336952659217,
      "grad_norm": 0.939417839050293,
      "learning_rate": 1.2210289414458878e-05,
      "loss": 0.7041,
      "step": 44470
    },
    {
      "epoch": 1.7562285308169148,
      "grad_norm": 0.7889817357063293,
      "learning_rate": 1.2190547636909228e-05,
      "loss": 0.7118,
      "step": 44480
    },
    {
      "epoch": 1.7566233663679078,
      "grad_norm": 0.7707017064094543,
      "learning_rate": 1.2170805859359576e-05,
      "loss": 0.6855,
      "step": 44490
    },
    {
      "epoch": 1.7570182019189007,
      "grad_norm": 0.7204559445381165,
      "learning_rate": 1.2151064081809926e-05,
      "loss": 0.7092,
      "step": 44500
    },
    {
      "epoch": 1.7574130374698937,
      "grad_norm": 0.7311267852783203,
      "learning_rate": 1.2131322304260276e-05,
      "loss": 0.7534,
      "step": 44510
    },
    {
      "epoch": 1.7578078730208868,
      "grad_norm": 0.6014564633369446,
      "learning_rate": 1.2111580526710626e-05,
      "loss": 0.6888,
      "step": 44520
    },
    {
      "epoch": 1.7582027085718797,
      "grad_norm": 1.8032203912734985,
      "learning_rate": 1.2091838749160974e-05,
      "loss": 0.7435,
      "step": 44530
    },
    {
      "epoch": 1.758597544122873,
      "grad_norm": 0.9445497393608093,
      "learning_rate": 1.2072096971611324e-05,
      "loss": 0.7171,
      "step": 44540
    },
    {
      "epoch": 1.7589923796738658,
      "grad_norm": 0.7736949324607849,
      "learning_rate": 1.2052355194061674e-05,
      "loss": 0.7163,
      "step": 44550
    },
    {
      "epoch": 1.7593872152248589,
      "grad_norm": 0.859352171421051,
      "learning_rate": 1.2032613416512022e-05,
      "loss": 0.7172,
      "step": 44560
    },
    {
      "epoch": 1.759782050775852,
      "grad_norm": 0.8095852732658386,
      "learning_rate": 1.2012871638962372e-05,
      "loss": 0.7034,
      "step": 44570
    },
    {
      "epoch": 1.7601768863268448,
      "grad_norm": 0.7541753649711609,
      "learning_rate": 1.1993129861412722e-05,
      "loss": 0.6892,
      "step": 44580
    },
    {
      "epoch": 1.7605717218778378,
      "grad_norm": 0.8621528148651123,
      "learning_rate": 1.1973388083863072e-05,
      "loss": 0.7172,
      "step": 44590
    },
    {
      "epoch": 1.760966557428831,
      "grad_norm": 0.9136029481887817,
      "learning_rate": 1.195364630631342e-05,
      "loss": 0.7033,
      "step": 44600
    },
    {
      "epoch": 1.7613613929798237,
      "grad_norm": 1.0718977451324463,
      "learning_rate": 1.193390452876377e-05,
      "loss": 0.7219,
      "step": 44610
    },
    {
      "epoch": 1.761756228530817,
      "grad_norm": 0.7590236663818359,
      "learning_rate": 1.191416275121412e-05,
      "loss": 0.7218,
      "step": 44620
    },
    {
      "epoch": 1.7621510640818099,
      "grad_norm": 0.8233703374862671,
      "learning_rate": 1.1894420973664468e-05,
      "loss": 0.6898,
      "step": 44630
    },
    {
      "epoch": 1.762545899632803,
      "grad_norm": 0.7839276790618896,
      "learning_rate": 1.1874679196114818e-05,
      "loss": 0.7267,
      "step": 44640
    },
    {
      "epoch": 1.762940735183796,
      "grad_norm": 0.9360666275024414,
      "learning_rate": 1.1854937418565168e-05,
      "loss": 0.7008,
      "step": 44650
    },
    {
      "epoch": 1.7633355707347889,
      "grad_norm": 0.959855854511261,
      "learning_rate": 1.1835195641015518e-05,
      "loss": 0.7158,
      "step": 44660
    },
    {
      "epoch": 1.763730406285782,
      "grad_norm": 0.9685149788856506,
      "learning_rate": 1.1815453863465866e-05,
      "loss": 0.689,
      "step": 44670
    },
    {
      "epoch": 1.764125241836775,
      "grad_norm": 0.7847285270690918,
      "learning_rate": 1.1795712085916216e-05,
      "loss": 0.7037,
      "step": 44680
    },
    {
      "epoch": 1.7645200773877678,
      "grad_norm": 0.992858350276947,
      "learning_rate": 1.1775970308366566e-05,
      "loss": 0.6747,
      "step": 44690
    },
    {
      "epoch": 1.7649149129387611,
      "grad_norm": 0.8005595803260803,
      "learning_rate": 1.1756228530816914e-05,
      "loss": 0.6411,
      "step": 44700
    },
    {
      "epoch": 1.765309748489754,
      "grad_norm": 0.6764969825744629,
      "learning_rate": 1.1736486753267264e-05,
      "loss": 0.6578,
      "step": 44710
    },
    {
      "epoch": 1.765704584040747,
      "grad_norm": 0.8201037645339966,
      "learning_rate": 1.1716744975717614e-05,
      "loss": 0.7328,
      "step": 44720
    },
    {
      "epoch": 1.76609941959174,
      "grad_norm": 0.8420898914337158,
      "learning_rate": 1.1697003198167964e-05,
      "loss": 0.6876,
      "step": 44730
    },
    {
      "epoch": 1.766494255142733,
      "grad_norm": 0.9417955279350281,
      "learning_rate": 1.1677261420618312e-05,
      "loss": 0.7498,
      "step": 44740
    },
    {
      "epoch": 1.766889090693726,
      "grad_norm": 0.7459937930107117,
      "learning_rate": 1.1657519643068662e-05,
      "loss": 0.657,
      "step": 44750
    },
    {
      "epoch": 1.767283926244719,
      "grad_norm": 0.845156729221344,
      "learning_rate": 1.1637777865519012e-05,
      "loss": 0.748,
      "step": 44760
    },
    {
      "epoch": 1.767678761795712,
      "grad_norm": 0.7268500328063965,
      "learning_rate": 1.1618036087969362e-05,
      "loss": 0.7452,
      "step": 44770
    },
    {
      "epoch": 1.7680735973467052,
      "grad_norm": 0.869903564453125,
      "learning_rate": 1.159829431041971e-05,
      "loss": 0.7154,
      "step": 44780
    },
    {
      "epoch": 1.768468432897698,
      "grad_norm": 0.8891585469245911,
      "learning_rate": 1.157855253287006e-05,
      "loss": 0.7534,
      "step": 44790
    },
    {
      "epoch": 1.7688632684486911,
      "grad_norm": 0.764498233795166,
      "learning_rate": 1.155881075532041e-05,
      "loss": 0.6642,
      "step": 44800
    },
    {
      "epoch": 1.7692581039996842,
      "grad_norm": 0.8560827374458313,
      "learning_rate": 1.1539068977770758e-05,
      "loss": 0.6727,
      "step": 44810
    },
    {
      "epoch": 1.769652939550677,
      "grad_norm": 0.7824625372886658,
      "learning_rate": 1.1519327200221108e-05,
      "loss": 0.7341,
      "step": 44820
    },
    {
      "epoch": 1.77004777510167,
      "grad_norm": 1.054245114326477,
      "learning_rate": 1.1499585422671458e-05,
      "loss": 0.6975,
      "step": 44830
    },
    {
      "epoch": 1.7704426106526632,
      "grad_norm": 0.7400891780853271,
      "learning_rate": 1.1479843645121808e-05,
      "loss": 0.7025,
      "step": 44840
    },
    {
      "epoch": 1.770837446203656,
      "grad_norm": 0.8177803754806519,
      "learning_rate": 1.1460101867572156e-05,
      "loss": 0.6814,
      "step": 44850
    },
    {
      "epoch": 1.7712322817546493,
      "grad_norm": 1.0200622081756592,
      "learning_rate": 1.1440360090022506e-05,
      "loss": 0.7412,
      "step": 44860
    },
    {
      "epoch": 1.7716271173056422,
      "grad_norm": 0.866799533367157,
      "learning_rate": 1.1420618312472856e-05,
      "loss": 0.7084,
      "step": 44870
    },
    {
      "epoch": 1.7720219528566352,
      "grad_norm": 0.7237913608551025,
      "learning_rate": 1.1400876534923204e-05,
      "loss": 0.6903,
      "step": 44880
    },
    {
      "epoch": 1.7724167884076283,
      "grad_norm": 0.8343866467475891,
      "learning_rate": 1.1381134757373554e-05,
      "loss": 0.7012,
      "step": 44890
    },
    {
      "epoch": 1.7728116239586211,
      "grad_norm": 0.7897263765335083,
      "learning_rate": 1.1361392979823904e-05,
      "loss": 0.7143,
      "step": 44900
    },
    {
      "epoch": 1.7732064595096142,
      "grad_norm": 0.7650623321533203,
      "learning_rate": 1.1341651202274254e-05,
      "loss": 0.679,
      "step": 44910
    },
    {
      "epoch": 1.7736012950606073,
      "grad_norm": 0.8184637427330017,
      "learning_rate": 1.1321909424724602e-05,
      "loss": 0.7188,
      "step": 44920
    },
    {
      "epoch": 1.7739961306116,
      "grad_norm": 0.7161241769790649,
      "learning_rate": 1.1302167647174952e-05,
      "loss": 0.7161,
      "step": 44930
    },
    {
      "epoch": 1.7743909661625934,
      "grad_norm": 0.9147241711616516,
      "learning_rate": 1.1282425869625302e-05,
      "loss": 0.7042,
      "step": 44940
    },
    {
      "epoch": 1.7747858017135862,
      "grad_norm": 0.6037636995315552,
      "learning_rate": 1.126268409207565e-05,
      "loss": 0.7082,
      "step": 44950
    },
    {
      "epoch": 1.7751806372645793,
      "grad_norm": 0.8732886910438538,
      "learning_rate": 1.1242942314526e-05,
      "loss": 0.7444,
      "step": 44960
    },
    {
      "epoch": 1.7755754728155724,
      "grad_norm": 0.8169829845428467,
      "learning_rate": 1.122320053697635e-05,
      "loss": 0.6993,
      "step": 44970
    },
    {
      "epoch": 1.7759703083665652,
      "grad_norm": 0.796241044998169,
      "learning_rate": 1.12034587594267e-05,
      "loss": 0.6377,
      "step": 44980
    },
    {
      "epoch": 1.7763651439175583,
      "grad_norm": 0.8472680449485779,
      "learning_rate": 1.1183716981877048e-05,
      "loss": 0.6896,
      "step": 44990
    },
    {
      "epoch": 1.7767599794685514,
      "grad_norm": 0.6944222450256348,
      "learning_rate": 1.1163975204327398e-05,
      "loss": 0.7398,
      "step": 45000
    },
    {
      "epoch": 1.7767599794685514,
      "eval_loss": 0.7559207677841187,
      "eval_runtime": 1218.0503,
      "eval_samples_per_second": 9.242,
      "eval_steps_per_second": 9.242,
      "step": 45000
    },
    {
      "epoch": 1.7771548150195442,
      "grad_norm": 0.7357565760612488,
      "learning_rate": 1.1144233426777748e-05,
      "loss": 0.6872,
      "step": 45010
    },
    {
      "epoch": 1.7775496505705375,
      "grad_norm": 0.9261999130249023,
      "learning_rate": 1.1124491649228096e-05,
      "loss": 0.6853,
      "step": 45020
    },
    {
      "epoch": 1.7779444861215303,
      "grad_norm": 0.6614230275154114,
      "learning_rate": 1.1104749871678446e-05,
      "loss": 0.6791,
      "step": 45030
    },
    {
      "epoch": 1.7783393216725234,
      "grad_norm": 0.7131901979446411,
      "learning_rate": 1.1085008094128796e-05,
      "loss": 0.7115,
      "step": 45040
    },
    {
      "epoch": 1.7787341572235165,
      "grad_norm": 0.6713850498199463,
      "learning_rate": 1.1065266316579146e-05,
      "loss": 0.74,
      "step": 45050
    },
    {
      "epoch": 1.7791289927745093,
      "grad_norm": 0.8649391531944275,
      "learning_rate": 1.1045524539029494e-05,
      "loss": 0.7143,
      "step": 45060
    },
    {
      "epoch": 1.7795238283255024,
      "grad_norm": 0.8775069713592529,
      "learning_rate": 1.1025782761479844e-05,
      "loss": 0.7504,
      "step": 45070
    },
    {
      "epoch": 1.7799186638764954,
      "grad_norm": 0.7046902179718018,
      "learning_rate": 1.1006040983930194e-05,
      "loss": 0.6824,
      "step": 45080
    },
    {
      "epoch": 1.7803134994274883,
      "grad_norm": 0.9602298140525818,
      "learning_rate": 1.0986299206380542e-05,
      "loss": 0.71,
      "step": 45090
    },
    {
      "epoch": 1.7807083349784816,
      "grad_norm": 0.9252178072929382,
      "learning_rate": 1.0966557428830892e-05,
      "loss": 0.7519,
      "step": 45100
    },
    {
      "epoch": 1.7811031705294744,
      "grad_norm": 0.7286549806594849,
      "learning_rate": 1.0946815651281242e-05,
      "loss": 0.656,
      "step": 45110
    },
    {
      "epoch": 1.7814980060804675,
      "grad_norm": 0.72747403383255,
      "learning_rate": 1.0927073873731592e-05,
      "loss": 0.698,
      "step": 45120
    },
    {
      "epoch": 1.7818928416314606,
      "grad_norm": 0.8173978924751282,
      "learning_rate": 1.090733209618194e-05,
      "loss": 0.7062,
      "step": 45130
    },
    {
      "epoch": 1.7822876771824534,
      "grad_norm": 0.6661428809165955,
      "learning_rate": 1.088759031863229e-05,
      "loss": 0.7112,
      "step": 45140
    },
    {
      "epoch": 1.7826825127334465,
      "grad_norm": 0.9397720694541931,
      "learning_rate": 1.086784854108264e-05,
      "loss": 0.6617,
      "step": 45150
    },
    {
      "epoch": 1.7830773482844395,
      "grad_norm": 0.7278032898902893,
      "learning_rate": 1.084810676353299e-05,
      "loss": 0.6995,
      "step": 45160
    },
    {
      "epoch": 1.7834721838354324,
      "grad_norm": 0.7700123190879822,
      "learning_rate": 1.0828364985983338e-05,
      "loss": 0.6692,
      "step": 45170
    },
    {
      "epoch": 1.7838670193864257,
      "grad_norm": 0.7094849944114685,
      "learning_rate": 1.0808623208433688e-05,
      "loss": 0.7207,
      "step": 45180
    },
    {
      "epoch": 1.7842618549374185,
      "grad_norm": 0.8817087411880493,
      "learning_rate": 1.0788881430884038e-05,
      "loss": 0.7506,
      "step": 45190
    },
    {
      "epoch": 1.7846566904884116,
      "grad_norm": 0.7329108119010925,
      "learning_rate": 1.0769139653334386e-05,
      "loss": 0.6838,
      "step": 45200
    },
    {
      "epoch": 1.7850515260394046,
      "grad_norm": 0.9470437169075012,
      "learning_rate": 1.0749397875784736e-05,
      "loss": 0.7375,
      "step": 45210
    },
    {
      "epoch": 1.7854463615903975,
      "grad_norm": 0.650696873664856,
      "learning_rate": 1.0729656098235086e-05,
      "loss": 0.6748,
      "step": 45220
    },
    {
      "epoch": 1.7858411971413906,
      "grad_norm": 1.1243070363998413,
      "learning_rate": 1.0709914320685436e-05,
      "loss": 0.6936,
      "step": 45230
    },
    {
      "epoch": 1.7862360326923836,
      "grad_norm": 1.1133427619934082,
      "learning_rate": 1.0690172543135784e-05,
      "loss": 0.7445,
      "step": 45240
    },
    {
      "epoch": 1.7866308682433765,
      "grad_norm": 0.8413253426551819,
      "learning_rate": 1.0670430765586134e-05,
      "loss": 0.7214,
      "step": 45250
    },
    {
      "epoch": 1.7870257037943698,
      "grad_norm": 0.8689599633216858,
      "learning_rate": 1.0650688988036484e-05,
      "loss": 0.7061,
      "step": 45260
    },
    {
      "epoch": 1.7874205393453626,
      "grad_norm": 0.8115736246109009,
      "learning_rate": 1.0630947210486832e-05,
      "loss": 0.6834,
      "step": 45270
    },
    {
      "epoch": 1.7878153748963557,
      "grad_norm": 0.7810097932815552,
      "learning_rate": 1.0611205432937182e-05,
      "loss": 0.6841,
      "step": 45280
    },
    {
      "epoch": 1.7882102104473487,
      "grad_norm": 0.6767035126686096,
      "learning_rate": 1.0591463655387532e-05,
      "loss": 0.7142,
      "step": 45290
    },
    {
      "epoch": 1.7886050459983416,
      "grad_norm": 0.8222616910934448,
      "learning_rate": 1.0571721877837882e-05,
      "loss": 0.7037,
      "step": 45300
    },
    {
      "epoch": 1.7889998815493346,
      "grad_norm": 0.690395176410675,
      "learning_rate": 1.055198010028823e-05,
      "loss": 0.6703,
      "step": 45310
    },
    {
      "epoch": 1.7893947171003277,
      "grad_norm": 0.7990915179252625,
      "learning_rate": 1.053223832273858e-05,
      "loss": 0.7392,
      "step": 45320
    },
    {
      "epoch": 1.7897895526513206,
      "grad_norm": 0.6974703669548035,
      "learning_rate": 1.051249654518893e-05,
      "loss": 0.6703,
      "step": 45330
    },
    {
      "epoch": 1.7901843882023138,
      "grad_norm": 0.8777974247932434,
      "learning_rate": 1.0492754767639278e-05,
      "loss": 0.7087,
      "step": 45340
    },
    {
      "epoch": 1.7905792237533067,
      "grad_norm": 0.9973637461662292,
      "learning_rate": 1.0473012990089628e-05,
      "loss": 0.7408,
      "step": 45350
    },
    {
      "epoch": 1.7909740593042998,
      "grad_norm": 0.7499393224716187,
      "learning_rate": 1.0453271212539978e-05,
      "loss": 0.6914,
      "step": 45360
    },
    {
      "epoch": 1.7913688948552928,
      "grad_norm": 0.7353923320770264,
      "learning_rate": 1.0433529434990328e-05,
      "loss": 0.6934,
      "step": 45370
    },
    {
      "epoch": 1.7917637304062857,
      "grad_norm": 0.9975892305374146,
      "learning_rate": 1.0413787657440676e-05,
      "loss": 0.6918,
      "step": 45380
    },
    {
      "epoch": 1.792158565957279,
      "grad_norm": 0.7530601620674133,
      "learning_rate": 1.0394045879891026e-05,
      "loss": 0.6903,
      "step": 45390
    },
    {
      "epoch": 1.7925534015082718,
      "grad_norm": 0.7860444784164429,
      "learning_rate": 1.0374304102341376e-05,
      "loss": 0.7144,
      "step": 45400
    },
    {
      "epoch": 1.7929482370592649,
      "grad_norm": 0.8502130508422852,
      "learning_rate": 1.0354562324791724e-05,
      "loss": 0.7386,
      "step": 45410
    },
    {
      "epoch": 1.793343072610258,
      "grad_norm": 0.7891644239425659,
      "learning_rate": 1.0334820547242074e-05,
      "loss": 0.7261,
      "step": 45420
    },
    {
      "epoch": 1.7937379081612508,
      "grad_norm": 0.8838341236114502,
      "learning_rate": 1.0315078769692424e-05,
      "loss": 0.6828,
      "step": 45430
    },
    {
      "epoch": 1.7941327437122438,
      "grad_norm": 0.796589732170105,
      "learning_rate": 1.0295336992142774e-05,
      "loss": 0.7064,
      "step": 45440
    },
    {
      "epoch": 1.794527579263237,
      "grad_norm": 0.7447862029075623,
      "learning_rate": 1.0275595214593122e-05,
      "loss": 0.7144,
      "step": 45450
    },
    {
      "epoch": 1.7949224148142298,
      "grad_norm": 0.7450835704803467,
      "learning_rate": 1.0255853437043472e-05,
      "loss": 0.7156,
      "step": 45460
    },
    {
      "epoch": 1.795317250365223,
      "grad_norm": 0.778060793876648,
      "learning_rate": 1.0236111659493822e-05,
      "loss": 0.7367,
      "step": 45470
    },
    {
      "epoch": 1.795712085916216,
      "grad_norm": 0.7536194920539856,
      "learning_rate": 1.021636988194417e-05,
      "loss": 0.7038,
      "step": 45480
    },
    {
      "epoch": 1.796106921467209,
      "grad_norm": 0.8264312148094177,
      "learning_rate": 1.019662810439452e-05,
      "loss": 0.6934,
      "step": 45490
    },
    {
      "epoch": 1.796501757018202,
      "grad_norm": 0.828515887260437,
      "learning_rate": 1.017688632684487e-05,
      "loss": 0.7517,
      "step": 45500
    },
    {
      "epoch": 1.7968965925691949,
      "grad_norm": 0.8179613947868347,
      "learning_rate": 1.015714454929522e-05,
      "loss": 0.6895,
      "step": 45510
    },
    {
      "epoch": 1.797291428120188,
      "grad_norm": 0.6468988656997681,
      "learning_rate": 1.0137402771745568e-05,
      "loss": 0.6627,
      "step": 45520
    },
    {
      "epoch": 1.797686263671181,
      "grad_norm": 0.7844081521034241,
      "learning_rate": 1.0117660994195918e-05,
      "loss": 0.7426,
      "step": 45530
    },
    {
      "epoch": 1.7980810992221739,
      "grad_norm": 0.7820551991462708,
      "learning_rate": 1.0097919216646268e-05,
      "loss": 0.6997,
      "step": 45540
    },
    {
      "epoch": 1.7984759347731671,
      "grad_norm": 0.9522305130958557,
      "learning_rate": 1.0078177439096616e-05,
      "loss": 0.7141,
      "step": 45550
    },
    {
      "epoch": 1.79887077032416,
      "grad_norm": 0.6296835541725159,
      "learning_rate": 1.0058435661546966e-05,
      "loss": 0.7053,
      "step": 45560
    },
    {
      "epoch": 1.799265605875153,
      "grad_norm": 0.7807915210723877,
      "learning_rate": 1.0038693883997316e-05,
      "loss": 0.7394,
      "step": 45570
    },
    {
      "epoch": 1.7996604414261461,
      "grad_norm": 0.7546504139900208,
      "learning_rate": 1.0018952106447666e-05,
      "loss": 0.6969,
      "step": 45580
    },
    {
      "epoch": 1.800055276977139,
      "grad_norm": 0.7828893065452576,
      "learning_rate": 9.999210328898014e-06,
      "loss": 0.6838,
      "step": 45590
    },
    {
      "epoch": 1.800450112528132,
      "grad_norm": 0.7941153645515442,
      "learning_rate": 9.979468551348364e-06,
      "loss": 0.6975,
      "step": 45600
    },
    {
      "epoch": 1.800844948079125,
      "grad_norm": 0.7083747982978821,
      "learning_rate": 9.959726773798714e-06,
      "loss": 0.6864,
      "step": 45610
    },
    {
      "epoch": 1.801239783630118,
      "grad_norm": 1.2176682949066162,
      "learning_rate": 9.939984996249064e-06,
      "loss": 0.7237,
      "step": 45620
    },
    {
      "epoch": 1.8016346191811112,
      "grad_norm": 0.7989044785499573,
      "learning_rate": 9.920243218699412e-06,
      "loss": 0.6944,
      "step": 45630
    },
    {
      "epoch": 1.802029454732104,
      "grad_norm": 1.1489559412002563,
      "learning_rate": 9.900501441149762e-06,
      "loss": 0.683,
      "step": 45640
    },
    {
      "epoch": 1.8024242902830971,
      "grad_norm": 0.9801672697067261,
      "learning_rate": 9.880759663600112e-06,
      "loss": 0.707,
      "step": 45650
    },
    {
      "epoch": 1.8028191258340902,
      "grad_norm": 0.7982776761054993,
      "learning_rate": 9.86101788605046e-06,
      "loss": 0.7569,
      "step": 45660
    },
    {
      "epoch": 1.803213961385083,
      "grad_norm": 0.8339875936508179,
      "learning_rate": 9.84127610850081e-06,
      "loss": 0.6797,
      "step": 45670
    },
    {
      "epoch": 1.8036087969360761,
      "grad_norm": 0.8723600506782532,
      "learning_rate": 9.82153433095116e-06,
      "loss": 0.7251,
      "step": 45680
    },
    {
      "epoch": 1.8040036324870692,
      "grad_norm": 0.8591354489326477,
      "learning_rate": 9.80179255340151e-06,
      "loss": 0.6638,
      "step": 45690
    },
    {
      "epoch": 1.804398468038062,
      "grad_norm": 0.827506959438324,
      "learning_rate": 9.782050775851858e-06,
      "loss": 0.7179,
      "step": 45700
    },
    {
      "epoch": 1.8047933035890553,
      "grad_norm": 0.779743492603302,
      "learning_rate": 9.762308998302208e-06,
      "loss": 0.6703,
      "step": 45710
    },
    {
      "epoch": 1.8051881391400482,
      "grad_norm": 0.9038282036781311,
      "learning_rate": 9.742567220752558e-06,
      "loss": 0.6772,
      "step": 45720
    },
    {
      "epoch": 1.8055829746910412,
      "grad_norm": 0.7888540625572205,
      "learning_rate": 9.722825443202906e-06,
      "loss": 0.7174,
      "step": 45730
    },
    {
      "epoch": 1.8059778102420343,
      "grad_norm": 0.7420411109924316,
      "learning_rate": 9.703083665653256e-06,
      "loss": 0.6991,
      "step": 45740
    },
    {
      "epoch": 1.8063726457930271,
      "grad_norm": 0.8049668073654175,
      "learning_rate": 9.683341888103606e-06,
      "loss": 0.7462,
      "step": 45750
    },
    {
      "epoch": 1.8067674813440202,
      "grad_norm": 0.7660771012306213,
      "learning_rate": 9.663600110553956e-06,
      "loss": 0.7308,
      "step": 45760
    },
    {
      "epoch": 1.8071623168950133,
      "grad_norm": 0.8865984678268433,
      "learning_rate": 9.643858333004304e-06,
      "loss": 0.6787,
      "step": 45770
    },
    {
      "epoch": 1.8075571524460061,
      "grad_norm": 0.8730733394622803,
      "learning_rate": 9.624116555454654e-06,
      "loss": 0.6693,
      "step": 45780
    },
    {
      "epoch": 1.8079519879969994,
      "grad_norm": 0.8047686219215393,
      "learning_rate": 9.604374777905004e-06,
      "loss": 0.6953,
      "step": 45790
    },
    {
      "epoch": 1.8083468235479923,
      "grad_norm": 0.7874835729598999,
      "learning_rate": 9.584633000355352e-06,
      "loss": 0.7279,
      "step": 45800
    },
    {
      "epoch": 1.8087416590989853,
      "grad_norm": 0.6601879596710205,
      "learning_rate": 9.564891222805702e-06,
      "loss": 0.6693,
      "step": 45810
    },
    {
      "epoch": 1.8091364946499784,
      "grad_norm": 0.7376148700714111,
      "learning_rate": 9.545149445256052e-06,
      "loss": 0.7065,
      "step": 45820
    },
    {
      "epoch": 1.8095313302009712,
      "grad_norm": 0.8480439186096191,
      "learning_rate": 9.525407667706402e-06,
      "loss": 0.7175,
      "step": 45830
    },
    {
      "epoch": 1.8099261657519643,
      "grad_norm": 0.6814018487930298,
      "learning_rate": 9.50566589015675e-06,
      "loss": 0.7135,
      "step": 45840
    },
    {
      "epoch": 1.8103210013029574,
      "grad_norm": 0.839943528175354,
      "learning_rate": 9.4859241126071e-06,
      "loss": 0.7176,
      "step": 45850
    },
    {
      "epoch": 1.8107158368539502,
      "grad_norm": 0.8528590202331543,
      "learning_rate": 9.46618233505745e-06,
      "loss": 0.6835,
      "step": 45860
    },
    {
      "epoch": 1.8111106724049435,
      "grad_norm": 0.8328132033348083,
      "learning_rate": 9.446440557507798e-06,
      "loss": 0.6916,
      "step": 45870
    },
    {
      "epoch": 1.8115055079559363,
      "grad_norm": 0.7248687744140625,
      "learning_rate": 9.426698779958148e-06,
      "loss": 0.717,
      "step": 45880
    },
    {
      "epoch": 1.8119003435069294,
      "grad_norm": 1.0311028957366943,
      "learning_rate": 9.406957002408498e-06,
      "loss": 0.7665,
      "step": 45890
    },
    {
      "epoch": 1.8122951790579225,
      "grad_norm": 0.8066024780273438,
      "learning_rate": 9.387215224858848e-06,
      "loss": 0.7206,
      "step": 45900
    },
    {
      "epoch": 1.8126900146089153,
      "grad_norm": 0.8480910658836365,
      "learning_rate": 9.367473447309196e-06,
      "loss": 0.7175,
      "step": 45910
    },
    {
      "epoch": 1.8130848501599084,
      "grad_norm": 0.7643603682518005,
      "learning_rate": 9.347731669759546e-06,
      "loss": 0.6973,
      "step": 45920
    },
    {
      "epoch": 1.8134796857109015,
      "grad_norm": 0.8527660965919495,
      "learning_rate": 9.327989892209894e-06,
      "loss": 0.7047,
      "step": 45930
    },
    {
      "epoch": 1.8138745212618943,
      "grad_norm": 0.8007194399833679,
      "learning_rate": 9.308248114660244e-06,
      "loss": 0.6978,
      "step": 45940
    },
    {
      "epoch": 1.8142693568128876,
      "grad_norm": 1.2773276567459106,
      "learning_rate": 9.288506337110593e-06,
      "loss": 0.7247,
      "step": 45950
    },
    {
      "epoch": 1.8146641923638804,
      "grad_norm": 0.7781781554222107,
      "learning_rate": 9.268764559560942e-06,
      "loss": 0.6723,
      "step": 45960
    },
    {
      "epoch": 1.8150590279148735,
      "grad_norm": 0.7096894383430481,
      "learning_rate": 9.249022782011292e-06,
      "loss": 0.6904,
      "step": 45970
    },
    {
      "epoch": 1.8154538634658666,
      "grad_norm": 0.8549636602401733,
      "learning_rate": 9.229281004461642e-06,
      "loss": 0.7217,
      "step": 45980
    },
    {
      "epoch": 1.8158486990168594,
      "grad_norm": 1.0951300859451294,
      "learning_rate": 9.20953922691199e-06,
      "loss": 0.7033,
      "step": 45990
    },
    {
      "epoch": 1.8162435345678525,
      "grad_norm": 0.6797633171081543,
      "learning_rate": 9.18979744936234e-06,
      "loss": 0.6624,
      "step": 46000
    },
    {
      "epoch": 1.8162435345678525,
      "eval_loss": 0.7550397515296936,
      "eval_runtime": 1218.6682,
      "eval_samples_per_second": 9.237,
      "eval_steps_per_second": 9.237,
      "step": 46000
    },
    {
      "epoch": 1.8166383701188455,
      "grad_norm": 0.8752979636192322,
      "learning_rate": 9.17005567181269e-06,
      "loss": 0.7053,
      "step": 46010
    },
    {
      "epoch": 1.8170332056698384,
      "grad_norm": 0.7113985419273376,
      "learning_rate": 9.150313894263039e-06,
      "loss": 0.6776,
      "step": 46020
    },
    {
      "epoch": 1.8174280412208317,
      "grad_norm": 1.068538784980774,
      "learning_rate": 9.130572116713388e-06,
      "loss": 0.7469,
      "step": 46030
    },
    {
      "epoch": 1.8178228767718245,
      "grad_norm": 0.7133006453514099,
      "learning_rate": 9.110830339163738e-06,
      "loss": 0.6798,
      "step": 46040
    },
    {
      "epoch": 1.8182177123228176,
      "grad_norm": 1.0813508033752441,
      "learning_rate": 9.091088561614088e-06,
      "loss": 0.7319,
      "step": 46050
    },
    {
      "epoch": 1.8186125478738107,
      "grad_norm": 0.8336858153343201,
      "learning_rate": 9.071346784064437e-06,
      "loss": 0.7379,
      "step": 46060
    },
    {
      "epoch": 1.8190073834248035,
      "grad_norm": 0.8248600959777832,
      "learning_rate": 9.051605006514786e-06,
      "loss": 0.7005,
      "step": 46070
    },
    {
      "epoch": 1.8194022189757966,
      "grad_norm": 0.9470250606536865,
      "learning_rate": 9.031863228965136e-06,
      "loss": 0.7146,
      "step": 46080
    },
    {
      "epoch": 1.8197970545267896,
      "grad_norm": 0.7696089744567871,
      "learning_rate": 9.012121451415485e-06,
      "loss": 0.6577,
      "step": 46090
    },
    {
      "epoch": 1.8201918900777825,
      "grad_norm": 0.7820139527320862,
      "learning_rate": 8.992379673865834e-06,
      "loss": 0.6946,
      "step": 46100
    },
    {
      "epoch": 1.8205867256287758,
      "grad_norm": 0.8592981696128845,
      "learning_rate": 8.972637896316184e-06,
      "loss": 0.7306,
      "step": 46110
    },
    {
      "epoch": 1.8209815611797686,
      "grad_norm": 0.7453604340553284,
      "learning_rate": 8.952896118766534e-06,
      "loss": 0.7117,
      "step": 46120
    },
    {
      "epoch": 1.8213763967307617,
      "grad_norm": 0.8156565427780151,
      "learning_rate": 8.933154341216883e-06,
      "loss": 0.6552,
      "step": 46130
    },
    {
      "epoch": 1.8217712322817547,
      "grad_norm": 0.8655098676681519,
      "learning_rate": 8.913412563667232e-06,
      "loss": 0.6991,
      "step": 46140
    },
    {
      "epoch": 1.8221660678327476,
      "grad_norm": 1.0606940984725952,
      "learning_rate": 8.893670786117582e-06,
      "loss": 0.6863,
      "step": 46150
    },
    {
      "epoch": 1.8225609033837407,
      "grad_norm": 0.6821739077568054,
      "learning_rate": 8.87392900856793e-06,
      "loss": 0.7067,
      "step": 46160
    },
    {
      "epoch": 1.8229557389347337,
      "grad_norm": 0.6783199906349182,
      "learning_rate": 8.85418723101828e-06,
      "loss": 0.6569,
      "step": 46170
    },
    {
      "epoch": 1.8233505744857266,
      "grad_norm": 0.682209849357605,
      "learning_rate": 8.83444545346863e-06,
      "loss": 0.6506,
      "step": 46180
    },
    {
      "epoch": 1.8237454100367199,
      "grad_norm": 0.7934228181838989,
      "learning_rate": 8.81470367591898e-06,
      "loss": 0.6704,
      "step": 46190
    },
    {
      "epoch": 1.8241402455877127,
      "grad_norm": 0.8145917057991028,
      "learning_rate": 8.794961898369329e-06,
      "loss": 0.7266,
      "step": 46200
    },
    {
      "epoch": 1.8245350811387058,
      "grad_norm": 0.6699566841125488,
      "learning_rate": 8.775220120819678e-06,
      "loss": 0.7069,
      "step": 46210
    },
    {
      "epoch": 1.8249299166896988,
      "grad_norm": 0.862200915813446,
      "learning_rate": 8.755478343270028e-06,
      "loss": 0.6976,
      "step": 46220
    },
    {
      "epoch": 1.8253247522406917,
      "grad_norm": 0.8216803669929504,
      "learning_rate": 8.735736565720377e-06,
      "loss": 0.7072,
      "step": 46230
    },
    {
      "epoch": 1.8257195877916848,
      "grad_norm": 0.5406054854393005,
      "learning_rate": 8.715994788170726e-06,
      "loss": 0.6824,
      "step": 46240
    },
    {
      "epoch": 1.8261144233426778,
      "grad_norm": 0.9563420414924622,
      "learning_rate": 8.696253010621076e-06,
      "loss": 0.7132,
      "step": 46250
    },
    {
      "epoch": 1.8265092588936707,
      "grad_norm": 0.9946041107177734,
      "learning_rate": 8.676511233071426e-06,
      "loss": 0.7137,
      "step": 46260
    },
    {
      "epoch": 1.826904094444664,
      "grad_norm": 0.6959279179573059,
      "learning_rate": 8.656769455521775e-06,
      "loss": 0.7031,
      "step": 46270
    },
    {
      "epoch": 1.8272989299956568,
      "grad_norm": 0.7853866815567017,
      "learning_rate": 8.637027677972124e-06,
      "loss": 0.7295,
      "step": 46280
    },
    {
      "epoch": 1.8276937655466499,
      "grad_norm": 0.9138997793197632,
      "learning_rate": 8.617285900422474e-06,
      "loss": 0.6905,
      "step": 46290
    },
    {
      "epoch": 1.828088601097643,
      "grad_norm": 0.739971399307251,
      "learning_rate": 8.597544122872824e-06,
      "loss": 0.7059,
      "step": 46300
    },
    {
      "epoch": 1.8284834366486358,
      "grad_norm": 0.786186158657074,
      "learning_rate": 8.577802345323172e-06,
      "loss": 0.7171,
      "step": 46310
    },
    {
      "epoch": 1.8288782721996288,
      "grad_norm": 0.7813563942909241,
      "learning_rate": 8.558060567773522e-06,
      "loss": 0.7002,
      "step": 46320
    },
    {
      "epoch": 1.829273107750622,
      "grad_norm": 0.6992368102073669,
      "learning_rate": 8.538318790223872e-06,
      "loss": 0.685,
      "step": 46330
    },
    {
      "epoch": 1.8296679433016148,
      "grad_norm": 0.7624765038490295,
      "learning_rate": 8.51857701267422e-06,
      "loss": 0.7215,
      "step": 46340
    },
    {
      "epoch": 1.830062778852608,
      "grad_norm": 0.91742342710495,
      "learning_rate": 8.49883523512457e-06,
      "loss": 0.6838,
      "step": 46350
    },
    {
      "epoch": 1.8304576144036009,
      "grad_norm": 0.904115617275238,
      "learning_rate": 8.47909345757492e-06,
      "loss": 0.6759,
      "step": 46360
    },
    {
      "epoch": 1.830852449954594,
      "grad_norm": 0.9898088574409485,
      "learning_rate": 8.45935168002527e-06,
      "loss": 0.6884,
      "step": 46370
    },
    {
      "epoch": 1.831247285505587,
      "grad_norm": 0.6653373837471008,
      "learning_rate": 8.439609902475619e-06,
      "loss": 0.7393,
      "step": 46380
    },
    {
      "epoch": 1.8316421210565799,
      "grad_norm": 0.7887622714042664,
      "learning_rate": 8.419868124925968e-06,
      "loss": 0.679,
      "step": 46390
    },
    {
      "epoch": 1.832036956607573,
      "grad_norm": 0.9080142974853516,
      "learning_rate": 8.400126347376318e-06,
      "loss": 0.7212,
      "step": 46400
    },
    {
      "epoch": 1.832431792158566,
      "grad_norm": 0.8571429252624512,
      "learning_rate": 8.380384569826667e-06,
      "loss": 0.7033,
      "step": 46410
    },
    {
      "epoch": 1.8328266277095588,
      "grad_norm": 0.9799490571022034,
      "learning_rate": 8.360642792277016e-06,
      "loss": 0.7177,
      "step": 46420
    },
    {
      "epoch": 1.8332214632605521,
      "grad_norm": 0.8548548221588135,
      "learning_rate": 8.340901014727366e-06,
      "loss": 0.7036,
      "step": 46430
    },
    {
      "epoch": 1.833616298811545,
      "grad_norm": 0.8587722778320312,
      "learning_rate": 8.321159237177716e-06,
      "loss": 0.6976,
      "step": 46440
    },
    {
      "epoch": 1.834011134362538,
      "grad_norm": 0.8510643243789673,
      "learning_rate": 8.301417459628065e-06,
      "loss": 0.7009,
      "step": 46450
    },
    {
      "epoch": 1.834405969913531,
      "grad_norm": 0.9713353514671326,
      "learning_rate": 8.281675682078414e-06,
      "loss": 0.7109,
      "step": 46460
    },
    {
      "epoch": 1.834800805464524,
      "grad_norm": 0.9164509177207947,
      "learning_rate": 8.261933904528764e-06,
      "loss": 0.6787,
      "step": 46470
    },
    {
      "epoch": 1.835195641015517,
      "grad_norm": 0.8340564966201782,
      "learning_rate": 8.242192126979113e-06,
      "loss": 0.6739,
      "step": 46480
    },
    {
      "epoch": 1.83559047656651,
      "grad_norm": 0.8591435551643372,
      "learning_rate": 8.222450349429462e-06,
      "loss": 0.7005,
      "step": 46490
    },
    {
      "epoch": 1.835985312117503,
      "grad_norm": 0.9747328162193298,
      "learning_rate": 8.202708571879812e-06,
      "loss": 0.7566,
      "step": 46500
    },
    {
      "epoch": 1.8363801476684962,
      "grad_norm": 0.9057523608207703,
      "learning_rate": 8.182966794330162e-06,
      "loss": 0.7402,
      "step": 46510
    },
    {
      "epoch": 1.836774983219489,
      "grad_norm": 0.8461530208587646,
      "learning_rate": 8.16322501678051e-06,
      "loss": 0.6897,
      "step": 46520
    },
    {
      "epoch": 1.8371698187704821,
      "grad_norm": 0.9522161483764648,
      "learning_rate": 8.14348323923086e-06,
      "loss": 0.6942,
      "step": 46530
    },
    {
      "epoch": 1.8375646543214752,
      "grad_norm": 0.7028820514678955,
      "learning_rate": 8.12374146168121e-06,
      "loss": 0.7021,
      "step": 46540
    },
    {
      "epoch": 1.837959489872468,
      "grad_norm": 0.8512560129165649,
      "learning_rate": 8.103999684131559e-06,
      "loss": 0.7088,
      "step": 46550
    },
    {
      "epoch": 1.838354325423461,
      "grad_norm": 0.9096468091011047,
      "learning_rate": 8.084257906581908e-06,
      "loss": 0.6926,
      "step": 46560
    },
    {
      "epoch": 1.8387491609744542,
      "grad_norm": 0.7651574611663818,
      "learning_rate": 8.064516129032258e-06,
      "loss": 0.7342,
      "step": 46570
    },
    {
      "epoch": 1.839143996525447,
      "grad_norm": 0.8073740005493164,
      "learning_rate": 8.044774351482608e-06,
      "loss": 0.6758,
      "step": 46580
    },
    {
      "epoch": 1.8395388320764403,
      "grad_norm": 0.725365400314331,
      "learning_rate": 8.025032573932957e-06,
      "loss": 0.7077,
      "step": 46590
    },
    {
      "epoch": 1.8399336676274332,
      "grad_norm": 0.698872447013855,
      "learning_rate": 8.005290796383306e-06,
      "loss": 0.6747,
      "step": 46600
    },
    {
      "epoch": 1.8403285031784262,
      "grad_norm": 0.839924156665802,
      "learning_rate": 7.985549018833656e-06,
      "loss": 0.6969,
      "step": 46610
    },
    {
      "epoch": 1.8407233387294193,
      "grad_norm": 0.811491072177887,
      "learning_rate": 7.965807241284005e-06,
      "loss": 0.6479,
      "step": 46620
    },
    {
      "epoch": 1.8411181742804121,
      "grad_norm": 0.734035074710846,
      "learning_rate": 7.946065463734354e-06,
      "loss": 0.7471,
      "step": 46630
    },
    {
      "epoch": 1.8415130098314052,
      "grad_norm": 0.7178214192390442,
      "learning_rate": 7.926323686184704e-06,
      "loss": 0.6754,
      "step": 46640
    },
    {
      "epoch": 1.8419078453823983,
      "grad_norm": 0.8955589532852173,
      "learning_rate": 7.906581908635054e-06,
      "loss": 0.6906,
      "step": 46650
    },
    {
      "epoch": 1.8423026809333911,
      "grad_norm": 0.8749362826347351,
      "learning_rate": 7.886840131085403e-06,
      "loss": 0.7141,
      "step": 46660
    },
    {
      "epoch": 1.8426975164843844,
      "grad_norm": 0.7508519291877747,
      "learning_rate": 7.867098353535752e-06,
      "loss": 0.7053,
      "step": 46670
    },
    {
      "epoch": 1.8430923520353772,
      "grad_norm": 0.7028515338897705,
      "learning_rate": 7.847356575986102e-06,
      "loss": 0.6945,
      "step": 46680
    },
    {
      "epoch": 1.8434871875863703,
      "grad_norm": 0.9608295559883118,
      "learning_rate": 7.827614798436452e-06,
      "loss": 0.708,
      "step": 46690
    },
    {
      "epoch": 1.8438820231373634,
      "grad_norm": 0.8041428327560425,
      "learning_rate": 7.8078730208868e-06,
      "loss": 0.7157,
      "step": 46700
    },
    {
      "epoch": 1.8442768586883562,
      "grad_norm": 0.8600552678108215,
      "learning_rate": 7.78813124333715e-06,
      "loss": 0.6789,
      "step": 46710
    },
    {
      "epoch": 1.8446716942393493,
      "grad_norm": 0.7549857497215271,
      "learning_rate": 7.7683894657875e-06,
      "loss": 0.6903,
      "step": 46720
    },
    {
      "epoch": 1.8450665297903424,
      "grad_norm": 0.7692672610282898,
      "learning_rate": 7.748647688237849e-06,
      "loss": 0.7064,
      "step": 46730
    },
    {
      "epoch": 1.8454613653413352,
      "grad_norm": 0.8689179420471191,
      "learning_rate": 7.728905910688198e-06,
      "loss": 0.734,
      "step": 46740
    },
    {
      "epoch": 1.8458562008923285,
      "grad_norm": 0.6276715397834778,
      "learning_rate": 7.709164133138548e-06,
      "loss": 0.7054,
      "step": 46750
    },
    {
      "epoch": 1.8462510364433213,
      "grad_norm": 1.0241814851760864,
      "learning_rate": 7.689422355588898e-06,
      "loss": 0.7009,
      "step": 46760
    },
    {
      "epoch": 1.8466458719943144,
      "grad_norm": 0.8161004781723022,
      "learning_rate": 7.669680578039247e-06,
      "loss": 0.6466,
      "step": 46770
    },
    {
      "epoch": 1.8470407075453075,
      "grad_norm": 0.7369834780693054,
      "learning_rate": 7.649938800489596e-06,
      "loss": 0.6806,
      "step": 46780
    },
    {
      "epoch": 1.8474355430963003,
      "grad_norm": 0.7780243754386902,
      "learning_rate": 7.630197022939946e-06,
      "loss": 0.6739,
      "step": 46790
    },
    {
      "epoch": 1.8478303786472934,
      "grad_norm": 0.9309291839599609,
      "learning_rate": 7.610455245390295e-06,
      "loss": 0.7191,
      "step": 46800
    },
    {
      "epoch": 1.8482252141982864,
      "grad_norm": 0.8455262780189514,
      "learning_rate": 7.5907134678406445e-06,
      "loss": 0.7158,
      "step": 46810
    },
    {
      "epoch": 1.8486200497492793,
      "grad_norm": 0.7331398129463196,
      "learning_rate": 7.570971690290994e-06,
      "loss": 0.689,
      "step": 46820
    },
    {
      "epoch": 1.8490148853002726,
      "grad_norm": 0.8462820053100586,
      "learning_rate": 7.5512299127413435e-06,
      "loss": 0.7293,
      "step": 46830
    },
    {
      "epoch": 1.8494097208512654,
      "grad_norm": 0.8414919376373291,
      "learning_rate": 7.531488135191693e-06,
      "loss": 0.6852,
      "step": 46840
    },
    {
      "epoch": 1.8498045564022585,
      "grad_norm": 0.9335523247718811,
      "learning_rate": 7.5117463576420424e-06,
      "loss": 0.6644,
      "step": 46850
    },
    {
      "epoch": 1.8501993919532516,
      "grad_norm": 0.9094613790512085,
      "learning_rate": 7.4920045800923915e-06,
      "loss": 0.7334,
      "step": 46860
    },
    {
      "epoch": 1.8505942275042444,
      "grad_norm": 0.80772465467453,
      "learning_rate": 7.472262802542741e-06,
      "loss": 0.7056,
      "step": 46870
    },
    {
      "epoch": 1.8509890630552375,
      "grad_norm": 1.0587953329086304,
      "learning_rate": 7.4525210249930905e-06,
      "loss": 0.7642,
      "step": 46880
    },
    {
      "epoch": 1.8513838986062305,
      "grad_norm": 0.8077258467674255,
      "learning_rate": 7.43277924744344e-06,
      "loss": 0.7195,
      "step": 46890
    },
    {
      "epoch": 1.8517787341572234,
      "grad_norm": 0.8125216364860535,
      "learning_rate": 7.4130374698937895e-06,
      "loss": 0.6801,
      "step": 46900
    },
    {
      "epoch": 1.8521735697082167,
      "grad_norm": 0.7048467993736267,
      "learning_rate": 7.393295692344139e-06,
      "loss": 0.6425,
      "step": 46910
    },
    {
      "epoch": 1.8525684052592095,
      "grad_norm": 0.8001709580421448,
      "learning_rate": 7.3735539147944884e-06,
      "loss": 0.6796,
      "step": 46920
    },
    {
      "epoch": 1.8529632408102026,
      "grad_norm": 0.684623122215271,
      "learning_rate": 7.3538121372448375e-06,
      "loss": 0.7236,
      "step": 46930
    },
    {
      "epoch": 1.8533580763611956,
      "grad_norm": 0.8024210929870605,
      "learning_rate": 7.334070359695187e-06,
      "loss": 0.666,
      "step": 46940
    },
    {
      "epoch": 1.8537529119121885,
      "grad_norm": 1.2607898712158203,
      "learning_rate": 7.3143285821455365e-06,
      "loss": 0.7178,
      "step": 46950
    },
    {
      "epoch": 1.8541477474631816,
      "grad_norm": 0.9003880620002747,
      "learning_rate": 7.294586804595886e-06,
      "loss": 0.6864,
      "step": 46960
    },
    {
      "epoch": 1.8545425830141746,
      "grad_norm": 0.7998266220092773,
      "learning_rate": 7.2748450270462355e-06,
      "loss": 0.7105,
      "step": 46970
    },
    {
      "epoch": 1.8549374185651675,
      "grad_norm": 0.780170738697052,
      "learning_rate": 7.255103249496585e-06,
      "loss": 0.7521,
      "step": 46980
    },
    {
      "epoch": 1.8553322541161608,
      "grad_norm": 0.7967107892036438,
      "learning_rate": 7.2353614719469344e-06,
      "loss": 0.6878,
      "step": 46990
    },
    {
      "epoch": 1.8557270896671536,
      "grad_norm": 0.7746988534927368,
      "learning_rate": 7.215619694397284e-06,
      "loss": 0.721,
      "step": 47000
    },
    {
      "epoch": 1.8557270896671536,
      "eval_loss": 0.7544804811477661,
      "eval_runtime": 1217.9604,
      "eval_samples_per_second": 9.243,
      "eval_steps_per_second": 9.243,
      "step": 47000
    },
    {
      "epoch": 1.8561219252181467,
      "grad_norm": 0.7707980871200562,
      "learning_rate": 7.1958779168476334e-06,
      "loss": 0.7167,
      "step": 47010
    },
    {
      "epoch": 1.8565167607691397,
      "grad_norm": 0.8793439865112305,
      "learning_rate": 7.1761361392979825e-06,
      "loss": 0.7366,
      "step": 47020
    },
    {
      "epoch": 1.8569115963201326,
      "grad_norm": 0.719347357749939,
      "learning_rate": 7.156394361748332e-06,
      "loss": 0.709,
      "step": 47030
    },
    {
      "epoch": 1.8573064318711257,
      "grad_norm": 0.8498075604438782,
      "learning_rate": 7.1366525841986815e-06,
      "loss": 0.7224,
      "step": 47040
    },
    {
      "epoch": 1.8577012674221187,
      "grad_norm": 0.7502905130386353,
      "learning_rate": 7.116910806649031e-06,
      "loss": 0.6949,
      "step": 47050
    },
    {
      "epoch": 1.8580961029731116,
      "grad_norm": 0.8525855541229248,
      "learning_rate": 7.0971690290993805e-06,
      "loss": 0.7056,
      "step": 47060
    },
    {
      "epoch": 1.8584909385241049,
      "grad_norm": 0.9122633934020996,
      "learning_rate": 7.07742725154973e-06,
      "loss": 0.6717,
      "step": 47070
    },
    {
      "epoch": 1.8588857740750977,
      "grad_norm": 0.8252778053283691,
      "learning_rate": 7.0576854740000794e-06,
      "loss": 0.7532,
      "step": 47080
    },
    {
      "epoch": 1.8592806096260908,
      "grad_norm": 0.7604790329933167,
      "learning_rate": 7.0379436964504285e-06,
      "loss": 0.651,
      "step": 47090
    },
    {
      "epoch": 1.8596754451770838,
      "grad_norm": 0.7431934475898743,
      "learning_rate": 7.018201918900778e-06,
      "loss": 0.6869,
      "step": 47100
    },
    {
      "epoch": 1.8600702807280767,
      "grad_norm": 1.0558007955551147,
      "learning_rate": 6.9984601413511275e-06,
      "loss": 0.7308,
      "step": 47110
    },
    {
      "epoch": 1.8604651162790697,
      "grad_norm": 0.7677638530731201,
      "learning_rate": 6.978718363801477e-06,
      "loss": 0.7096,
      "step": 47120
    },
    {
      "epoch": 1.8608599518300628,
      "grad_norm": 0.8938320875167847,
      "learning_rate": 6.9589765862518265e-06,
      "loss": 0.6842,
      "step": 47130
    },
    {
      "epoch": 1.8612547873810557,
      "grad_norm": 0.6545189619064331,
      "learning_rate": 6.939234808702176e-06,
      "loss": 0.6797,
      "step": 47140
    },
    {
      "epoch": 1.861649622932049,
      "grad_norm": 0.7162421941757202,
      "learning_rate": 6.9194930311525254e-06,
      "loss": 0.6923,
      "step": 47150
    },
    {
      "epoch": 1.8620444584830418,
      "grad_norm": 0.7824987769126892,
      "learning_rate": 6.899751253602875e-06,
      "loss": 0.742,
      "step": 47160
    },
    {
      "epoch": 1.8624392940340349,
      "grad_norm": 0.9429476261138916,
      "learning_rate": 6.8800094760532244e-06,
      "loss": 0.7193,
      "step": 47170
    },
    {
      "epoch": 1.862834129585028,
      "grad_norm": 0.6450710296630859,
      "learning_rate": 6.8602676985035735e-06,
      "loss": 0.6929,
      "step": 47180
    },
    {
      "epoch": 1.8632289651360208,
      "grad_norm": 0.8707125782966614,
      "learning_rate": 6.840525920953923e-06,
      "loss": 0.7143,
      "step": 47190
    },
    {
      "epoch": 1.8636238006870138,
      "grad_norm": 0.7306587100028992,
      "learning_rate": 6.8207841434042725e-06,
      "loss": 0.6916,
      "step": 47200
    },
    {
      "epoch": 1.864018636238007,
      "grad_norm": 1.1157294511795044,
      "learning_rate": 6.801042365854622e-06,
      "loss": 0.7029,
      "step": 47210
    },
    {
      "epoch": 1.8644134717889997,
      "grad_norm": 0.7484585642814636,
      "learning_rate": 6.7813005883049715e-06,
      "loss": 0.7226,
      "step": 47220
    },
    {
      "epoch": 1.864808307339993,
      "grad_norm": 0.8419356346130371,
      "learning_rate": 6.761558810755321e-06,
      "loss": 0.694,
      "step": 47230
    },
    {
      "epoch": 1.8652031428909859,
      "grad_norm": 0.793476402759552,
      "learning_rate": 6.7418170332056704e-06,
      "loss": 0.6983,
      "step": 47240
    },
    {
      "epoch": 1.865597978441979,
      "grad_norm": 0.7315465807914734,
      "learning_rate": 6.7220752556560195e-06,
      "loss": 0.7158,
      "step": 47250
    },
    {
      "epoch": 1.865992813992972,
      "grad_norm": 0.769397497177124,
      "learning_rate": 6.702333478106369e-06,
      "loss": 0.7439,
      "step": 47260
    },
    {
      "epoch": 1.8663876495439649,
      "grad_norm": 0.9942103624343872,
      "learning_rate": 6.6825917005567185e-06,
      "loss": 0.7077,
      "step": 47270
    },
    {
      "epoch": 1.866782485094958,
      "grad_norm": 0.7100070714950562,
      "learning_rate": 6.662849923007068e-06,
      "loss": 0.6878,
      "step": 47280
    },
    {
      "epoch": 1.867177320645951,
      "grad_norm": 0.6614181399345398,
      "learning_rate": 6.6431081454574175e-06,
      "loss": 0.6885,
      "step": 47290
    },
    {
      "epoch": 1.8675721561969438,
      "grad_norm": 0.7366678714752197,
      "learning_rate": 6.623366367907767e-06,
      "loss": 0.7046,
      "step": 47300
    },
    {
      "epoch": 1.8679669917479371,
      "grad_norm": 1.0719581842422485,
      "learning_rate": 6.6036245903581164e-06,
      "loss": 0.7347,
      "step": 47310
    },
    {
      "epoch": 1.86836182729893,
      "grad_norm": 0.7200345396995544,
      "learning_rate": 6.5838828128084655e-06,
      "loss": 0.6697,
      "step": 47320
    },
    {
      "epoch": 1.868756662849923,
      "grad_norm": 0.754326343536377,
      "learning_rate": 6.5641410352588154e-06,
      "loss": 0.6766,
      "step": 47330
    },
    {
      "epoch": 1.869151498400916,
      "grad_norm": 0.7146022915840149,
      "learning_rate": 6.5443992577091645e-06,
      "loss": 0.6711,
      "step": 47340
    },
    {
      "epoch": 1.869546333951909,
      "grad_norm": 0.7577906847000122,
      "learning_rate": 6.524657480159514e-06,
      "loss": 0.6767,
      "step": 47350
    },
    {
      "epoch": 1.869941169502902,
      "grad_norm": 0.7845191359519958,
      "learning_rate": 6.5049157026098635e-06,
      "loss": 0.7215,
      "step": 47360
    },
    {
      "epoch": 1.870336005053895,
      "grad_norm": 0.6392821073532104,
      "learning_rate": 6.485173925060213e-06,
      "loss": 0.6362,
      "step": 47370
    },
    {
      "epoch": 1.870730840604888,
      "grad_norm": 0.8585682511329651,
      "learning_rate": 6.4654321475105625e-06,
      "loss": 0.6906,
      "step": 47380
    },
    {
      "epoch": 1.8711256761558812,
      "grad_norm": 0.703103244304657,
      "learning_rate": 6.445690369960912e-06,
      "loss": 0.6861,
      "step": 47390
    },
    {
      "epoch": 1.871520511706874,
      "grad_norm": 0.8005165457725525,
      "learning_rate": 6.4259485924112614e-06,
      "loss": 0.7022,
      "step": 47400
    },
    {
      "epoch": 1.8719153472578671,
      "grad_norm": 0.793250322341919,
      "learning_rate": 6.4062068148616105e-06,
      "loss": 0.6872,
      "step": 47410
    },
    {
      "epoch": 1.8723101828088602,
      "grad_norm": 0.8082048296928406,
      "learning_rate": 6.38646503731196e-06,
      "loss": 0.697,
      "step": 47420
    },
    {
      "epoch": 1.872705018359853,
      "grad_norm": 0.7309751510620117,
      "learning_rate": 6.3667232597623095e-06,
      "loss": 0.6968,
      "step": 47430
    },
    {
      "epoch": 1.873099853910846,
      "grad_norm": 0.9805635213851929,
      "learning_rate": 6.346981482212659e-06,
      "loss": 0.6668,
      "step": 47440
    },
    {
      "epoch": 1.8734946894618392,
      "grad_norm": 1.0038679838180542,
      "learning_rate": 6.3272397046630085e-06,
      "loss": 0.6918,
      "step": 47450
    },
    {
      "epoch": 1.873889525012832,
      "grad_norm": 0.8433971405029297,
      "learning_rate": 6.307497927113358e-06,
      "loss": 0.6849,
      "step": 47460
    },
    {
      "epoch": 1.8742843605638253,
      "grad_norm": 0.836633563041687,
      "learning_rate": 6.2877561495637074e-06,
      "loss": 0.7011,
      "step": 47470
    },
    {
      "epoch": 1.8746791961148181,
      "grad_norm": 0.873997688293457,
      "learning_rate": 6.2680143720140565e-06,
      "loss": 0.7232,
      "step": 47480
    },
    {
      "epoch": 1.8750740316658112,
      "grad_norm": 0.7216944694519043,
      "learning_rate": 6.248272594464406e-06,
      "loss": 0.6877,
      "step": 47490
    },
    {
      "epoch": 1.8754688672168043,
      "grad_norm": 0.6789066791534424,
      "learning_rate": 6.228530816914755e-06,
      "loss": 0.6935,
      "step": 47500
    },
    {
      "epoch": 1.8758637027677971,
      "grad_norm": 0.9404872059822083,
      "learning_rate": 6.2087890393651046e-06,
      "loss": 0.7162,
      "step": 47510
    },
    {
      "epoch": 1.8762585383187902,
      "grad_norm": 0.618905246257782,
      "learning_rate": 6.189047261815454e-06,
      "loss": 0.6664,
      "step": 47520
    },
    {
      "epoch": 1.8766533738697833,
      "grad_norm": 0.765815258026123,
      "learning_rate": 6.1693054842658035e-06,
      "loss": 0.6853,
      "step": 47530
    },
    {
      "epoch": 1.877048209420776,
      "grad_norm": 0.9339948892593384,
      "learning_rate": 6.149563706716153e-06,
      "loss": 0.6901,
      "step": 47540
    },
    {
      "epoch": 1.8774430449717694,
      "grad_norm": 0.9133154153823853,
      "learning_rate": 6.1298219291665025e-06,
      "loss": 0.7194,
      "step": 47550
    },
    {
      "epoch": 1.8778378805227622,
      "grad_norm": 0.9564730525016785,
      "learning_rate": 6.110080151616852e-06,
      "loss": 0.7128,
      "step": 47560
    },
    {
      "epoch": 1.8782327160737553,
      "grad_norm": 0.8438473343849182,
      "learning_rate": 6.0903383740672015e-06,
      "loss": 0.7096,
      "step": 47570
    },
    {
      "epoch": 1.8786275516247484,
      "grad_norm": 0.7940175533294678,
      "learning_rate": 6.0705965965175506e-06,
      "loss": 0.7198,
      "step": 47580
    },
    {
      "epoch": 1.8790223871757412,
      "grad_norm": 0.8317682147026062,
      "learning_rate": 6.0508548189679e-06,
      "loss": 0.7254,
      "step": 47590
    },
    {
      "epoch": 1.8794172227267343,
      "grad_norm": 0.909123957157135,
      "learning_rate": 6.0311130414182495e-06,
      "loss": 0.7346,
      "step": 47600
    },
    {
      "epoch": 1.8798120582777273,
      "grad_norm": 0.7123786807060242,
      "learning_rate": 6.011371263868599e-06,
      "loss": 0.6715,
      "step": 47610
    },
    {
      "epoch": 1.8802068938287202,
      "grad_norm": 1.045251727104187,
      "learning_rate": 5.9916294863189485e-06,
      "loss": 0.6509,
      "step": 47620
    },
    {
      "epoch": 1.8806017293797135,
      "grad_norm": 0.6567856669425964,
      "learning_rate": 5.971887708769298e-06,
      "loss": 0.6771,
      "step": 47630
    },
    {
      "epoch": 1.8809965649307063,
      "grad_norm": 0.8289692997932434,
      "learning_rate": 5.9521459312196475e-06,
      "loss": 0.7045,
      "step": 47640
    },
    {
      "epoch": 1.8813914004816994,
      "grad_norm": 1.0120041370391846,
      "learning_rate": 5.932404153669997e-06,
      "loss": 0.7114,
      "step": 47650
    },
    {
      "epoch": 1.8817862360326925,
      "grad_norm": 0.9213926792144775,
      "learning_rate": 5.912662376120346e-06,
      "loss": 0.6607,
      "step": 47660
    },
    {
      "epoch": 1.8821810715836853,
      "grad_norm": 0.881729245185852,
      "learning_rate": 5.8929205985706956e-06,
      "loss": 0.7362,
      "step": 47670
    },
    {
      "epoch": 1.8825759071346784,
      "grad_norm": 0.7604683637619019,
      "learning_rate": 5.873178821021045e-06,
      "loss": 0.6694,
      "step": 47680
    },
    {
      "epoch": 1.8829707426856714,
      "grad_norm": 0.7801480889320374,
      "learning_rate": 5.8534370434713945e-06,
      "loss": 0.6832,
      "step": 47690
    },
    {
      "epoch": 1.8833655782366643,
      "grad_norm": 0.6619493961334229,
      "learning_rate": 5.833695265921744e-06,
      "loss": 0.714,
      "step": 47700
    },
    {
      "epoch": 1.8837604137876576,
      "grad_norm": 0.8021597266197205,
      "learning_rate": 5.8139534883720935e-06,
      "loss": 0.7205,
      "step": 47710
    },
    {
      "epoch": 1.8841552493386504,
      "grad_norm": 0.708436131477356,
      "learning_rate": 5.794211710822443e-06,
      "loss": 0.7272,
      "step": 47720
    },
    {
      "epoch": 1.8845500848896435,
      "grad_norm": 0.8327857851982117,
      "learning_rate": 5.7744699332727925e-06,
      "loss": 0.6653,
      "step": 47730
    },
    {
      "epoch": 1.8849449204406366,
      "grad_norm": 0.6927425861358643,
      "learning_rate": 5.7547281557231416e-06,
      "loss": 0.6784,
      "step": 47740
    },
    {
      "epoch": 1.8853397559916294,
      "grad_norm": 0.8293842077255249,
      "learning_rate": 5.734986378173491e-06,
      "loss": 0.7147,
      "step": 47750
    },
    {
      "epoch": 1.8857345915426225,
      "grad_norm": 0.9779945015907288,
      "learning_rate": 5.7152446006238405e-06,
      "loss": 0.7287,
      "step": 47760
    },
    {
      "epoch": 1.8861294270936155,
      "grad_norm": 0.7932211756706238,
      "learning_rate": 5.69550282307419e-06,
      "loss": 0.6698,
      "step": 47770
    },
    {
      "epoch": 1.8865242626446084,
      "grad_norm": 0.7248130440711975,
      "learning_rate": 5.6757610455245395e-06,
      "loss": 0.6841,
      "step": 47780
    },
    {
      "epoch": 1.8869190981956017,
      "grad_norm": 0.8628813028335571,
      "learning_rate": 5.656019267974889e-06,
      "loss": 0.7087,
      "step": 47790
    },
    {
      "epoch": 1.8873139337465945,
      "grad_norm": 0.8821097612380981,
      "learning_rate": 5.6362774904252385e-06,
      "loss": 0.7046,
      "step": 47800
    },
    {
      "epoch": 1.8877087692975876,
      "grad_norm": 0.7662725448608398,
      "learning_rate": 5.6165357128755876e-06,
      "loss": 0.7089,
      "step": 47810
    },
    {
      "epoch": 1.8881036048485806,
      "grad_norm": 0.6564510464668274,
      "learning_rate": 5.596793935325937e-06,
      "loss": 0.7035,
      "step": 47820
    },
    {
      "epoch": 1.8884984403995735,
      "grad_norm": 0.6755431890487671,
      "learning_rate": 5.5770521577762866e-06,
      "loss": 0.6788,
      "step": 47830
    },
    {
      "epoch": 1.8888932759505666,
      "grad_norm": 0.7899835109710693,
      "learning_rate": 5.557310380226636e-06,
      "loss": 0.6972,
      "step": 47840
    },
    {
      "epoch": 1.8892881115015596,
      "grad_norm": 0.845319926738739,
      "learning_rate": 5.5375686026769855e-06,
      "loss": 0.6495,
      "step": 47850
    },
    {
      "epoch": 1.8896829470525525,
      "grad_norm": 0.8197422027587891,
      "learning_rate": 5.517826825127335e-06,
      "loss": 0.6966,
      "step": 47860
    },
    {
      "epoch": 1.8900777826035458,
      "grad_norm": 0.6247815489768982,
      "learning_rate": 5.4980850475776845e-06,
      "loss": 0.7233,
      "step": 47870
    },
    {
      "epoch": 1.8904726181545386,
      "grad_norm": 0.9221707582473755,
      "learning_rate": 5.478343270028034e-06,
      "loss": 0.6761,
      "step": 47880
    },
    {
      "epoch": 1.8908674537055317,
      "grad_norm": 0.8979669809341431,
      "learning_rate": 5.458601492478383e-06,
      "loss": 0.6133,
      "step": 47890
    },
    {
      "epoch": 1.8912622892565247,
      "grad_norm": 0.8799835443496704,
      "learning_rate": 5.4388597149287326e-06,
      "loss": 0.6982,
      "step": 47900
    },
    {
      "epoch": 1.8916571248075176,
      "grad_norm": 0.995053231716156,
      "learning_rate": 5.419117937379082e-06,
      "loss": 0.7199,
      "step": 47910
    },
    {
      "epoch": 1.8920519603585106,
      "grad_norm": 0.7505191564559937,
      "learning_rate": 5.3993761598294315e-06,
      "loss": 0.7152,
      "step": 47920
    },
    {
      "epoch": 1.8924467959095037,
      "grad_norm": 0.6645926833152771,
      "learning_rate": 5.379634382279781e-06,
      "loss": 0.6709,
      "step": 47930
    },
    {
      "epoch": 1.8928416314604966,
      "grad_norm": 0.9127821326255798,
      "learning_rate": 5.3598926047301305e-06,
      "loss": 0.6804,
      "step": 47940
    },
    {
      "epoch": 1.8932364670114898,
      "grad_norm": 0.9358828067779541,
      "learning_rate": 5.34015082718048e-06,
      "loss": 0.663,
      "step": 47950
    },
    {
      "epoch": 1.8936313025624827,
      "grad_norm": 0.7280204892158508,
      "learning_rate": 5.3204090496308295e-06,
      "loss": 0.6745,
      "step": 47960
    },
    {
      "epoch": 1.8940261381134758,
      "grad_norm": 0.9522601366043091,
      "learning_rate": 5.3006672720811786e-06,
      "loss": 0.7384,
      "step": 47970
    },
    {
      "epoch": 1.8944209736644688,
      "grad_norm": 0.8514822721481323,
      "learning_rate": 5.280925494531528e-06,
      "loss": 0.6554,
      "step": 47980
    },
    {
      "epoch": 1.8948158092154617,
      "grad_norm": 0.789240837097168,
      "learning_rate": 5.2611837169818776e-06,
      "loss": 0.715,
      "step": 47990
    },
    {
      "epoch": 1.8952106447664547,
      "grad_norm": 0.9099814295768738,
      "learning_rate": 5.241441939432227e-06,
      "loss": 0.717,
      "step": 48000
    },
    {
      "epoch": 1.8952106447664547,
      "eval_loss": 0.7539747953414917,
      "eval_runtime": 1218.3413,
      "eval_samples_per_second": 9.24,
      "eval_steps_per_second": 9.24,
      "step": 48000
    },
    {
      "epoch": 1.8956054803174478,
      "grad_norm": 0.7512897253036499,
      "learning_rate": 5.2217001618825765e-06,
      "loss": 0.6972,
      "step": 48010
    },
    {
      "epoch": 1.8960003158684406,
      "grad_norm": 0.8074014782905579,
      "learning_rate": 5.201958384332926e-06,
      "loss": 0.6849,
      "step": 48020
    },
    {
      "epoch": 1.896395151419434,
      "grad_norm": 0.7338407635688782,
      "learning_rate": 5.1822166067832755e-06,
      "loss": 0.7111,
      "step": 48030
    },
    {
      "epoch": 1.8967899869704268,
      "grad_norm": 0.7543435096740723,
      "learning_rate": 5.162474829233625e-06,
      "loss": 0.7386,
      "step": 48040
    },
    {
      "epoch": 1.8971848225214198,
      "grad_norm": 0.9038782119750977,
      "learning_rate": 5.142733051683974e-06,
      "loss": 0.7128,
      "step": 48050
    },
    {
      "epoch": 1.897579658072413,
      "grad_norm": 0.8986155390739441,
      "learning_rate": 5.1229912741343236e-06,
      "loss": 0.7115,
      "step": 48060
    },
    {
      "epoch": 1.8979744936234058,
      "grad_norm": 1.0205087661743164,
      "learning_rate": 5.103249496584673e-06,
      "loss": 0.7304,
      "step": 48070
    },
    {
      "epoch": 1.8983693291743988,
      "grad_norm": 0.7085068225860596,
      "learning_rate": 5.0835077190350225e-06,
      "loss": 0.6644,
      "step": 48080
    },
    {
      "epoch": 1.898764164725392,
      "grad_norm": 0.6418894529342651,
      "learning_rate": 5.063765941485372e-06,
      "loss": 0.7113,
      "step": 48090
    },
    {
      "epoch": 1.8991590002763847,
      "grad_norm": 0.9449887871742249,
      "learning_rate": 5.0440241639357215e-06,
      "loss": 0.7,
      "step": 48100
    },
    {
      "epoch": 1.899553835827378,
      "grad_norm": 0.7549515962600708,
      "learning_rate": 5.024282386386071e-06,
      "loss": 0.6929,
      "step": 48110
    },
    {
      "epoch": 1.8999486713783709,
      "grad_norm": 0.8049249053001404,
      "learning_rate": 5.0045406088364205e-06,
      "loss": 0.7184,
      "step": 48120
    },
    {
      "epoch": 1.900343506929364,
      "grad_norm": 0.6208657622337341,
      "learning_rate": 4.9847988312867696e-06,
      "loss": 0.6913,
      "step": 48130
    },
    {
      "epoch": 1.900738342480357,
      "grad_norm": 0.8264468908309937,
      "learning_rate": 4.965057053737119e-06,
      "loss": 0.7196,
      "step": 48140
    },
    {
      "epoch": 1.9011331780313498,
      "grad_norm": 0.6391756534576416,
      "learning_rate": 4.9453152761874686e-06,
      "loss": 0.6763,
      "step": 48150
    },
    {
      "epoch": 1.901528013582343,
      "grad_norm": 0.9514193534851074,
      "learning_rate": 4.925573498637818e-06,
      "loss": 0.7264,
      "step": 48160
    },
    {
      "epoch": 1.901922849133336,
      "grad_norm": 0.7266397476196289,
      "learning_rate": 4.9058317210881675e-06,
      "loss": 0.6555,
      "step": 48170
    },
    {
      "epoch": 1.9023176846843288,
      "grad_norm": 1.0921790599822998,
      "learning_rate": 4.886089943538517e-06,
      "loss": 0.7246,
      "step": 48180
    },
    {
      "epoch": 1.9027125202353221,
      "grad_norm": 0.8557822704315186,
      "learning_rate": 4.8663481659888665e-06,
      "loss": 0.6797,
      "step": 48190
    },
    {
      "epoch": 1.903107355786315,
      "grad_norm": 0.8163785934448242,
      "learning_rate": 4.846606388439216e-06,
      "loss": 0.6751,
      "step": 48200
    },
    {
      "epoch": 1.903502191337308,
      "grad_norm": 0.7269656658172607,
      "learning_rate": 4.826864610889565e-06,
      "loss": 0.7095,
      "step": 48210
    },
    {
      "epoch": 1.903897026888301,
      "grad_norm": 0.7591850757598877,
      "learning_rate": 4.8071228333399146e-06,
      "loss": 0.7186,
      "step": 48220
    },
    {
      "epoch": 1.904291862439294,
      "grad_norm": 0.8347899317741394,
      "learning_rate": 4.787381055790264e-06,
      "loss": 0.6623,
      "step": 48230
    },
    {
      "epoch": 1.904686697990287,
      "grad_norm": 0.7255823612213135,
      "learning_rate": 4.7676392782406135e-06,
      "loss": 0.6907,
      "step": 48240
    },
    {
      "epoch": 1.90508153354128,
      "grad_norm": 0.7645082473754883,
      "learning_rate": 4.747897500690963e-06,
      "loss": 0.7353,
      "step": 48250
    },
    {
      "epoch": 1.905476369092273,
      "grad_norm": 0.8405361175537109,
      "learning_rate": 4.7281557231413125e-06,
      "loss": 0.7231,
      "step": 48260
    },
    {
      "epoch": 1.9058712046432662,
      "grad_norm": 1.0021871328353882,
      "learning_rate": 4.708413945591662e-06,
      "loss": 0.7047,
      "step": 48270
    },
    {
      "epoch": 1.906266040194259,
      "grad_norm": 0.8902440667152405,
      "learning_rate": 4.688672168042011e-06,
      "loss": 0.6819,
      "step": 48280
    },
    {
      "epoch": 1.9066608757452521,
      "grad_norm": 1.0521453619003296,
      "learning_rate": 4.6689303904923606e-06,
      "loss": 0.7167,
      "step": 48290
    },
    {
      "epoch": 1.9070557112962452,
      "grad_norm": 0.7168603539466858,
      "learning_rate": 4.64918861294271e-06,
      "loss": 0.6957,
      "step": 48300
    },
    {
      "epoch": 1.907450546847238,
      "grad_norm": 0.7624123692512512,
      "learning_rate": 4.629446835393059e-06,
      "loss": 0.6916,
      "step": 48310
    },
    {
      "epoch": 1.907845382398231,
      "grad_norm": 0.8543710708618164,
      "learning_rate": 4.609705057843408e-06,
      "loss": 0.7038,
      "step": 48320
    },
    {
      "epoch": 1.9082402179492242,
      "grad_norm": 0.9230392575263977,
      "learning_rate": 4.589963280293758e-06,
      "loss": 0.7048,
      "step": 48330
    },
    {
      "epoch": 1.908635053500217,
      "grad_norm": 0.677672266960144,
      "learning_rate": 4.570221502744107e-06,
      "loss": 0.7608,
      "step": 48340
    },
    {
      "epoch": 1.9090298890512103,
      "grad_norm": 0.7189939022064209,
      "learning_rate": 4.550479725194457e-06,
      "loss": 0.6948,
      "step": 48350
    },
    {
      "epoch": 1.9094247246022031,
      "grad_norm": 0.8054037690162659,
      "learning_rate": 4.530737947644806e-06,
      "loss": 0.6993,
      "step": 48360
    },
    {
      "epoch": 1.9098195601531962,
      "grad_norm": 0.728659987449646,
      "learning_rate": 4.510996170095156e-06,
      "loss": 0.7296,
      "step": 48370
    },
    {
      "epoch": 1.9102143957041893,
      "grad_norm": 0.7827717065811157,
      "learning_rate": 4.491254392545505e-06,
      "loss": 0.6486,
      "step": 48380
    },
    {
      "epoch": 1.9106092312551821,
      "grad_norm": 0.7786186933517456,
      "learning_rate": 4.471512614995854e-06,
      "loss": 0.6927,
      "step": 48390
    },
    {
      "epoch": 1.9110040668061752,
      "grad_norm": 0.774491548538208,
      "learning_rate": 4.451770837446204e-06,
      "loss": 0.7083,
      "step": 48400
    },
    {
      "epoch": 1.9113989023571683,
      "grad_norm": 0.6784380078315735,
      "learning_rate": 4.432029059896553e-06,
      "loss": 0.6864,
      "step": 48410
    },
    {
      "epoch": 1.911793737908161,
      "grad_norm": 0.9413374662399292,
      "learning_rate": 4.412287282346903e-06,
      "loss": 0.6931,
      "step": 48420
    },
    {
      "epoch": 1.9121885734591544,
      "grad_norm": 0.7788183093070984,
      "learning_rate": 4.392545504797252e-06,
      "loss": 0.7115,
      "step": 48430
    },
    {
      "epoch": 1.9125834090101472,
      "grad_norm": 0.908157229423523,
      "learning_rate": 4.372803727247602e-06,
      "loss": 0.7194,
      "step": 48440
    },
    {
      "epoch": 1.9129782445611403,
      "grad_norm": 0.8250314593315125,
      "learning_rate": 4.353061949697951e-06,
      "loss": 0.7305,
      "step": 48450
    },
    {
      "epoch": 1.9133730801121334,
      "grad_norm": 0.9455855488777161,
      "learning_rate": 4.333320172148301e-06,
      "loss": 0.688,
      "step": 48460
    },
    {
      "epoch": 1.9137679156631262,
      "grad_norm": 0.8199595212936401,
      "learning_rate": 4.31357839459865e-06,
      "loss": 0.7009,
      "step": 48470
    },
    {
      "epoch": 1.9141627512141193,
      "grad_norm": 0.740222156047821,
      "learning_rate": 4.293836617048999e-06,
      "loss": 0.6474,
      "step": 48480
    },
    {
      "epoch": 1.9145575867651123,
      "grad_norm": 0.801051914691925,
      "learning_rate": 4.274094839499349e-06,
      "loss": 0.691,
      "step": 48490
    },
    {
      "epoch": 1.9149524223161052,
      "grad_norm": 0.7701072096824646,
      "learning_rate": 4.254353061949698e-06,
      "loss": 0.7263,
      "step": 48500
    },
    {
      "epoch": 1.9153472578670985,
      "grad_norm": 0.8874740600585938,
      "learning_rate": 4.234611284400048e-06,
      "loss": 0.6858,
      "step": 48510
    },
    {
      "epoch": 1.9157420934180913,
      "grad_norm": 0.7695954442024231,
      "learning_rate": 4.214869506850397e-06,
      "loss": 0.7193,
      "step": 48520
    },
    {
      "epoch": 1.9161369289690844,
      "grad_norm": 0.7765610814094543,
      "learning_rate": 4.195127729300747e-06,
      "loss": 0.694,
      "step": 48530
    },
    {
      "epoch": 1.9165317645200775,
      "grad_norm": 1.0718578100204468,
      "learning_rate": 4.175385951751096e-06,
      "loss": 0.6849,
      "step": 48540
    },
    {
      "epoch": 1.9169266000710703,
      "grad_norm": 0.9714473485946655,
      "learning_rate": 4.155644174201445e-06,
      "loss": 0.7775,
      "step": 48550
    },
    {
      "epoch": 1.9173214356220634,
      "grad_norm": 1.1433618068695068,
      "learning_rate": 4.135902396651795e-06,
      "loss": 0.6632,
      "step": 48560
    },
    {
      "epoch": 1.9177162711730564,
      "grad_norm": 0.6880604028701782,
      "learning_rate": 4.116160619102144e-06,
      "loss": 0.6878,
      "step": 48570
    },
    {
      "epoch": 1.9181111067240493,
      "grad_norm": 1.1696699857711792,
      "learning_rate": 4.096418841552494e-06,
      "loss": 0.6996,
      "step": 48580
    },
    {
      "epoch": 1.9185059422750426,
      "grad_norm": 0.7356840372085571,
      "learning_rate": 4.076677064002843e-06,
      "loss": 0.6826,
      "step": 48590
    },
    {
      "epoch": 1.9189007778260354,
      "grad_norm": 0.6958502531051636,
      "learning_rate": 4.056935286453193e-06,
      "loss": 0.6376,
      "step": 48600
    },
    {
      "epoch": 1.9192956133770285,
      "grad_norm": 1.0721765756607056,
      "learning_rate": 4.037193508903542e-06,
      "loss": 0.6728,
      "step": 48610
    },
    {
      "epoch": 1.9196904489280215,
      "grad_norm": 0.8344100117683411,
      "learning_rate": 4.017451731353891e-06,
      "loss": 0.6899,
      "step": 48620
    },
    {
      "epoch": 1.9200852844790144,
      "grad_norm": 0.8536255359649658,
      "learning_rate": 3.997709953804241e-06,
      "loss": 0.6657,
      "step": 48630
    },
    {
      "epoch": 1.9204801200300075,
      "grad_norm": 0.7347168326377869,
      "learning_rate": 3.97796817625459e-06,
      "loss": 0.6801,
      "step": 48640
    },
    {
      "epoch": 1.9208749555810005,
      "grad_norm": 0.6740089058876038,
      "learning_rate": 3.95822639870494e-06,
      "loss": 0.7315,
      "step": 48650
    },
    {
      "epoch": 1.9212697911319934,
      "grad_norm": 0.9734135866165161,
      "learning_rate": 3.938484621155289e-06,
      "loss": 0.679,
      "step": 48660
    },
    {
      "epoch": 1.9216646266829867,
      "grad_norm": 0.9597223997116089,
      "learning_rate": 3.918742843605639e-06,
      "loss": 0.7077,
      "step": 48670
    },
    {
      "epoch": 1.9220594622339795,
      "grad_norm": 0.9522143006324768,
      "learning_rate": 3.899001066055988e-06,
      "loss": 0.6751,
      "step": 48680
    },
    {
      "epoch": 1.9224542977849726,
      "grad_norm": 0.7724425792694092,
      "learning_rate": 3.879259288506338e-06,
      "loss": 0.6777,
      "step": 48690
    },
    {
      "epoch": 1.9228491333359656,
      "grad_norm": 0.8916980028152466,
      "learning_rate": 3.859517510956687e-06,
      "loss": 0.6668,
      "step": 48700
    },
    {
      "epoch": 1.9232439688869585,
      "grad_norm": 0.7800838947296143,
      "learning_rate": 3.839775733407036e-06,
      "loss": 0.6947,
      "step": 48710
    },
    {
      "epoch": 1.9236388044379515,
      "grad_norm": 0.7472417950630188,
      "learning_rate": 3.820033955857386e-06,
      "loss": 0.7082,
      "step": 48720
    },
    {
      "epoch": 1.9240336399889446,
      "grad_norm": 0.8848564028739929,
      "learning_rate": 3.800292178307735e-06,
      "loss": 0.6832,
      "step": 48730
    },
    {
      "epoch": 1.9244284755399375,
      "grad_norm": 0.7716830372810364,
      "learning_rate": 3.7805504007580842e-06,
      "loss": 0.7206,
      "step": 48740
    },
    {
      "epoch": 1.9248233110909307,
      "grad_norm": 0.8914066553115845,
      "learning_rate": 3.7608086232084337e-06,
      "loss": 0.7234,
      "step": 48750
    },
    {
      "epoch": 1.9252181466419236,
      "grad_norm": 1.0014879703521729,
      "learning_rate": 3.7410668456587832e-06,
      "loss": 0.6921,
      "step": 48760
    },
    {
      "epoch": 1.9256129821929167,
      "grad_norm": 0.9962103366851807,
      "learning_rate": 3.7213250681091327e-06,
      "loss": 0.6755,
      "step": 48770
    },
    {
      "epoch": 1.9260078177439097,
      "grad_norm": 0.7504005432128906,
      "learning_rate": 3.701583290559482e-06,
      "loss": 0.7186,
      "step": 48780
    },
    {
      "epoch": 1.9264026532949026,
      "grad_norm": 0.8101167678833008,
      "learning_rate": 3.6818415130098317e-06,
      "loss": 0.6997,
      "step": 48790
    },
    {
      "epoch": 1.9267974888458956,
      "grad_norm": 0.750409722328186,
      "learning_rate": 3.662099735460181e-06,
      "loss": 0.684,
      "step": 48800
    },
    {
      "epoch": 1.9271923243968887,
      "grad_norm": 0.7606672644615173,
      "learning_rate": 3.6423579579105307e-06,
      "loss": 0.7205,
      "step": 48810
    },
    {
      "epoch": 1.9275871599478815,
      "grad_norm": 0.7394407987594604,
      "learning_rate": 3.6226161803608797e-06,
      "loss": 0.6994,
      "step": 48820
    },
    {
      "epoch": 1.9279819954988748,
      "grad_norm": 0.7235375642776489,
      "learning_rate": 3.6028744028112292e-06,
      "loss": 0.6838,
      "step": 48830
    },
    {
      "epoch": 1.9283768310498677,
      "grad_norm": 0.6984066367149353,
      "learning_rate": 3.5831326252615787e-06,
      "loss": 0.6852,
      "step": 48840
    },
    {
      "epoch": 1.9287716666008607,
      "grad_norm": 0.8473891615867615,
      "learning_rate": 3.5633908477119282e-06,
      "loss": 0.7094,
      "step": 48850
    },
    {
      "epoch": 1.9291665021518538,
      "grad_norm": 1.0002883672714233,
      "learning_rate": 3.5436490701622777e-06,
      "loss": 0.6593,
      "step": 48860
    },
    {
      "epoch": 1.9295613377028467,
      "grad_norm": 0.8977869153022766,
      "learning_rate": 3.523907292612627e-06,
      "loss": 0.6727,
      "step": 48870
    },
    {
      "epoch": 1.9299561732538397,
      "grad_norm": 0.7159480452537537,
      "learning_rate": 3.5041655150629767e-06,
      "loss": 0.7269,
      "step": 48880
    },
    {
      "epoch": 1.9303510088048328,
      "grad_norm": 0.7357332110404968,
      "learning_rate": 3.4844237375133258e-06,
      "loss": 0.6942,
      "step": 48890
    },
    {
      "epoch": 1.9307458443558256,
      "grad_norm": 0.7686513662338257,
      "learning_rate": 3.4646819599636752e-06,
      "loss": 0.7148,
      "step": 48900
    },
    {
      "epoch": 1.931140679906819,
      "grad_norm": 0.9795507788658142,
      "learning_rate": 3.4449401824140247e-06,
      "loss": 0.7141,
      "step": 48910
    },
    {
      "epoch": 1.9315355154578118,
      "grad_norm": 1.0883936882019043,
      "learning_rate": 3.4251984048643742e-06,
      "loss": 0.7508,
      "step": 48920
    },
    {
      "epoch": 1.9319303510088048,
      "grad_norm": 1.0980671644210815,
      "learning_rate": 3.4054566273147237e-06,
      "loss": 0.7306,
      "step": 48930
    },
    {
      "epoch": 1.932325186559798,
      "grad_norm": 0.7927227020263672,
      "learning_rate": 3.385714849765073e-06,
      "loss": 0.6902,
      "step": 48940
    },
    {
      "epoch": 1.9327200221107907,
      "grad_norm": 0.7154226303100586,
      "learning_rate": 3.3659730722154227e-06,
      "loss": 0.6526,
      "step": 48950
    },
    {
      "epoch": 1.9331148576617838,
      "grad_norm": 1.0095382928848267,
      "learning_rate": 3.346231294665772e-06,
      "loss": 0.7422,
      "step": 48960
    },
    {
      "epoch": 1.9335096932127769,
      "grad_norm": 0.6857640147209167,
      "learning_rate": 3.3264895171161213e-06,
      "loss": 0.6876,
      "step": 48970
    },
    {
      "epoch": 1.9339045287637697,
      "grad_norm": 0.7995591163635254,
      "learning_rate": 3.3067477395664707e-06,
      "loss": 0.6819,
      "step": 48980
    },
    {
      "epoch": 1.934299364314763,
      "grad_norm": 0.8869935870170593,
      "learning_rate": 3.2870059620168202e-06,
      "loss": 0.7137,
      "step": 48990
    },
    {
      "epoch": 1.9346941998657559,
      "grad_norm": 0.7544696927070618,
      "learning_rate": 3.2672641844671697e-06,
      "loss": 0.6948,
      "step": 49000
    },
    {
      "epoch": 1.9346941998657559,
      "eval_loss": 0.7532870769500732,
      "eval_runtime": 1217.8323,
      "eval_samples_per_second": 9.243,
      "eval_steps_per_second": 9.243,
      "step": 49000
    },
    {
      "epoch": 1.935089035416749,
      "grad_norm": 0.9029582738876343,
      "learning_rate": 3.2475224069175192e-06,
      "loss": 0.6892,
      "step": 49010
    },
    {
      "epoch": 1.935483870967742,
      "grad_norm": 0.8161847591400146,
      "learning_rate": 3.2277806293678687e-06,
      "loss": 0.6844,
      "step": 49020
    },
    {
      "epoch": 1.9358787065187348,
      "grad_norm": 0.6532162427902222,
      "learning_rate": 3.208038851818218e-06,
      "loss": 0.6863,
      "step": 49030
    },
    {
      "epoch": 1.936273542069728,
      "grad_norm": 0.7369716167449951,
      "learning_rate": 3.1882970742685677e-06,
      "loss": 0.6972,
      "step": 49040
    },
    {
      "epoch": 1.936668377620721,
      "grad_norm": 0.7231096029281616,
      "learning_rate": 3.1685552967189168e-06,
      "loss": 0.6811,
      "step": 49050
    },
    {
      "epoch": 1.9370632131717138,
      "grad_norm": 0.9429194331169128,
      "learning_rate": 3.1488135191692662e-06,
      "loss": 0.6858,
      "step": 49060
    },
    {
      "epoch": 1.937458048722707,
      "grad_norm": 0.9199492931365967,
      "learning_rate": 3.1290717416196157e-06,
      "loss": 0.7109,
      "step": 49070
    },
    {
      "epoch": 1.9378528842737,
      "grad_norm": 0.6877577304840088,
      "learning_rate": 3.109329964069965e-06,
      "loss": 0.6893,
      "step": 49080
    },
    {
      "epoch": 1.938247719824693,
      "grad_norm": 1.1094940900802612,
      "learning_rate": 3.0895881865203143e-06,
      "loss": 0.7395,
      "step": 49090
    },
    {
      "epoch": 1.938642555375686,
      "grad_norm": 0.7730705142021179,
      "learning_rate": 3.0698464089706638e-06,
      "loss": 0.6738,
      "step": 49100
    },
    {
      "epoch": 1.939037390926679,
      "grad_norm": 0.7347548007965088,
      "learning_rate": 3.0501046314210133e-06,
      "loss": 0.683,
      "step": 49110
    },
    {
      "epoch": 1.939432226477672,
      "grad_norm": 1.3548552989959717,
      "learning_rate": 3.0303628538713628e-06,
      "loss": 0.6858,
      "step": 49120
    },
    {
      "epoch": 1.939827062028665,
      "grad_norm": 0.7892659902572632,
      "learning_rate": 3.0106210763217123e-06,
      "loss": 0.6899,
      "step": 49130
    },
    {
      "epoch": 1.940221897579658,
      "grad_norm": 0.7490978240966797,
      "learning_rate": 2.9908792987720613e-06,
      "loss": 0.7172,
      "step": 49140
    },
    {
      "epoch": 1.9406167331306512,
      "grad_norm": 0.7187089323997498,
      "learning_rate": 2.971137521222411e-06,
      "loss": 0.6567,
      "step": 49150
    },
    {
      "epoch": 1.941011568681644,
      "grad_norm": 0.8460789918899536,
      "learning_rate": 2.9513957436727603e-06,
      "loss": 0.7098,
      "step": 49160
    },
    {
      "epoch": 1.941406404232637,
      "grad_norm": 0.7629234790802002,
      "learning_rate": 2.93165396612311e-06,
      "loss": 0.6834,
      "step": 49170
    },
    {
      "epoch": 1.9418012397836302,
      "grad_norm": 0.9275245070457458,
      "learning_rate": 2.9119121885734593e-06,
      "loss": 0.6987,
      "step": 49180
    },
    {
      "epoch": 1.942196075334623,
      "grad_norm": 0.8560448288917542,
      "learning_rate": 2.8921704110238088e-06,
      "loss": 0.7021,
      "step": 49190
    },
    {
      "epoch": 1.9425909108856163,
      "grad_norm": 1.0664756298065186,
      "learning_rate": 2.8724286334741583e-06,
      "loss": 0.7111,
      "step": 49200
    },
    {
      "epoch": 1.9429857464366092,
      "grad_norm": 0.9471900463104248,
      "learning_rate": 2.8526868559245078e-06,
      "loss": 0.7034,
      "step": 49210
    },
    {
      "epoch": 1.9433805819876022,
      "grad_norm": 0.633551299571991,
      "learning_rate": 2.832945078374857e-06,
      "loss": 0.7065,
      "step": 49220
    },
    {
      "epoch": 1.9437754175385953,
      "grad_norm": 0.8633872866630554,
      "learning_rate": 2.8132033008252063e-06,
      "loss": 0.6945,
      "step": 49230
    },
    {
      "epoch": 1.9441702530895881,
      "grad_norm": 0.961718738079071,
      "learning_rate": 2.793461523275556e-06,
      "loss": 0.6641,
      "step": 49240
    },
    {
      "epoch": 1.9445650886405812,
      "grad_norm": 0.7965267896652222,
      "learning_rate": 2.7737197457259053e-06,
      "loss": 0.7029,
      "step": 49250
    },
    {
      "epoch": 1.9449599241915743,
      "grad_norm": 0.7480447888374329,
      "learning_rate": 2.7539779681762548e-06,
      "loss": 0.7221,
      "step": 49260
    },
    {
      "epoch": 1.945354759742567,
      "grad_norm": 0.9671467542648315,
      "learning_rate": 2.7342361906266043e-06,
      "loss": 0.6949,
      "step": 49270
    },
    {
      "epoch": 1.9457495952935604,
      "grad_norm": 0.7548119425773621,
      "learning_rate": 2.7144944130769538e-06,
      "loss": 0.7123,
      "step": 49280
    },
    {
      "epoch": 1.9461444308445532,
      "grad_norm": 0.6577569842338562,
      "learning_rate": 2.6947526355273033e-06,
      "loss": 0.6934,
      "step": 49290
    },
    {
      "epoch": 1.9465392663955463,
      "grad_norm": 1.112991452217102,
      "learning_rate": 2.6750108579776523e-06,
      "loss": 0.6733,
      "step": 49300
    },
    {
      "epoch": 1.9469341019465394,
      "grad_norm": 0.7419365048408508,
      "learning_rate": 2.655269080428002e-06,
      "loss": 0.6859,
      "step": 49310
    },
    {
      "epoch": 1.9473289374975322,
      "grad_norm": 0.6819169521331787,
      "learning_rate": 2.6355273028783513e-06,
      "loss": 0.6769,
      "step": 49320
    },
    {
      "epoch": 1.9477237730485253,
      "grad_norm": 1.0058488845825195,
      "learning_rate": 2.615785525328701e-06,
      "loss": 0.6935,
      "step": 49330
    },
    {
      "epoch": 1.9481186085995184,
      "grad_norm": 0.6033874154090881,
      "learning_rate": 2.5960437477790503e-06,
      "loss": 0.7041,
      "step": 49340
    },
    {
      "epoch": 1.9485134441505112,
      "grad_norm": 0.8994418382644653,
      "learning_rate": 2.5763019702293998e-06,
      "loss": 0.7456,
      "step": 49350
    },
    {
      "epoch": 1.9489082797015045,
      "grad_norm": 0.6468858122825623,
      "learning_rate": 2.5565601926797493e-06,
      "loss": 0.6608,
      "step": 49360
    },
    {
      "epoch": 1.9493031152524973,
      "grad_norm": 0.838232159614563,
      "learning_rate": 2.5368184151300983e-06,
      "loss": 0.7153,
      "step": 49370
    },
    {
      "epoch": 1.9496979508034904,
      "grad_norm": 1.006052017211914,
      "learning_rate": 2.517076637580448e-06,
      "loss": 0.6768,
      "step": 49380
    },
    {
      "epoch": 1.9500927863544835,
      "grad_norm": 0.8735328316688538,
      "learning_rate": 2.4973348600307973e-06,
      "loss": 0.6982,
      "step": 49390
    },
    {
      "epoch": 1.9504876219054763,
      "grad_norm": 0.8243538737297058,
      "learning_rate": 2.477593082481147e-06,
      "loss": 0.6719,
      "step": 49400
    },
    {
      "epoch": 1.9508824574564694,
      "grad_norm": 0.8742908835411072,
      "learning_rate": 2.4578513049314963e-06,
      "loss": 0.7198,
      "step": 49410
    },
    {
      "epoch": 1.9512772930074624,
      "grad_norm": 0.9771730303764343,
      "learning_rate": 2.4381095273818458e-06,
      "loss": 0.6896,
      "step": 49420
    },
    {
      "epoch": 1.9516721285584553,
      "grad_norm": 0.8952116966247559,
      "learning_rate": 2.4183677498321953e-06,
      "loss": 0.6823,
      "step": 49430
    },
    {
      "epoch": 1.9520669641094486,
      "grad_norm": 0.8449310064315796,
      "learning_rate": 2.3986259722825448e-06,
      "loss": 0.692,
      "step": 49440
    },
    {
      "epoch": 1.9524617996604414,
      "grad_norm": 0.8000101447105408,
      "learning_rate": 2.378884194732894e-06,
      "loss": 0.6818,
      "step": 49450
    },
    {
      "epoch": 1.9528566352114345,
      "grad_norm": 0.7640025615692139,
      "learning_rate": 2.3591424171832433e-06,
      "loss": 0.7098,
      "step": 49460
    },
    {
      "epoch": 1.9532514707624276,
      "grad_norm": 0.7669826745986938,
      "learning_rate": 2.339400639633593e-06,
      "loss": 0.7303,
      "step": 49470
    },
    {
      "epoch": 1.9536463063134204,
      "grad_norm": 0.8828250169754028,
      "learning_rate": 2.319658862083942e-06,
      "loss": 0.6665,
      "step": 49480
    },
    {
      "epoch": 1.9540411418644135,
      "grad_norm": 0.7817801237106323,
      "learning_rate": 2.2999170845342914e-06,
      "loss": 0.6615,
      "step": 49490
    },
    {
      "epoch": 1.9544359774154065,
      "grad_norm": 0.7741534113883972,
      "learning_rate": 2.280175306984641e-06,
      "loss": 0.6882,
      "step": 49500
    },
    {
      "epoch": 1.9548308129663994,
      "grad_norm": 0.780171275138855,
      "learning_rate": 2.2604335294349903e-06,
      "loss": 0.7251,
      "step": 49510
    },
    {
      "epoch": 1.9552256485173927,
      "grad_norm": 0.7002862691879272,
      "learning_rate": 2.24069175188534e-06,
      "loss": 0.6956,
      "step": 49520
    },
    {
      "epoch": 1.9556204840683855,
      "grad_norm": 0.9781205058097839,
      "learning_rate": 2.2209499743356893e-06,
      "loss": 0.7043,
      "step": 49530
    },
    {
      "epoch": 1.9560153196193786,
      "grad_norm": 0.7979531288146973,
      "learning_rate": 2.2012081967860384e-06,
      "loss": 0.7409,
      "step": 49540
    },
    {
      "epoch": 1.9564101551703716,
      "grad_norm": 0.7638075351715088,
      "learning_rate": 2.181466419236388e-06,
      "loss": 0.7266,
      "step": 49550
    },
    {
      "epoch": 1.9568049907213645,
      "grad_norm": 1.0690054893493652,
      "learning_rate": 2.1617246416867374e-06,
      "loss": 0.6661,
      "step": 49560
    },
    {
      "epoch": 1.9571998262723576,
      "grad_norm": 0.6098462343215942,
      "learning_rate": 2.141982864137087e-06,
      "loss": 0.6963,
      "step": 49570
    },
    {
      "epoch": 1.9575946618233506,
      "grad_norm": 1.060288906097412,
      "learning_rate": 2.1222410865874364e-06,
      "loss": 0.6909,
      "step": 49580
    },
    {
      "epoch": 1.9579894973743435,
      "grad_norm": 0.8368332982063293,
      "learning_rate": 2.102499309037786e-06,
      "loss": 0.7041,
      "step": 49590
    },
    {
      "epoch": 1.9583843329253368,
      "grad_norm": 0.8770245313644409,
      "learning_rate": 2.0827575314881353e-06,
      "loss": 0.7529,
      "step": 49600
    },
    {
      "epoch": 1.9587791684763296,
      "grad_norm": 0.7707680463790894,
      "learning_rate": 2.063015753938485e-06,
      "loss": 0.7029,
      "step": 49610
    },
    {
      "epoch": 1.9591740040273227,
      "grad_norm": 1.262336254119873,
      "learning_rate": 2.043273976388834e-06,
      "loss": 0.7249,
      "step": 49620
    },
    {
      "epoch": 1.9595688395783157,
      "grad_norm": 1.0180758237838745,
      "learning_rate": 2.0235321988391834e-06,
      "loss": 0.6967,
      "step": 49630
    },
    {
      "epoch": 1.9599636751293086,
      "grad_norm": 0.7207696437835693,
      "learning_rate": 2.003790421289533e-06,
      "loss": 0.633,
      "step": 49640
    },
    {
      "epoch": 1.9603585106803016,
      "grad_norm": 0.803379237651825,
      "learning_rate": 1.9840486437398824e-06,
      "loss": 0.6972,
      "step": 49650
    },
    {
      "epoch": 1.9607533462312947,
      "grad_norm": 0.8514224290847778,
      "learning_rate": 1.964306866190232e-06,
      "loss": 0.6699,
      "step": 49660
    },
    {
      "epoch": 1.9611481817822876,
      "grad_norm": 0.7910423278808594,
      "learning_rate": 1.9445650886405813e-06,
      "loss": 0.7032,
      "step": 49670
    },
    {
      "epoch": 1.9615430173332808,
      "grad_norm": 0.8739110231399536,
      "learning_rate": 1.924823311090931e-06,
      "loss": 0.6795,
      "step": 49680
    },
    {
      "epoch": 1.9619378528842737,
      "grad_norm": 0.9325962662696838,
      "learning_rate": 1.9050815335412801e-06,
      "loss": 0.6854,
      "step": 49690
    },
    {
      "epoch": 1.9623326884352668,
      "grad_norm": 0.757957398891449,
      "learning_rate": 1.8853397559916296e-06,
      "loss": 0.6914,
      "step": 49700
    },
    {
      "epoch": 1.9627275239862598,
      "grad_norm": 1.0368609428405762,
      "learning_rate": 1.865597978441979e-06,
      "loss": 0.7469,
      "step": 49710
    },
    {
      "epoch": 1.9631223595372527,
      "grad_norm": 0.9136457443237305,
      "learning_rate": 1.8458562008923284e-06,
      "loss": 0.6653,
      "step": 49720
    },
    {
      "epoch": 1.9635171950882457,
      "grad_norm": 0.7584401369094849,
      "learning_rate": 1.8261144233426779e-06,
      "loss": 0.6822,
      "step": 49730
    },
    {
      "epoch": 1.9639120306392388,
      "grad_norm": 0.7021849751472473,
      "learning_rate": 1.8063726457930274e-06,
      "loss": 0.7048,
      "step": 49740
    },
    {
      "epoch": 1.9643068661902316,
      "grad_norm": 0.6814867258071899,
      "learning_rate": 1.7866308682433768e-06,
      "loss": 0.7014,
      "step": 49750
    },
    {
      "epoch": 1.964701701741225,
      "grad_norm": 0.834445595741272,
      "learning_rate": 1.7668890906937261e-06,
      "loss": 0.7071,
      "step": 49760
    },
    {
      "epoch": 1.9650965372922178,
      "grad_norm": 0.7931252717971802,
      "learning_rate": 1.7471473131440756e-06,
      "loss": 0.726,
      "step": 49770
    },
    {
      "epoch": 1.9654913728432108,
      "grad_norm": 0.7394412159919739,
      "learning_rate": 1.727405535594425e-06,
      "loss": 0.7095,
      "step": 49780
    },
    {
      "epoch": 1.965886208394204,
      "grad_norm": 0.6989794969558716,
      "learning_rate": 1.7076637580447746e-06,
      "loss": 0.6947,
      "step": 49790
    },
    {
      "epoch": 1.9662810439451968,
      "grad_norm": 1.078932285308838,
      "learning_rate": 1.6879219804951239e-06,
      "loss": 0.7126,
      "step": 49800
    },
    {
      "epoch": 1.9666758794961898,
      "grad_norm": 0.851528525352478,
      "learning_rate": 1.6681802029454734e-06,
      "loss": 0.663,
      "step": 49810
    },
    {
      "epoch": 1.967070715047183,
      "grad_norm": 0.7667286396026611,
      "learning_rate": 1.6484384253958229e-06,
      "loss": 0.6635,
      "step": 49820
    },
    {
      "epoch": 1.9674655505981757,
      "grad_norm": 0.5901515483856201,
      "learning_rate": 1.6286966478461723e-06,
      "loss": 0.6543,
      "step": 49830
    },
    {
      "epoch": 1.967860386149169,
      "grad_norm": 0.8282231688499451,
      "learning_rate": 1.6089548702965216e-06,
      "loss": 0.6902,
      "step": 49840
    },
    {
      "epoch": 1.9682552217001619,
      "grad_norm": 0.8621819615364075,
      "learning_rate": 1.5892130927468711e-06,
      "loss": 0.7094,
      "step": 49850
    },
    {
      "epoch": 1.968650057251155,
      "grad_norm": 0.7416271567344666,
      "learning_rate": 1.5694713151972206e-06,
      "loss": 0.6772,
      "step": 49860
    },
    {
      "epoch": 1.969044892802148,
      "grad_norm": 0.8436712026596069,
      "learning_rate": 1.5497295376475699e-06,
      "loss": 0.7318,
      "step": 49870
    },
    {
      "epoch": 1.9694397283531409,
      "grad_norm": 0.7191198468208313,
      "learning_rate": 1.5299877600979192e-06,
      "loss": 0.7224,
      "step": 49880
    },
    {
      "epoch": 1.969834563904134,
      "grad_norm": 0.901800811290741,
      "learning_rate": 1.5102459825482686e-06,
      "loss": 0.6787,
      "step": 49890
    },
    {
      "epoch": 1.970229399455127,
      "grad_norm": 0.8483249545097351,
      "learning_rate": 1.4905042049986181e-06,
      "loss": 0.7378,
      "step": 49900
    },
    {
      "epoch": 1.9706242350061198,
      "grad_norm": 0.9293670058250427,
      "learning_rate": 1.4707624274489676e-06,
      "loss": 0.7025,
      "step": 49910
    },
    {
      "epoch": 1.9710190705571131,
      "grad_norm": 0.9129623174667358,
      "learning_rate": 1.451020649899317e-06,
      "loss": 0.6882,
      "step": 49920
    },
    {
      "epoch": 1.971413906108106,
      "grad_norm": 0.9165840744972229,
      "learning_rate": 1.4312788723496664e-06,
      "loss": 0.6787,
      "step": 49930
    },
    {
      "epoch": 1.971808741659099,
      "grad_norm": 0.6526946425437927,
      "learning_rate": 1.4115370948000159e-06,
      "loss": 0.671,
      "step": 49940
    },
    {
      "epoch": 1.972203577210092,
      "grad_norm": 0.845241367816925,
      "learning_rate": 1.3917953172503654e-06,
      "loss": 0.6623,
      "step": 49950
    },
    {
      "epoch": 1.972598412761085,
      "grad_norm": 0.7588886618614197,
      "learning_rate": 1.3720535397007147e-06,
      "loss": 0.6898,
      "step": 49960
    },
    {
      "epoch": 1.972993248312078,
      "grad_norm": 0.797325074672699,
      "learning_rate": 1.3523117621510641e-06,
      "loss": 0.7161,
      "step": 49970
    },
    {
      "epoch": 1.973388083863071,
      "grad_norm": 0.7561139464378357,
      "learning_rate": 1.3325699846014136e-06,
      "loss": 0.6988,
      "step": 49980
    },
    {
      "epoch": 1.973782919414064,
      "grad_norm": 0.8219815492630005,
      "learning_rate": 1.3128282070517631e-06,
      "loss": 0.71,
      "step": 49990
    },
    {
      "epoch": 1.9741777549650572,
      "grad_norm": 0.8616400361061096,
      "learning_rate": 1.2930864295021124e-06,
      "loss": 0.6618,
      "step": 50000
    },
    {
      "epoch": 1.9741777549650572,
      "eval_loss": 0.7528449296951294,
      "eval_runtime": 1214.9909,
      "eval_samples_per_second": 9.265,
      "eval_steps_per_second": 9.265,
      "step": 50000
    },
    {
      "epoch": 1.97457259051605,
      "grad_norm": 0.7711885571479797,
      "learning_rate": 1.273344651952462e-06,
      "loss": 0.6644,
      "step": 50010
    },
    {
      "epoch": 1.9749674260670431,
      "grad_norm": 0.6493716835975647,
      "learning_rate": 1.2536028744028114e-06,
      "loss": 0.7249,
      "step": 50020
    },
    {
      "epoch": 1.9753622616180362,
      "grad_norm": 0.912082850933075,
      "learning_rate": 1.2338610968531609e-06,
      "loss": 0.6893,
      "step": 50030
    },
    {
      "epoch": 1.975757097169029,
      "grad_norm": 0.7904784679412842,
      "learning_rate": 1.2141193193035102e-06,
      "loss": 0.7181,
      "step": 50040
    },
    {
      "epoch": 1.976151932720022,
      "grad_norm": 0.7455959320068359,
      "learning_rate": 1.1943775417538596e-06,
      "loss": 0.6433,
      "step": 50050
    },
    {
      "epoch": 1.9765467682710152,
      "grad_norm": 0.9415998458862305,
      "learning_rate": 1.1746357642042091e-06,
      "loss": 0.678,
      "step": 50060
    },
    {
      "epoch": 1.976941603822008,
      "grad_norm": 0.6895598769187927,
      "learning_rate": 1.1548939866545584e-06,
      "loss": 0.6737,
      "step": 50070
    },
    {
      "epoch": 1.9773364393730013,
      "grad_norm": 0.7989007830619812,
      "learning_rate": 1.1351522091049077e-06,
      "loss": 0.7296,
      "step": 50080
    },
    {
      "epoch": 1.9777312749239941,
      "grad_norm": 0.8294663429260254,
      "learning_rate": 1.1154104315552572e-06,
      "loss": 0.6711,
      "step": 50090
    },
    {
      "epoch": 1.9781261104749872,
      "grad_norm": 0.8062527775764465,
      "learning_rate": 1.0956686540056067e-06,
      "loss": 0.7228,
      "step": 50100
    },
    {
      "epoch": 1.9785209460259803,
      "grad_norm": 0.8339361548423767,
      "learning_rate": 1.0759268764559562e-06,
      "loss": 0.6738,
      "step": 50110
    },
    {
      "epoch": 1.9789157815769731,
      "grad_norm": 0.7543624639511108,
      "learning_rate": 1.0561850989063054e-06,
      "loss": 0.705,
      "step": 50120
    },
    {
      "epoch": 1.9793106171279662,
      "grad_norm": 0.7514467835426331,
      "learning_rate": 1.036443321356655e-06,
      "loss": 0.6912,
      "step": 50130
    },
    {
      "epoch": 1.9797054526789593,
      "grad_norm": 0.8855422735214233,
      "learning_rate": 1.0167015438070044e-06,
      "loss": 0.7159,
      "step": 50140
    },
    {
      "epoch": 1.980100288229952,
      "grad_norm": 0.8215518593788147,
      "learning_rate": 9.96959766257354e-07,
      "loss": 0.6626,
      "step": 50150
    },
    {
      "epoch": 1.9804951237809454,
      "grad_norm": 0.7591889500617981,
      "learning_rate": 9.772179887077032e-07,
      "loss": 0.6933,
      "step": 50160
    },
    {
      "epoch": 1.9808899593319382,
      "grad_norm": 0.8874889612197876,
      "learning_rate": 9.574762111580527e-07,
      "loss": 0.6909,
      "step": 50170
    },
    {
      "epoch": 1.9812847948829313,
      "grad_norm": 0.76964271068573,
      "learning_rate": 9.377344336084022e-07,
      "loss": 0.7131,
      "step": 50180
    },
    {
      "epoch": 1.9816796304339244,
      "grad_norm": 0.8053301572799683,
      "learning_rate": 9.179926560587516e-07,
      "loss": 0.7124,
      "step": 50190
    },
    {
      "epoch": 1.9820744659849172,
      "grad_norm": 0.7532334327697754,
      "learning_rate": 8.98250878509101e-07,
      "loss": 0.6539,
      "step": 50200
    },
    {
      "epoch": 1.9824693015359103,
      "grad_norm": 0.6380783915519714,
      "learning_rate": 8.785091009594504e-07,
      "loss": 0.6951,
      "step": 50210
    },
    {
      "epoch": 1.9828641370869033,
      "grad_norm": 1.0828461647033691,
      "learning_rate": 8.587673234097999e-07,
      "loss": 0.7086,
      "step": 50220
    },
    {
      "epoch": 1.9832589726378962,
      "grad_norm": 0.8435251712799072,
      "learning_rate": 8.390255458601493e-07,
      "loss": 0.6953,
      "step": 50230
    },
    {
      "epoch": 1.9836538081888895,
      "grad_norm": 1.001004695892334,
      "learning_rate": 8.192837683104988e-07,
      "loss": 0.6674,
      "step": 50240
    },
    {
      "epoch": 1.9840486437398823,
      "grad_norm": 0.8145090937614441,
      "learning_rate": 7.995419907608482e-07,
      "loss": 0.7284,
      "step": 50250
    },
    {
      "epoch": 1.9844434792908754,
      "grad_norm": 0.7647377848625183,
      "learning_rate": 7.798002132111976e-07,
      "loss": 0.6655,
      "step": 50260
    },
    {
      "epoch": 1.9848383148418685,
      "grad_norm": 0.7845481038093567,
      "learning_rate": 7.60058435661547e-07,
      "loss": 0.6638,
      "step": 50270
    },
    {
      "epoch": 1.9852331503928613,
      "grad_norm": 1.1314611434936523,
      "learning_rate": 7.403166581118964e-07,
      "loss": 0.7063,
      "step": 50280
    },
    {
      "epoch": 1.9856279859438544,
      "grad_norm": 0.7665324211120605,
      "learning_rate": 7.205748805622458e-07,
      "loss": 0.7231,
      "step": 50290
    },
    {
      "epoch": 1.9860228214948474,
      "grad_norm": 0.9262526035308838,
      "learning_rate": 7.008331030125953e-07,
      "loss": 0.7189,
      "step": 50300
    },
    {
      "epoch": 1.9864176570458403,
      "grad_norm": 1.1071890592575073,
      "learning_rate": 6.810913254629447e-07,
      "loss": 0.6735,
      "step": 50310
    },
    {
      "epoch": 1.9868124925968336,
      "grad_norm": 0.6667319536209106,
      "learning_rate": 6.613495479132942e-07,
      "loss": 0.6913,
      "step": 50320
    },
    {
      "epoch": 1.9872073281478264,
      "grad_norm": 0.7509380578994751,
      "learning_rate": 6.416077703636436e-07,
      "loss": 0.7119,
      "step": 50330
    },
    {
      "epoch": 1.9876021636988195,
      "grad_norm": 0.8366678357124329,
      "learning_rate": 6.218659928139931e-07,
      "loss": 0.7074,
      "step": 50340
    },
    {
      "epoch": 1.9879969992498125,
      "grad_norm": 1.0169451236724854,
      "learning_rate": 6.021242152643425e-07,
      "loss": 0.6698,
      "step": 50350
    },
    {
      "epoch": 1.9883918348008054,
      "grad_norm": 0.8618068695068359,
      "learning_rate": 5.823824377146918e-07,
      "loss": 0.6994,
      "step": 50360
    },
    {
      "epoch": 1.9887866703517985,
      "grad_norm": 0.5752989649772644,
      "learning_rate": 5.626406601650412e-07,
      "loss": 0.6653,
      "step": 50370
    },
    {
      "epoch": 1.9891815059027915,
      "grad_norm": 0.9422464370727539,
      "learning_rate": 5.428988826153907e-07,
      "loss": 0.6781,
      "step": 50380
    },
    {
      "epoch": 1.9895763414537844,
      "grad_norm": 0.8983109593391418,
      "learning_rate": 5.231571050657401e-07,
      "loss": 0.6725,
      "step": 50390
    },
    {
      "epoch": 1.9899711770047777,
      "grad_norm": 0.7554910778999329,
      "learning_rate": 5.034153275160896e-07,
      "loss": 0.6718,
      "step": 50400
    },
    {
      "epoch": 1.9903660125557705,
      "grad_norm": 0.6920779347419739,
      "learning_rate": 4.83673549966439e-07,
      "loss": 0.6705,
      "step": 50410
    },
    {
      "epoch": 1.9907608481067636,
      "grad_norm": 0.8243977427482605,
      "learning_rate": 4.639317724167884e-07,
      "loss": 0.684,
      "step": 50420
    },
    {
      "epoch": 1.9911556836577566,
      "grad_norm": 1.01282799243927,
      "learning_rate": 4.4418999486713784e-07,
      "loss": 0.7322,
      "step": 50430
    },
    {
      "epoch": 1.9915505192087495,
      "grad_norm": 0.8589758276939392,
      "learning_rate": 4.244482173174873e-07,
      "loss": 0.7218,
      "step": 50440
    },
    {
      "epoch": 1.9919453547597425,
      "grad_norm": 0.76494300365448,
      "learning_rate": 4.047064397678367e-07,
      "loss": 0.681,
      "step": 50450
    },
    {
      "epoch": 1.9923401903107356,
      "grad_norm": 0.7689602375030518,
      "learning_rate": 3.8496466221818616e-07,
      "loss": 0.688,
      "step": 50460
    },
    {
      "epoch": 1.9927350258617285,
      "grad_norm": 0.8553540110588074,
      "learning_rate": 3.6522288466853554e-07,
      "loss": 0.6864,
      "step": 50470
    },
    {
      "epoch": 1.9931298614127217,
      "grad_norm": 0.8335593938827515,
      "learning_rate": 3.45481107118885e-07,
      "loss": 0.7334,
      "step": 50480
    },
    {
      "epoch": 1.9935246969637146,
      "grad_norm": 0.6737549901008606,
      "learning_rate": 3.257393295692344e-07,
      "loss": 0.6948,
      "step": 50490
    },
    {
      "epoch": 1.9939195325147077,
      "grad_norm": 0.7818537950515747,
      "learning_rate": 3.0599755201958385e-07,
      "loss": 0.7294,
      "step": 50500
    },
    {
      "epoch": 1.9943143680657007,
      "grad_norm": 0.7827877402305603,
      "learning_rate": 2.862557744699333e-07,
      "loss": 0.7261,
      "step": 50510
    },
    {
      "epoch": 1.9947092036166936,
      "grad_norm": 0.9176741242408752,
      "learning_rate": 2.6651399692028273e-07,
      "loss": 0.7091,
      "step": 50520
    },
    {
      "epoch": 1.9951040391676866,
      "grad_norm": 0.7527133822441101,
      "learning_rate": 2.4677221937063216e-07,
      "loss": 0.692,
      "step": 50530
    },
    {
      "epoch": 1.9954988747186797,
      "grad_norm": 0.667702853679657,
      "learning_rate": 2.2703044182098158e-07,
      "loss": 0.6667,
      "step": 50540
    },
    {
      "epoch": 1.9958937102696725,
      "grad_norm": 0.7034487724304199,
      "learning_rate": 2.07288664271331e-07,
      "loss": 0.7225,
      "step": 50550
    },
    {
      "epoch": 1.9962885458206658,
      "grad_norm": 0.7183810472488403,
      "learning_rate": 1.8754688672168042e-07,
      "loss": 0.7242,
      "step": 50560
    },
    {
      "epoch": 1.9966833813716587,
      "grad_norm": 0.7524063587188721,
      "learning_rate": 1.6780510917202986e-07,
      "loss": 0.6863,
      "step": 50570
    },
    {
      "epoch": 1.9970782169226517,
      "grad_norm": 0.9110432267189026,
      "learning_rate": 1.480633316223793e-07,
      "loss": 0.6621,
      "step": 50580
    },
    {
      "epoch": 1.9974730524736448,
      "grad_norm": 0.8534930944442749,
      "learning_rate": 1.283215540727287e-07,
      "loss": 0.7026,
      "step": 50590
    },
    {
      "epoch": 1.9978678880246377,
      "grad_norm": 0.7719559669494629,
      "learning_rate": 1.0857977652307815e-07,
      "loss": 0.6947,
      "step": 50600
    },
    {
      "epoch": 1.9982627235756307,
      "grad_norm": 0.6515035629272461,
      "learning_rate": 8.883799897342757e-08,
      "loss": 0.6912,
      "step": 50610
    },
    {
      "epoch": 1.9986575591266238,
      "grad_norm": 0.6320328116416931,
      "learning_rate": 6.9096221423777e-08,
      "loss": 0.7017,
      "step": 50620
    },
    {
      "epoch": 1.9990523946776166,
      "grad_norm": 0.7488444447517395,
      "learning_rate": 4.935444387412643e-08,
      "loss": 0.662,
      "step": 50630
    },
    {
      "epoch": 1.99944723022861,
      "grad_norm": 0.9838852286338806,
      "learning_rate": 2.9612666324475857e-08,
      "loss": 0.6636,
      "step": 50640
    },
    {
      "epoch": 1.9998420657796028,
      "grad_norm": 0.8728947639465332,
      "learning_rate": 9.870888774825286e-09,
      "loss": 0.706,
      "step": 50650
    }
  ],
  "logging_steps": 10,
  "max_steps": 50654,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 400,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 6.062389714782536e+18,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
