import json
import os
import numpy as np
from typing import List, Dict, Any
import faiss
import pickle
from pathlib import Path
import re
import torch

# PDFå¤„ç†
import PyPDF2
from io import BytesIO

# æ¨¡å‹ç›¸å…³
from openai import OpenAI
from modelscope import AutoModelForCausalLM, AutoTokenizer
from sentence_transformers import CrossEncoder

# æ–‡æœ¬å¤„ç†
import jieba
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity


class RAGSystem:
    def __init__(self, 
                 chunk_size=500,
                 chunk_overlap=50,
                 top_k=10,
                 rerank_top_k=3):
        
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.top_k = top_k
        self.rerank_top_k = rerank_top_k
        
        # å›ºå®šä½¿ç”¨Qwen3-14Bæ¨¡å‹å’Œç¼“å­˜ç›®å½•
        model_name = "Qwen/Qwen3-14B"
        model_cache_dir = "autodl-tmp/qwen"
        
        # åˆ›å»ºæ¨¡å‹ç¼“å­˜ç›®å½•
        os.makedirs(model_cache_dir, exist_ok=True)
        print(f"æ¨¡å‹ç¼“å­˜ç›®å½•: {model_cache_dir}")
        
        # åˆå§‹åŒ–æœ¬åœ°LLM
        print("æ­£åœ¨åŠ è½½æœ¬åœ°LLMæ¨¡å‹...")
        print(f"ä½¿ç”¨æ¨¡å‹: {model_name}")
        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                cache_dir=model_cache_dir,  # è®¾ç½®ç¼“å­˜ç›®å½•
                local_files_only=False  # å…è®¸ä»ç½‘ç»œä¸‹è½½
            )
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                cache_dir=model_cache_dir,  # è®¾ç½®ç¼“å­˜ç›®å½•
                torch_dtype="auto",
                device_map="auto",
                low_cpu_mem_usage=True,  # æ·»åŠ ä½å†…å­˜ä½¿ç”¨é€‰é¡¹
                local_files_only=False  # å…è®¸ä»ç½‘ç»œä¸‹è½½
            )
            print("æœ¬åœ°LLMæ¨¡å‹åŠ è½½å®Œæˆ")
        except Exception as e:
            print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–æ¨¡å‹åç§°")
            raise e
        
        # åˆå§‹åŒ–embeddingå®¢æˆ·ç«¯
        self.embedding_client = OpenAI(
            api_key=os.getenv("DASHSCOPE_API_KEY"),
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
        )
        self.embedding_model = "text-embedding-v4"
        
        # åˆå§‹åŒ–rerankæ¨¡å‹
        print("æ­£åœ¨åŠ è½½é‡æ’åºæ¨¡å‹...")
        try:
            self.reranker = CrossEncoder("BAAI/bge-reranker-base")
            print("é‡æ’åºæ¨¡å‹åŠ è½½å®Œæˆ")
        except:
            print("é‡æ’åºæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ä½œä¸ºå¤‡é€‰")
            self.reranker = None
        
        # å‘é‡åº“ç›¸å…³
        self.index = None
        self.chunks = []
        self.chunk_embeddings = []
    
    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """ä»PDFæ–‡ä»¶ä¸­æå–æ–‡æœ¬"""
        text = ""
        try:
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                for page in pdf_reader.pages:
                    text += page.extract_text() + "\n"
        except Exception as e:
            print(f"è¯»å–PDFæ–‡ä»¶ {pdf_path} æ—¶å‡ºé”™: {e}")
        return text
    
    def chunk_text(self, text: str) -> List[str]:
        """å°†æ–‡æœ¬åˆ†å‰²æˆå—"""
        chunks = []
        start = 0
        text_length = len(text)
        
        while start < text_length:
            end = start + self.chunk_size
            if end > text_length:
                end = text_length
            
            chunk = text[start:end]
            chunks.append(chunk)
            
            start = end - self.chunk_overlap
            if start >= text_length:
                break
        
        return chunks
    
    def get_embedding(self, text: str) -> List[float]:
        """è·å–æ–‡æœ¬çš„embedding"""
        try:
            response = self.embedding_client.embeddings.create(
                model=self.embedding_model,
                input=text
            )
            return response.data[0].embedding
        except Exception as e:
            print(f"è·å–embeddingå¤±è´¥: {e}")
            return []
    
    def build_knowledge_base(self, pdf_folder: str):
        """æ„å»ºçŸ¥è¯†åº“"""
        print(f"æ­£åœ¨æ„å»ºçŸ¥è¯†åº“ï¼ŒPDFæ–‡ä»¶å¤¹: {pdf_folder}")
        
        # æ£€æŸ¥PDFæ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨
        if not os.path.exists(pdf_folder):
            print(f"PDFæ–‡ä»¶å¤¹ä¸å­˜åœ¨: {pdf_folder}")
            return
        
        # è·å–æ‰€æœ‰PDFæ–‡ä»¶
        pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith('.pdf')]
        if not pdf_files:
            print(f"åœ¨ {pdf_folder} ä¸­æœªæ‰¾åˆ°PDFæ–‡ä»¶")
            return
        
        print(f"æ‰¾åˆ° {len(pdf_files)} ä¸ªPDFæ–‡ä»¶")
        
        # å¤„ç†æ¯ä¸ªPDFæ–‡ä»¶
        for pdf_file in pdf_files:
            pdf_path = os.path.join(pdf_folder, pdf_file)
            print(f"æ­£åœ¨å¤„ç†: {pdf_file}")
            
            # æå–æ–‡æœ¬
            text = self.extract_text_from_pdf(pdf_path)
            if not text.strip():
                print(f"  {pdf_file} æœªæå–åˆ°æ–‡æœ¬å†…å®¹")
                continue
            
            # åˆ†å‰²æ–‡æœ¬
            file_chunks = self.chunk_text(text)
            print(f"  åˆ†å‰²æˆ {len(file_chunks)} ä¸ªæ–‡æœ¬å—")
            
            # è·å–embedding
            for chunk in file_chunks:
                embedding = self.get_embedding(chunk)
                if embedding:
                    self.chunks.append(chunk)
                    self.chunk_embeddings.append(embedding)
        
        if not self.chunks:
            print("æœªæˆåŠŸæå–ä»»ä½•æ–‡æœ¬å—")
            return
        
        print(f"æ€»å…±æå–äº† {len(self.chunks)} ä¸ªæ–‡æœ¬å—")
        
        # æ„å»ºFAISSç´¢å¼•
        print("æ­£åœ¨æ„å»ºFAISSç´¢å¼•...")
        embeddings_array = np.array(self.chunk_embeddings).astype('float32')
        self.index = faiss.IndexFlatIP(embeddings_array.shape[1])
        self.index.add(embeddings_array)
        
        print("çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼")
    
    def retrieve_documents(self, query: str, top_k: int = None) -> List[Dict[str, Any]]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        if not self.index or not self.chunks:
            return []
        
        if top_k is None:
            top_k = self.top_k
        
        # è·å–æŸ¥è¯¢çš„embedding
        query_embedding = self.get_embedding(query)
        if not query_embedding:
            return []
        
        # æœç´¢ç›¸ä¼¼æ–‡æ¡£
        query_embedding_array = np.array([query_embedding]).astype('float32')
        scores, indices = self.index.search(query_embedding_array, top_k)
        
        # æ„å»ºç»“æœ
        retrieved_docs = []
        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
            if idx < len(self.chunks):
                retrieved_docs.append({
                    'text': self.chunks[idx],
                    'score': float(score),
                    'index': int(idx)
                })
        
        return retrieved_docs
    
    def rerank_documents(self, query: str, documents: List[Dict[str, Any]], top_k: int = None) -> List[Dict[str, Any]]:
        """é‡æ’åºæ–‡æ¡£"""
        if not documents:
            return []
        
        if top_k is None:
            top_k = self.rerank_top_k
        
        if self.reranker:
            # ä½¿ç”¨CrossEncoderé‡æ’åº
            try:
                pairs = [(query, doc['text']) for doc in documents]
                scores = self.reranker.predict(pairs)
                
                # å°†åˆ†æ•°æ·»åŠ åˆ°æ–‡æ¡£ä¸­
                for doc, score in zip(documents, scores):
                    doc['rerank_score'] = float(score)
                
                # æŒ‰é‡æ’åºåˆ†æ•°æ’åº
                reranked_docs = sorted(documents, key=lambda x: x['rerank_score'], reverse=True)
                return reranked_docs[:top_k]
            except Exception as e:
                print(f"CrossEncoderé‡æ’åºå¤±è´¥: {e}")
                # å›é€€åˆ°ä½™å¼¦ç›¸ä¼¼åº¦
                pass
        
        # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦é‡æ’åº
        try:
            # è·å–æŸ¥è¯¢çš„TF-IDFå‘é‡
            query_tokens = list(jieba.cut(query))
            query_text = ' '.join(query_tokens)
            
            # è·å–æ–‡æ¡£çš„TF-IDFå‘é‡
            doc_texts = [doc['text'] for doc in documents]
            doc_tokens = [list(jieba.cut(doc)) for doc in doc_texts]
            doc_texts_processed = [' '.join(tokens) for tokens in doc_tokens]
            
            # è®¡ç®—TF-IDF
            all_texts = [query_text] + doc_texts_processed
            vectorizer = TfidfVectorizer()
            tfidf_matrix = vectorizer.fit_transform(all_texts)
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            query_vector = tfidf_matrix[0:1]
            doc_vectors = tfidf_matrix[1:]
            similarities = cosine_similarity(query_vector, doc_vectors).flatten()
            
            # å°†ç›¸ä¼¼åº¦åˆ†æ•°æ·»åŠ åˆ°æ–‡æ¡£ä¸­
            for doc, similarity in zip(documents, similarities):
                doc['rerank_score'] = float(similarity)
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            reranked_docs = sorted(documents, key=lambda x: x['rerank_score'], reverse=True)
            return reranked_docs[:top_k]
            
        except Exception as e:
            print(f"TF-IDFé‡æ’åºä¹Ÿå¤±è´¥: {e}")
            # è¿”å›åŸå§‹æ’åºçš„æ–‡æ¡£
            return documents[:top_k]
    
    def generate_answer(self, query: str, context_docs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”Ÿæˆç­”æ¡ˆ"""
        try:
            # æ„å»ºä¸Šä¸‹æ–‡
            context = "\n\n".join([doc['text'] for doc in context_docs])
            
            # æ„å»ºæç¤ºè¯
            prompt = f"""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚è¯·æä¾›å‡†ç¡®ã€è¯¦ç»†çš„ç­”æ¡ˆã€‚

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{query}

è¯·å›ç­”ï¼š"""

            # ç”Ÿæˆç­”æ¡ˆ
            inputs = self.tokenizer(prompt, return_tensors="pt")
            
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs.input_ids,
                    max_new_tokens=512,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            answer = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # æå–ç”Ÿæˆçš„ç­”æ¡ˆéƒ¨åˆ†
            answer = answer.replace(prompt, "").strip()
            
            return {
                'answer': answer,
                'thinking': f"åŸºäº {len(context_docs)} ä¸ªç›¸å…³æ–‡æ¡£ç”Ÿæˆç­”æ¡ˆ",
                'success': True
            }
            
        except Exception as e:
            return {
                'answer': f"ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™: {e}",
                'thinking': "",
                'success': False
            }
    
    def process_single_query(self, query: str) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªæŸ¥è¯¢"""
        # æ£€ç´¢æ–‡æ¡£
        retrieved_docs = self.retrieve_documents(query)
        if not retrieved_docs:
            return {
                'answer': "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£",
                'thinking': "æ£€ç´¢å¤±è´¥",
                'success': False,
                'retrieved_docs': [],
                'reranked_docs': []
            }
        
        # é‡æ’åºæ–‡æ¡£
        reranked_docs = self.rerank_documents(query, retrieved_docs)
        
        # ç”Ÿæˆç­”æ¡ˆ
        result = self.generate_answer(query, reranked_docs)
        
        return {
            'retrieved_docs': retrieved_docs,
            'reranked_docs': reranked_docs,
            'answer': result['answer'],
            'thinking': result['thinking'],
            'success': result['success']
        }
    
    def batch_inference(self, input_json_path: str, output_json_path: str):
        """æ‰¹é‡æ¨ç†"""
        print(f"å¼€å§‹æ‰¹é‡æ¨ç†ï¼Œè¾“å…¥æ–‡ä»¶: {input_json_path}")
        
        # è¯»å–è¾“å…¥æ•°æ®
        with open(input_json_path, 'r', encoding='utf-8') as f:
            input_data = json.load(f)
        
        results = []
        
        for i, item in enumerate(input_data):
            query = item.get('input', '')
            expected_output = item.get('output', '')
            
            print(f"\nå¤„ç†ç¬¬ {i+1}/{len(input_data)} ä¸ªæŸ¥è¯¢...")
            
            # å¤„ç†æŸ¥è¯¢
            result = self.process_single_query(query)
            
            # æ·»åŠ åŸå§‹æ•°æ®å’ŒæœŸæœ›è¾“å‡º
            result.update({
                'original_instruction': item.get('instruction', ''),
                'original_input': item.get('input', ''),
                'expected_output': expected_output,
                'index': i
            })
            
            results.append(result)
            
            # å®šæœŸä¿å­˜ä¸­é—´ç»“æœ
            if (i + 1) % 10 == 0:
                temp_output_path = output_json_path.replace('.json', f'_temp_{i+1}.json')
                with open(temp_output_path, 'w', encoding='utf-8') as f:
                    json.dump(results, f, ensure_ascii=False, indent=2)
                print(f"å·²ä¿å­˜ä¸­é—´ç»“æœåˆ°: {temp_output_path}")
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        with open(output_json_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\næ‰¹é‡æ¨ç†å®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ°: {output_json_path}")
        print(f"æ€»å…±å¤„ç†äº† {len(results)} ä¸ªæŸ¥è¯¢")


def interactive_mode(rag_system):
    """äº¤äº’å¼æ¨ç†æ¨¡å¼"""
    print("\n" + "="*60)
    print("RAG + LLM äº¤äº’å¼é—®ç­”ç³»ç»Ÿ")
    print("="*60)
    print("ç³»ç»Ÿå·²å‡†å¤‡å°±ç»ªï¼æ‚¨å¯ä»¥å¼€å§‹æé—®äº†ã€‚")
    print("è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡ºç³»ç»Ÿ")
    print("è¾“å…¥ 'help' æŸ¥çœ‹å¸®åŠ©ä¿¡æ¯")
    print("-"*60)
    
    # äº¤äº’å¼é—®ç­”å¾ªç¯
    while True:
        try:
            # è·å–ç”¨æˆ·è¾“å…¥
            query = input("\nè¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ").strip()
            
            # æ£€æŸ¥é€€å‡ºå‘½ä»¤
            if query.lower() in ['quit', 'exit', 'é€€å‡º', 'q']:
                print("æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼")
                break
            
            # æ£€æŸ¥å¸®åŠ©å‘½ä»¤
            if query.lower() in ['help', 'å¸®åŠ©', 'h']:
                print("\nå¸®åŠ©ä¿¡æ¯:")
                print("- ç›´æ¥è¾“å…¥é—®é¢˜å³å¯è·å¾—ç­”æ¡ˆ")
                print("- ç³»ç»Ÿä¼šè‡ªåŠ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£å¹¶ç”Ÿæˆå›ç­”")
                print("- è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡ºç³»ç»Ÿ")
                print("- è¾“å…¥ 'help' æŸ¥çœ‹æ­¤å¸®åŠ©ä¿¡æ¯")
                continue
            
            # æ£€æŸ¥ç©ºè¾“å…¥
            if not query:
                print("è¯·è¾“å…¥æœ‰æ•ˆçš„é—®é¢˜ã€‚")
                continue
            
            print(f"\næ­£åœ¨å¤„ç†æ‚¨çš„é—®é¢˜: {query}")
            print("-"*40)
            
            # å¤„ç†æŸ¥è¯¢
            result = rag_system.process_single_query(query)
            
            # æ˜¾ç¤ºæ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£
            print("\nğŸ“š æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£:")
            print("-"*40)
            if result['reranked_docs']:
                for i, doc in enumerate(result['reranked_docs']):
                    print(f"æ–‡æ¡£ {i+1} (ç›¸ä¼¼åº¦: {doc.get('rerank_score', doc.get('score', 0)):.4f}):")
                    # æˆªå–æ–‡æ¡£å†…å®¹çš„å‰200ä¸ªå­—ç¬¦
                    doc_preview = doc['text'][:200] + "..." if len(doc['text']) > 200 else doc['text']
                    print(f"  {doc_preview}")
                    print()
            else:
                print("æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
            
            # æ˜¾ç¤ºç”Ÿæˆçš„ç­”æ¡ˆ
            print("\nğŸ¤– AI ç”Ÿæˆçš„ç­”æ¡ˆ:")
            print("-"*40)
            if result['success']:
                print(result['answer'])
                
                # å¦‚æœå¯ç”¨äº†thinkingï¼Œæ˜¾ç¤ºæ¨ç†è¿‡ç¨‹
                if result.get('thinking'):
                    print(f"\nğŸ’­ æ¨ç†è¿‡ç¨‹:")
                    print("-"*20)
                    print(result['thinking'])
            else:
                print(f"âŒ ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºç°é”™è¯¯: {result['answer']}")
            
            print("\n" + "="*60)
            
        except KeyboardInterrupt:
            print("\n\næ£€æµ‹åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨é€€å‡º...")
            break
        except Exception as e:
            print(f"\nâŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            print("è¯·é‡è¯•æˆ–è”ç³»æŠ€æœ¯æ”¯æŒ")


def batch_mode(rag_system):
    """æ‰¹é‡æ¨ç†æ¨¡å¼"""
    print("\n" + "="*60)
    print("RAG + LLM æ‰¹é‡æ¨ç†ç³»ç»Ÿ")
    print("="*60)
    
    # è·å–è¾“å…¥å’Œè¾“å‡ºæ–‡ä»¶è·¯å¾„
    input_json_path = input("è¯·è¾“å…¥JSONæ–‡ä»¶è·¯å¾„: ").strip()
    if not input_json_path:
        print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ–‡ä»¶è·¯å¾„")
        return
    
    if not os.path.exists(input_json_path):
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_json_path}")
        return
    
    output_json_path = input("è¯·è¾“å…¥è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„: ").strip()
    if not output_json_path:
        print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„è¾“å‡ºæ–‡ä»¶è·¯å¾„")
        return
    
    print(f"å¼€å§‹æ‰¹é‡æ¨ç†...")
    print(f"è¾“å…¥æ–‡ä»¶: {input_json_path}")
    print(f"è¾“å‡ºæ–‡ä»¶: {output_json_path}")
    
    # æ‰§è¡Œæ‰¹é‡æ¨ç†
    rag_system.batch_inference(input_json_path, output_json_path)


def main():
    # é…ç½®å‚æ•°
    PDF_FOLDER = "knowledge_base_TCM"  # PDFæ–‡ä»¶å¤¹è·¯å¾„,éœ€ä¿®æ”¹æ­¤å¤„ä»£ç æ›´æ–°ç»“æœ
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    if not os.getenv("DASHSCOPE_API_KEY"):
        print("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEY")
        return
    
    print("RAG + LLM ç³»ç»Ÿé…ç½®:")
    print("- ä½¿ç”¨æ¨¡å‹: Qwen3-14B")
    print("- æ¨¡å‹ç¼“å­˜ç›®å½•: autodl-tmp/qwen")
    print("- æ–‡æœ¬å—å¤§å°: 500å­—ç¬¦")
    print("- æ£€ç´¢æ•°é‡: 10ä¸ªæ–‡æ¡£")
    print("- é‡æ’åºåä¿ç•™: 3ä¸ªæ–‡æ¡£")
    print()
    
    # åˆå§‹åŒ–RAGç³»ç»Ÿ
    rag_system = RAGSystem(
        chunk_size=500,
        chunk_overlap=50,
        top_k=10,
        rerank_top_k=3
    )
    
    # æ„å»ºçŸ¥è¯†åº“
    rag_system.build_knowledge_base(PDF_FOLDER)
    
    print("\n" + "="*60)
    print("RAG + LLM ç³»ç»Ÿ")
    print("="*60)
    print("ç³»ç»Ÿå·²å‡†å¤‡å°±ç»ªï¼è¯·é€‰æ‹©è¿è¡Œæ¨¡å¼ï¼š")
    print("1. äº¤äº’å¼æ¨ç†æ¨¡å¼")
    print("2. æ‰¹é‡æ¨ç†æ¨¡å¼")
    print("-"*60)
    
    while True:
        try:
            choice = input("è¯·è¾“å…¥æ‚¨çš„é€‰æ‹© (1 æˆ– 2): ").strip()
            
            if choice == "1":
                interactive_mode(rag_system)
                break
            elif choice == "2":
                batch_mode(rag_system)
                break
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 1 æˆ– 2")
                continue
                
        except KeyboardInterrupt:
            print("\n\næ£€æµ‹åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨é€€å‡º...")
            break
        except Exception as e:
            print(f"\nâŒ é€‰æ‹©è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            print("è¯·é‡è¯•")


if __name__ == "__main__":
    main()
